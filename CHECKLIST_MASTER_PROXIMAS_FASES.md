# üéØ CHECKLIST MASTER - Pr√≥ximas Fases e Prioridades

**Projeto:** B3 AI Analysis Platform (invest-claude-web)
**Data de Cria√ß√£o:** 2025-11-13
**Respons√°vel:** Claude Code (Sonnet 4.5)
**Status:** üìã **PLANEJAMENTO ATIVO**

---

## üìä SITUA√á√ÉO ATUAL (2025-11-13 21:00)

### ‚úÖ O QUE EST√Å 100% COMPLETO

#### Frontend - P√°ginas (7/7)
- ‚úÖ FASE 4: Dashboard (/dashboard)
- ‚úÖ FASE 5: Portfolio (/portfolio)
- ‚úÖ FASE 6: Analysis (/analysis)
- ‚úÖ FASE 7: Reports (/reports) - 6 sub-fases (cleanup, endpoints, refactor, detail, downloads, e2e)
- ‚úÖ FASE 8: Data Sources (/data-sources)
- ‚úÖ FASE 9: OAuth Manager (/oauth-manager)
- ‚úÖ FASE 10: Settings (/settings)

#### Frontend - Valida√ß√µes (10/10)
- ‚úÖ FASE 12: Responsividade (mobile, tablet, desktop)
- ‚úÖ FASE 13: Navega√ß√£o (links, breadcrumbs, sidebar)
- ‚úÖ FASE 14: Performance (load 1.5s, bundle 87.6kB)
- ‚úÖ FASE 15: Network (requests, errors, retries)
- ‚úÖ FASE 16: Console (0 erros, 0 warnings)
- ‚úÖ FASE 17: Browser Compatibility (Chrome, Firefox)
- ‚úÖ FASE 18: TypeScript (0 erros, strict mode)
- ‚úÖ FASE 19: Integra√ß√µes Complexas (WebSocket, OAuth - 80%)
- ‚úÖ FASE 20: Estados e Transi√ß√µes (loading, success, error)
- ‚úÖ FASE 21: Acessibilidade (WCAG AA) ‚≠ê **FINAL**

#### Sistemas Especiais (3/3)
- ‚úÖ FASE 22: Sistema de Atualiza√ß√£o de Ativos (100%)
- ‚úÖ FASE 22.5: Corre√ß√µes Portfolio + Sidebar Toggle (100%)
- ‚úÖ FASE 3 (Reports): Refatora√ß√£o Completa (6 sub-fases - 100%)

#### Scrapers (1/1 - Documenta√ß√£o)
- ‚úÖ FASE 23 (Parcial): 4 scrapers implementados (Fundamentus, BRAPI, StatusInvest, Investidor10)
- ‚úÖ Documenta√ß√£o completa: 31 scrapers planejados (DOCUMENTACAO_SCRAPERS_COMPLETA.md)
- ‚úÖ Endpoint /scrapers/status funcionando
- ‚úÖ P√°gina /data-sources conectada com dados reais

### ‚è≥ O QUE EST√Å PENDENTE

#### 1. Scrapers (27/31 faltando - 87.10%)
**Prioridade:** üî¥ **ALTA**
**Status Atual:** 4/31 implementados (12.90%)

#### 2. Dados Hist√≥ricos BRAPI
**Prioridade:** üü° **M√âDIA**
**Status:** Planejado (FASE 23)

#### 3. Refatora√ß√£o Bot√£o "Solicitar An√°lises"
**Prioridade:** üü¢ **BAIXA**
**Status:** Aguardando aprova√ß√£o (FASE 24)

---

## üéØ DECIS√ÉO ESTRAT√âGICA: PR√ìXIMA FASE

### OP√á√ÉO RECOMENDADA: **FASE 23 - Completar Scrapers Fundamentalista**

**Justificativa:**
1. ‚úÖ Base s√≥lida: 4/6 scrapers j√° funcionam
2. ‚úÖ Infraestrutura pronta: ScrapersService, cross-validation, endpoint /status
3. ‚úÖ Frontend pronto: /data-sources exibindo dados reais
4. ‚úÖ Documenta√ß√£o completa: DOCUMENTACAO_SCRAPERS_COMPLETA.md
5. ‚úÖ Checklist existente: CHECKLIST_SCRAPERS_DATA_SOURCES.md
6. üéØ **Impacto direto**: Aumenta confian√ßa das an√°lises (6 fontes vs 4)
7. üéØ **Pr√≥ximo passo l√≥gico**: Completar categoria Fundamentalista (66% ‚Üí 100%)

**Alternativas:**
- ‚ùå FASE 23 (Hist√≥ricos BRAPI): Requer pesquisa extensiva, pode demorar
- ‚ùå FASE 24 (Refatora√ß√£o Bot√£o): Mudan√ßa cosm√©tica, baixo impacto

---

## üìã FASE 23: IMPLEMENTA√á√ÉO DE SCRAPERS - PLANO DETALHADO

### üéØ OBJETIVO
Completar a categoria **Fundamentalista** implementando os 2 scrapers faltantes:
1. **Fundamentei** (Privado - Google OAuth)
2. **Investsite** (P√∫blico - Sem login)

### üìä PROGRESSO ESPERADO
- **Antes:** 4/31 scrapers (12.90%)
- **Depois:** 6/31 scrapers (19.35%)
- **Categoria Fundamentalista:** 4/6 (66.67%) ‚Üí 6/6 (100%) ‚úÖ

---

## üîç PR√â-REQUISITOS OBRIGAT√ìRIOS

### 1Ô∏è‚É£ REVIS√ÉO COMPLETA DA FASE ANTERIOR

#### A. Git e Branch
- [ ] Verificar `git status` (deve estar limpo) ‚úÖ **CONFIRMADO**
- [ ] Verificar `git log` (√∫ltimo commit: `767bb3a` - documenta√ß√£o scrapers) ‚úÖ **CONFIRMADO**
- [ ] Verificar branch `main` atualizada com `origin/main` ‚úÖ **CONFIRMADO**
- [ ] Verificar commits recentes (√∫ltimos 10)

#### B. Ambiente Docker
- [ ] Frontend rodando (porta 3100) ‚úÖ **UP 12 mins (healthy)**
- [ ] Backend rodando (porta 3101) ‚úÖ **UP 12 mins (healthy)**
- [ ] PostgreSQL rodando (porta 5532) ‚úÖ **UP 27 hours (healthy)**
- [ ] Redis rodando (porta 6479) ‚úÖ **UP 27 hours (healthy)**
- [ ] API Service rodando (porta 8000) ‚úÖ **UP 27 hours (healthy)**
- [ ] Scrapers VNC rodando (porta 5900/6080) ‚úÖ **UP 27 hours (healthy)**

#### C. TypeScript e Build
- [ ] Backend: `npm run build` ‚Üí 0 erros
- [ ] Frontend: `npm run build` ‚Üí 0 erros
- [ ] Testes: `npm run test` ‚Üí Todos passando (se houver)

#### D. Validar Implementa√ß√£o Atual (/data-sources)
- [ ] Abrir http://localhost:3100/data-sources
- [ ] Verificar 4 cards renderizados
- [ ] Verificar dados reais (n√£o mocks)
- [ ] Verificar endpoint GET /scrapers/status funciona
- [ ] Screenshot: `pre-fase-23-data-sources-4-fontes.png`

#### E. An√°lise de C√≥digo Atual
- [ ] Ler `backend/src/scrapers/scrapers.service.ts`
  - [ ] Verificar `scrapeFundamentalData()` (linha ~38)
  - [ ] Verificar cross-validation (linha ~62)
  - [ ] Verificar confidence score (linha ~80)
  - [ ] Contar scrapers no Promise.allSettled (deve ser 4)
- [ ] Ler `backend/src/scrapers/scrapers.module.ts`
  - [ ] Verificar 4 scrapers registrados (Fundamentus, BRAPI, StatusInvest, Investidor10)
  - [ ] Verificar ScrapersController registrado
- [ ] Ler `backend/src/scrapers/scrapers.controller.ts`
  - [ ] Verificar endpoint GET /status retorna 4 fontes
  - [ ] TODO: Implementar m√©tricas reais (linha ~70)

---

## üõ†Ô∏è SUB-FASE 1: IMPLEMENTAR SCRAPER FUNDAMENTEI

### 1.1 Pesquisa e An√°lise do Site

#### A. An√°lise Manual (Browser)
- [ ] Abrir https://fundamentei.com
- [ ] Verificar se requer login (Google OAuth)
- [ ] Pesquisar ticker PETR4
- [ ] Identificar estrutura HTML:
  - [ ] Seletores CSS para P/L
  - [ ] Seletores CSS para P/VP
  - [ ] Seletores CSS para ROE
  - [ ] Seletores CSS para Dividend Yield
  - [ ] Seletores CSS para D√≠vida L√≠quida/EBITDA
  - [ ] Seletores CSS para Margem L√≠quida
- [ ] Screenshot da p√°gina de an√°lise (para refer√™ncia)
- [ ] Verificar requisi√ß√µes de rede (F12 ‚Üí Network)
- [ ] Verificar se tem API p√∫blica ou s√≥ scraping HTML

#### B. Comparar com StatusInvestScraper (OAuth j√° implementado)
- [ ] Ler `backend/src/scrapers/fundamental/statusinvest.scraper.ts`
- [ ] Verificar fluxo de OAuth (linhas ~20-50)
- [ ] Reutilizar l√≥gica de autentica√ß√£o
- [ ] Adaptar para Fundamentei

### 1.2 Implementa√ß√£o do Scraper

#### A. Criar Arquivo
- [ ] Criar `backend/src/scrapers/fundamental/fundamentei.scraper.ts`
- [ ] Copiar estrutura base de `statusinvest.scraper.ts`
- [ ] Adaptar para URL e seletores do Fundamentei

#### B. Interface e Tipos
```typescript
interface FundamenteiData {
  pl: number | null;           // P/L
  pvp: number | null;          // P/VP
  roe: number | null;          // ROE
  dy: number | null;           // Dividend Yield
  dividaLiquidaEbitda: number | null;
  margemLiquida: number | null;
}
```

#### C. Implementar M√©todo Principal
- [ ] `scrapeFundamentei(ticker: string): Promise<ScraperResult>`
- [ ] Login via Google OAuth (reutilizar StatusInvest)
- [ ] Navegar para p√°gina do ticker
- [ ] Extrair dados (seletores CSS)
- [ ] Validar dados (n√£o null, range v√°lido)
- [ ] Retornar ScraperResult padronizado

#### D. Error Handling
- [ ] Try/catch completo
- [ ] Logs detalhados (Logger.debug, Logger.error)
- [ ] Retry logic (3 tentativas)
- [ ] Timeout (30 segundos)

#### E. Testes Unit√°rios (Opcional, mas recomendado)
- [ ] Criar `fundamentei.scraper.spec.ts`
- [ ] Testar com ticker v√°lido (PETR4)
- [ ] Testar com ticker inv√°lido (AAAA99)
- [ ] Testar falha de autentica√ß√£o
- [ ] Testar timeout

---

## üõ†Ô∏è SUB-FASE 2: IMPLEMENTAR SCRAPER INVESTSITE

### 2.1 Pesquisa e An√°lise do Site

#### A. An√°lise Manual (Browser)
- [ ] Abrir https://investsite.com.br
- [ ] Verificar se √© p√∫blico (n√£o requer login) ‚úÖ
- [ ] Pesquisar ticker PETR4
- [ ] Identificar estrutura HTML:
  - [ ] Seletores CSS para P/L
  - [ ] Seletores CSS para P/VP
  - [ ] Seletores CSS para ROE
  - [ ] Seletores CSS para Dividend Yield
  - [ ] Seletores CSS para EV/EBITDA
  - [ ] Seletores CSS para Liquidez Corrente
- [ ] Screenshot da p√°gina de an√°lise
- [ ] Verificar requisi√ß√µes de rede (API p√∫blica?)

#### B. Comparar com FundamentusScraper (P√∫blico, sem login)
- [ ] Ler `backend/src/scrapers/fundamental/fundamentus.scraper.ts`
- [ ] Verificar uso do Playwright (linhas ~20-50)
- [ ] Reutilizar l√≥gica de navega√ß√£o
- [ ] Adaptar para Investsite

### 2.2 Implementa√ß√£o do Scraper

#### A. Criar Arquivo
- [ ] Criar `backend/src/scrapers/fundamental/investsite.scraper.ts`
- [ ] Copiar estrutura base de `fundamentus.scraper.ts`
- [ ] Adaptar para URL e seletores do Investsite

#### B. Interface e Tipos
```typescript
interface InvestsiteData {
  pl: number | null;           // P/L
  pvp: number | null;          // P/VP
  roe: number | null;          // ROE
  dy: number | null;           // Dividend Yield
  evEbitda: number | null;     // EV/EBITDA
  liquidezCorrente: number | null;
}
```

#### C. Implementar M√©todo Principal
- [ ] `scrapeInvestsite(ticker: string): Promise<ScraperResult>`
- [ ] Navegar para p√°gina do ticker (sem login)
- [ ] Extrair dados (seletores CSS)
- [ ] Validar dados
- [ ] Retornar ScraperResult padronizado

#### D. Error Handling
- [ ] Try/catch completo
- [ ] Logs detalhados
- [ ] Retry logic (3 tentativas)
- [ ] Timeout (30 segundos)

#### E. Testes Unit√°rios
- [ ] Criar `investsite.scraper.spec.ts`
- [ ] Testar com ticker v√°lido (PETR4)
- [ ] Testar com ticker inv√°lido
- [ ] Testar timeout

---

## üõ†Ô∏è SUB-FASE 3: INTEGRAR NOVOS SCRAPERS

### 3.1 Atualizar ScrapersModule

#### A. Arquivo: `backend/src/scrapers/scrapers.module.ts`
- [ ] Importar `FundamenteiScraper` (linha ~6)
- [ ] Importar `InvestsiteScraper` (linha ~7)
- [ ] Adicionar aos `providers` (linha ~18)
- [ ] Adicionar aos `exports` (linha ~26)

```typescript
import { FundamenteiScraper } from './fundamental/fundamentei.scraper';
import { InvestsiteScraper } from './fundamental/investsite.scraper';

@Module({
  controllers: [ScrapersController],
  providers: [
    FundamentusScraper,
    BrapiScraper,
    StatusInvestScraper,
    Investidor10Scraper,
    FundamenteiScraper,      // NOVO
    InvestsiteScraper,       // NOVO
    OpcoesScraper,
    ScrapersService,
  ],
  exports: [
    FundamentusScraper,
    BrapiScraper,
    StatusInvestScraper,
    Investidor10Scraper,
    FundamenteiScraper,      // NOVO
    InvestsiteScraper,       // NOVO
    OpcoesScraper,
    ScrapersService,
  ],
})
```

### 3.2 Atualizar ScrapersService

#### A. Arquivo: `backend/src/scrapers/scrapers.service.ts`
- [ ] Injetar `FundamenteiScraper` no constructor (linha ~15)
- [ ] Injetar `InvestsiteScraper` no constructor (linha ~16)
- [ ] Adicionar ao array em `scrapeFundamentalData()` (linha ~42)

```typescript
constructor(
  private readonly fundamentus: FundamentusScraper,
  private readonly brapi: BrapiScraper,
  private readonly statusInvest: StatusInvestScraper,
  private readonly investidor10: Investidor10Scraper,
  private readonly fundamentei: FundamenteiScraper,      // NOVO
  private readonly investsite: InvestsiteScraper,        // NOVO
  private readonly logger: Logger,
) {}

async scrapeFundamentalData(ticker: string): Promise<CrossValidationResult> {
  const results = await Promise.allSettled([
    this.fundamentus.scrape(ticker),
    this.brapi.scrape(ticker),
    this.statusInvest.scrape(ticker),
    this.investidor10.scrape(ticker),
    this.fundamentei.scrape(ticker),      // NOVO
    this.investsite.scrape(ticker),       // NOVO
  ]);
  // ... rest of cross-validation logic
}
```

#### B. Atualizar Cross-Validation
- [ ] Verificar m√©todo `crossValidate()` (linha ~62)
- [ ] Atualizar c√°lculo de confidence score:
  - 6 fontes concordam: 1.0
  - 5 fontes concordam: 0.90
  - 4 fontes concordam: 0.75
  - 3 fontes concordam: 0.60
  - Menos de 3: 0.40

### 3.3 Atualizar ScrapersController

#### A. Arquivo: `backend/src/scrapers/scrapers.controller.ts`
- [ ] Adicionar fonte "Fundamentei" ao array (linha ~25)
- [ ] Adicionar fonte "Investsite" ao array (linha ~47)

```typescript
const sources: DataSourceStatusDto[] = [
  {
    id: 'fundamentus',
    name: 'Fundamentus',
    // ... existing fields
  },
  {
    id: 'brapi',
    name: 'BRAPI',
    // ... existing fields
  },
  {
    id: 'statusinvest',
    name: 'Status Invest',
    // ... existing fields
  },
  {
    id: 'investidor10',
    name: 'Investidor10',
    // ... existing fields
  },
  {
    id: 'fundamentei',          // NOVO
    name: 'Fundamentei',
    url: 'https://fundamentei.com',
    type: 'fundamental',
    status: 'active',
    lastSync: new Date().toISOString(),
    successRate: 0,
    totalRequests: 0,
    failedRequests: 0,
    avgResponseTime: 0,
    requiresAuth: true,
  },
  {
    id: 'investsite',           // NOVO
    name: 'Investsite',
    url: 'https://investsite.com.br',
    type: 'fundamental',
    status: 'active',
    lastSync: new Date().toISOString(),
    successRate: 0,
    totalRequests: 0,
    failedRequests: 0,
    avgResponseTime: 0,
    requiresAuth: false,
  },
];
```

### 3.4 Build e Valida√ß√£o Backend

- [ ] `cd backend`
- [ ] `npm run build`
- [ ] Verificar 0 erros TypeScript
- [ ] `npm run start:dev` (se necess√°rio)
- [ ] Verificar logs: Scrapers injetados corretamente

---

## üé® SUB-FASE 4: ATUALIZAR FRONTEND /data-sources

### 4.1 Verificar Hook useDataSources

#### A. Arquivo: `frontend/src/lib/hooks/useDataSources.ts`
- [ ] Verificar endpoint: `GET /scrapers/status` (linha ~23)
- [ ] Verificar interface `DataSource` (linha ~4)
- [ ] **N√£o precisa alterar** (j√° busca array din√¢mico)

### 4.2 Verificar P√°gina /data-sources

#### A. Arquivo: `frontend/src/app/(dashboard)/data-sources/page.tsx`
- [ ] Verificar renderiza√ß√£o din√¢mica de cards (linha ~190)
- [ ] Verificar `filteredSources.map()` (linha ~190)
- [ ] **N√£o precisa alterar** (j√° renderiza dinamicamente)

### 4.3 Valida√ß√£o Visual

- [ ] Abrir http://localhost:3100/data-sources
- [ ] Verificar **6 cards** renderizados (n√£o 4)
- [ ] Verificar ordem:
  1. Fundamentus
  2. BRAPI
  3. Status Invest
  4. Investidor10
  5. **Fundamentei** (NOVO)
  6. **Investsite** (NOVO)
- [ ] Verificar badges de status (cores corretas)
- [ ] Verificar √≠cone "Requer Autentica√ß√£o" no Fundamentei
- [ ] Screenshot: `pos-fase-23-data-sources-6-fontes.png`

### 4.4 Build e Valida√ß√£o Frontend

- [ ] `cd frontend`
- [ ] `npm run build`
- [ ] Verificar 0 erros TypeScript
- [ ] Verificar rota `/data-sources` compilada

---

## üß™ SUB-FASE 5: TESTES COMPLETOS COM DADOS REAIS

### 5.1 Teste Backend (curl/Postman)

#### A. Endpoint /scrapers/status
```bash
curl http://localhost:3101/api/v1/scrapers/status
```
- [ ] Retorna 6 fontes
- [ ] Status 200 OK
- [ ] JSON v√°lido

#### B. Endpoint /analysis/fundamental/:ticker
```bash
curl -X POST http://localhost:3101/api/v1/analysis/fundamental/PETR4
```
- [ ] Criar an√°lise de PETR4
- [ ] Verificar logs backend (6 scrapers tentados)
- [ ] Verificar resposta JSON:
  - [ ] Campo `dataSources` tem 6 itens
  - [ ] Campo `sourcesCount` = 6
  - [ ] Campo `confidenceScore` baseado em 6 fontes
- [ ] Salvar resposta em `teste-analise-petr4-6-fontes.json`

### 5.2 Teste Frontend - Playwright MCP (OBRIGAT√ìRIO)

#### A. Setup
- [ ] Abrir MCP Playwright em janela separada
- [ ] Navegar para http://localhost:3100/data-sources

#### B. Verifica√ß√µes
- [ ] Snapshot da p√°gina
- [ ] Contar cards: `page.locator('[data-testid="source-card"]').count()` = 6
- [ ] Verificar texto "Fundamentei" presente
- [ ] Verificar texto "Investsite" presente
- [ ] Screenshot: `playwright-data-sources-6-fontes.png`

#### C. Teste de Intera√ß√£o
- [ ] Clicar em filtro "Fundamentalista"
- [ ] Verificar 6 cards vis√≠veis (todos fundamentalistas)
- [ ] Clicar bot√£o "Testar" de Fundamentei (se implementado)
- [ ] Verificar loading state

### 5.3 Teste Frontend - Selenium (OBRIGAT√ìRIO)

#### A. Setup
- [ ] Abrir MCP Selenium em janela separada
- [ ] Iniciar browser Chrome
- [ ] Navegar para http://localhost:3100/data-sources

#### B. Verifica√ß√µes DOM
- [ ] `find_elements(by="css", value="[role='article']")` ‚Üí 6 elementos
- [ ] Verificar textos:
  - [ ] "Fundamentus"
  - [ ] "BRAPI"
  - [ ] "Status Invest"
  - [ ] "Investidor10"
  - [ ] "Fundamentei"
  - [ ] "Investsite"
- [ ] Screenshot: `selenium-data-sources-6-fontes.png`

### 5.4 Teste Frontend - Chrome DevTools (OBRIGAT√ìRIO)

#### A. Setup
- [ ] Abrir MCP Chrome DevTools em janela separada
- [ ] Navegar para http://localhost:3100/data-sources

#### B. Console
- [ ] Verificar 0 erros
- [ ] Verificar 0 warnings cr√≠ticos
- [ ] Screenshot: `devtools-console-clean.png`

#### C. Network
- [ ] Verificar request `GET /scrapers/status`
- [ ] Status 200 OK
- [ ] Response tem 6 fontes
- [ ] Tempo de resposta < 500ms
- [ ] Screenshot: `devtools-network-6-fontes.png`

#### D. Performance (Lighthouse)
- [ ] Rodar Lighthouse
- [ ] Performance ‚â• 90
- [ ] Accessibility ‚â• 90
- [ ] Best Practices ‚â• 90
- [ ] Screenshot: `lighthouse-data-sources.png`

### 5.5 Teste de An√°lise Completa (E2E)

#### A. Criar An√°lise de PETR4
- [ ] Abrir http://localhost:3100/analysis
- [ ] Pesquisar PETR4
- [ ] Clicar "Solicitar An√°lise Completa"
- [ ] Aguardar processamento (~30-60 segundos)

#### B. Verificar Logs Backend
- [ ] `docker logs invest_backend --tail 100 | grep "Scraper"`
- [ ] Verificar tentativas dos 6 scrapers:
  1. FundamentusScraper.scrape(PETR4) ‚Üí SUCCESS/FAILED
  2. BrapiScraper.scrape(PETR4) ‚Üí SUCCESS/FAILED
  3. StatusInvestScraper.scrape(PETR4) ‚Üí SUCCESS/FAILED
  4. Investidor10Scraper.scrape(PETR4) ‚Üí SUCCESS/FAILED
  5. **FundamenteiScraper.scrape(PETR4) ‚Üí SUCCESS/FAILED**
  6. **InvestsiteScraper.scrape(PETR4) ‚Üí SUCCESS/FAILED**
- [ ] Salvar logs em `logs-analise-petr4-6-scrapers.txt`

#### C. Verificar Resultado
- [ ] Abrir http://localhost:3100/reports
- [ ] Encontrar an√°lise de PETR4
- [ ] Verificar campo "Fontes de Dados": 6 fontes listadas
- [ ] Verificar confidence score:
  - Se 6 fontes OK: 1.0
  - Se 5 fontes OK: 0.90
  - Se 4 fontes OK: 0.75
  - Se 3 fontes OK: 0.60
- [ ] Screenshot: `report-petr4-6-fontes.png`

### 5.6 Teste de Cross-Validation

#### A. Coletar Dados de Todas as Fontes
- [ ] Executar an√°lise de PETR4
- [ ] Extrair P/L de cada fonte:
  - Fundamentus: __.__
  - BRAPI: __.__
  - StatusInvest: __.__
  - Investidor10: __.__
  - **Fundamentei: __.__ ** (NOVO)
  - **Investsite: __.__ ** (NOVO)

#### B. Verificar Discrep√¢ncias
- [ ] Calcular m√©dia: (__+__+__+__+__+__) / 6 = __.__
- [ ] Verificar diverg√™ncia m√°xima: __ %
- [ ] Se diverg√™ncia > 10%: Logar warning
- [ ] Verificar campo `discrepancies` na an√°lise

#### C. Validar Confidence Score
- [ ] Se todas 6 fontes concordam (< 10% diverg√™ncia): 1.0 ‚úÖ
- [ ] Se 5 fontes concordam: 0.90 ‚úÖ
- [ ] Se 4 fontes concordam: 0.75 ‚úÖ
- [ ] Se 3 fontes concordam: 0.60 ‚ö†Ô∏è
- [ ] Se < 3 fontes concordam: 0.40 ‚ùå

---

## üìä SUB-FASE 6: VALIDA√á√ÉO COMPLETA

### 6.1 TypeScript

- [ ] Backend: `cd backend && npm run build` ‚Üí 0 erros
- [ ] Frontend: `cd frontend && npm run build` ‚Üí 0 erros

### 6.2 Console

- [ ] Abrir /data-sources no Chrome
- [ ] F12 ‚Üí Console ‚Üí 0 erros cr√≠ticos

### 6.3 Responsividade

- [ ] Desktop (1920x1080): 3 colunas de cards
- [ ] Tablet (768x1024): 2 colunas
- [ ] Mobile (375x667): 1 coluna
- [ ] Screenshot de cada: `responsive-desktop.png`, `responsive-tablet.png`, `responsive-mobile.png`

### 6.4 Acessibilidade

- [ ] Tab navigation funciona
- [ ] Focus visible em bot√µes
- [ ] Labels corretos (aria-label)
- [ ] Lighthouse Accessibility ‚â• 90

### 6.5 Backend Logs

- [ ] Verificar logs mostram 6 tentativas de scraping
- [ ] Verificar cross-validation funciona
- [ ] Verificar confidence score correto
- [ ] Salvar logs em `backend-logs-6-scrapers.txt`

---

## üì∏ EVID√äNCIAS OBRIGAT√ìRIAS

### Screenshots (12 obrigat√≥rios)

1. [ ] `pre-fase-23-data-sources-4-fontes.png` - Estado inicial (4 fontes)
2. [ ] `pos-fase-23-data-sources-6-fontes.png` - Estado final (6 fontes)
3. [ ] `playwright-data-sources-6-fontes.png` - Teste Playwright
4. [ ] `selenium-data-sources-6-fontes.png` - Teste Selenium
5. [ ] `devtools-console-clean.png` - Console 0 erros
6. [ ] `devtools-network-6-fontes.png` - Network request
7. [ ] `lighthouse-data-sources.png` - Lighthouse scores
8. [ ] `report-petr4-6-fontes.png` - Relat√≥rio com 6 fontes
9. [ ] `responsive-desktop.png` - Responsividade desktop
10. [ ] `responsive-tablet.png` - Responsividade tablet
11. [ ] `responsive-mobile.png` - Responsividade mobile
12. [ ] `backend-logs-6-scrapers.png` - Logs do backend

### Logs e Arquivos

1. [ ] `logs-analise-petr4-6-scrapers.txt` - Logs completos da an√°lise
2. [ ] `teste-analise-petr4-6-fontes.json` - Resposta JSON da an√°lise
3. [ ] `backend-logs-6-scrapers.txt` - Logs do backend durante scraping

---

## üìù DOCUMENTA√á√ÉO

### 7.1 Criar VALIDACAO_FASE_23_SCRAPERS_COMPLETA.md

#### Estrutura:
1. **Sum√°rio Executivo**
   - Scrapers implementados: 6/6 fundamentalistas (100%)
   - Testes executados: 50+
   - Status: ‚úÖ APROVADO

2. **Scrapers Implementados**
   - Fundamentus (P√∫blico)
   - BRAPI (API P√∫blica)
   - StatusInvest (Google OAuth)
   - Investidor10 (Google OAuth)
   - **Fundamentei (Google OAuth)** - NOVO
   - **Investsite (P√∫blico)** - NOVO

3. **Testes de Backend**
   - Endpoint /scrapers/status: 6 fontes
   - Endpoint /analysis/fundamental/:ticker: 6 scrapers tentados
   - Cross-validation: 6 fontes

4. **Testes de Frontend**
   - Playwright: 6 cards renderizados ‚úÖ
   - Selenium: 6 elementos DOM ‚úÖ
   - Chrome DevTools: 0 erros ‚úÖ

5. **Evid√™ncias**
   - 12 screenshots
   - 3 arquivos de logs
   - Dados reais de PETR4

6. **M√©tricas**
   - Success rate por scraper
   - Tempo m√©dio de resposta
   - Confidence score m√©dio
   - Discrep√¢ncias detectadas

7. **Problemas Encontrados e Resolvidos**
   - Listar problemas (se houver)
   - Solu√ß√µes aplicadas

### 7.2 Atualizar claude.md

#### Se√ß√£o "Fontes de Dados"
- [ ] Atualizar tabela: 6 fontes, todas ‚úÖ Implementadas
- [ ] Atualizar cross-validation: 6 fontes, m√≠nimo 3
- [ ] Atualizar progresso: Fundamentalista 100% (6/6)

```markdown
### An√°lise Fundamentalista (6 fontes implementadas - 100%)

| Fonte | Tipo | Login | Status | Scraper |
|-------|------|-------|--------|---------|
| **Fundamentus** | P√∫blico | N√£o | ‚úÖ Implementado | fundamentus.scraper.ts |
| **BRAPI** | API P√∫blica | N√£o | ‚úÖ Implementado | brapi.scraper.ts |
| **StatusInvest** | Privado | Google | ‚úÖ Implementado | statusinvest.scraper.ts |
| **Investidor10** | Privado | Google | ‚úÖ Implementado | investidor10.scraper.ts |
| **Fundamentei** | Privado | Google | ‚úÖ Implementado | fundamentei.scraper.ts |
| **Investsite** | P√∫blico | N√£o | ‚úÖ Implementado | investsite.scraper.ts |
```

#### Se√ß√£o "Roadmap"
- [ ] Atualizar FASE 23: ‚úÖ 100% COMPLETO (2025-11-13)
- [ ] Adicionar progresso: 6/31 scrapers (19.35%)

### 7.3 Atualizar DOCUMENTACAO_SCRAPERS_COMPLETA.md

- [ ] Se√ß√£o "Estat√≠sticas Gerais"
  - Total: 31
  - Implementadas: 6 (19.35%) ‚Üê **ATUALIZAR**
  - Em Desenvolvimento: 0
  - Planejadas: 25 (80.65%)

- [ ] Se√ß√£o "1. An√°lise Fundamentalista"
  - Status: 6/6 (100%) ‚Üê **ATUALIZAR**
  - Marcar Fundamentei: ‚úÖ Implementado
  - Marcar Investsite: ‚úÖ Implementado

---

## ‚úÖ CRIT√âRIOS DE APROVA√á√ÉO

### Bloqueadores (DEVEM estar 0)
- [ ] TypeScript: 0 erros (backend + frontend)
- [ ] Console: 0 erros cr√≠ticos
- [ ] Build: 0 falhas (backend + frontend)
- [ ] Git: 0 conflitos, branch limpa
- [ ] Scrapers: Todos 6 devem tentar coletar dados

### Qualidade M√≠nima
- [ ] 6 scrapers implementados e funcionais
- [ ] P√°gina /data-sources exibe 6 cards
- [ ] Dados s√£o REAIS (n√£o mocks)
- [ ] Cross-validation funciona com 6 fontes (m√≠nimo 3)
- [ ] Confidence score correto (baseado em 6 fontes)
- [ ] Endpoint /scrapers/status retorna 6 fontes
- [ ] An√°lise de PETR4 tenta usar 6 scrapers
- [ ] Logs mostram tentativas de todos scrapers
- [ ] Responsividade OK (mobile/tablet/desktop)
- [ ] Acessibilidade OK (keyboard, focus, aria)
- [ ] 12 screenshots capturados
- [ ] 3 arquivos de logs salvos
- [ ] Documenta√ß√£o atualizada (3 arquivos)
- [ ] Git limpo e atualizado

### Qualidade Ideal (Desej√°vel)
- [ ] Testes unit√°rios para ambos scrapers
- [ ] Success rate ‚â• 80% (m√≠nimo 5/6 scrapers funcionando)
- [ ] Confidence score ‚â• 0.75 em an√°lises
- [ ] Tempo de an√°lise < 60 segundos
- [ ] Performance Lighthouse ‚â• 90

---

## üö® PROBLEMAS CR√îNICOS A EVITAR

1. **N√£o usar mocks:** Sempre dados reais dos scrapers
2. **N√£o quebrar existente:** Testar 4 scrapers atuais antes de adicionar novos
3. **N√£o pular testes:** Playwright, Selenium E Chrome DevTools (todos 3 em janelas separadas)
4. **N√£o esquecer git:** Commit a cada sub-fase conclu√≠da
5. **N√£o ignorar errors:** Console deve ter 0 erros
6. **N√£o confiar em "parece funcionar":** Validar com dados reais (PETR4)
7. **N√£o pular screenshots:** 12 screenshots obrigat√≥rios
8. **N√£o esquecer logs:** 3 arquivos de logs obrigat√≥rios
9. **N√£o esquecer documenta√ß√£o:** 3 arquivos a atualizar

---

## üìÖ ESTIMATIVA DE TEMPO

- Sub-Fase 1 (Fundamentei): 2-3 horas
  - Pesquisa: 30 min
  - Implementa√ß√£o: 1-1.5 horas
  - Testes: 30 min
- Sub-Fase 2 (Investsite): 1.5-2 horas
  - Pesquisa: 20 min
  - Implementa√ß√£o: 1 hora
  - Testes: 20 min
- Sub-Fase 3 (Integra√ß√£o): 30 min
- Sub-Fase 4 (Frontend): 20 min (apenas valida√ß√£o)
- Sub-Fase 5 (Testes Completos): 2 horas
  - Playwright: 30 min
  - Selenium: 30 min
  - Chrome DevTools: 30 min
  - E2E: 30 min
- Sub-Fase 6 (Valida√ß√£o): 30 min
- Documenta√ß√£o: 1 hora

**Total Estimado:** 7-9 horas (desenvolvimento meticuloso e completo)

---

## üîÑ WORKFLOW VISUAL

```mermaid
graph TD
    A[PR√â-REQUISITOS] --> B{Git limpo?}
    B -->|N√£o| C[Resolver conflitos]
    C --> A
    B -->|Sim| D{Docker OK?}
    D -->|N√£o| E[Restart containers]
    E --> A
    D -->|Sim| F{Build OK?}
    F -->|N√£o| G[Fix TS errors]
    G --> A
    F -->|Sim| H[SUB-FASE 1: Fundamentei]
    H --> I[Pesquisar site]
    I --> J[Implementar scraper]
    J --> K[Testar unit√°rio]
    K --> L[SUB-FASE 2: Investsite]
    L --> M[Pesquisar site]
    M --> N[Implementar scraper]
    N --> O[Testar unit√°rio]
    O --> P[SUB-FASE 3: Integra√ß√£o]
    P --> Q[Atualizar ScrapersModule]
    Q --> R[Atualizar ScrapersService]
    R --> S[Atualizar ScrapersController]
    S --> T[Build backend]
    T --> U{Build OK?}
    U -->|N√£o| V[Fix errors]
    V --> T
    U -->|Sim| W[SUB-FASE 4: Frontend]
    W --> X[Validar /data-sources]
    X --> Y[Build frontend]
    Y --> Z{6 cards?}
    Z -->|N√£o| AA[Debug]
    AA --> X
    Z -->|Sim| AB[SUB-FASE 5: Testes]
    AB --> AC[Playwright]
    AC --> AD[Selenium]
    AD --> AE[Chrome DevTools]
    AE --> AF[E2E PETR4]
    AF --> AG{Todos OK?}
    AG -->|N√£o| AH[Corrigir problemas]
    AH --> AC
    AG -->|Sim| AI[SUB-FASE 6: Valida√ß√£o]
    AI --> AJ[TypeScript OK]
    AJ --> AK[Console OK]
    AK --> AL[Responsividade OK]
    AL --> AM[Acessibilidade OK]
    AM --> AN[Documenta√ß√£o]
    AN --> AO[Criar VALIDACAO_FASE_23]
    AO --> AP[Atualizar claude.md]
    AP --> AQ[Atualizar DOCUMENTACAO]
    AQ --> AR[Commit + Push]
    AR --> AS[‚úÖ FASE 23 APROVADA]
```

---

## üìä M√âTRICAS DE SUCESSO

### Quantitativas
- ‚úÖ 6 scrapers implementados (100% da categoria Fundamentalista)
- ‚úÖ 50+ testes executados (backend + frontend + E2E)
- ‚úÖ 12 screenshots capturados
- ‚úÖ 3 arquivos de logs salvos
- ‚úÖ 0 erros TypeScript
- ‚úÖ 0 erros cr√≠ticos de console
- ‚úÖ Confidence score ‚â• 0.75 (ideal)
- ‚úÖ Success rate ‚â• 80% (5/6 scrapers funcionando)

### Qualitativas
- ‚úÖ C√≥digo limpo e documentado
- ‚úÖ Error handling robusto
- ‚úÖ Logs detalhados
- ‚úÖ Testes completos (Playwright + Selenium + DevTools)
- ‚úÖ Documenta√ß√£o atualizada e completa
- ‚úÖ Git limpo e sincronizado

---

## üöÄ PR√ìXIMOS PASSOS (P√ìS FASE 23)

### Op√ß√£o 1: Continuar Scrapers (RECOMENDADO)
- **FASE 24:** Implementar categoria "Mercado" (3 scrapers)
  - Yahoo Finance Brasil
  - Google Finance
  - Valor Econ√¥mico
- **Estimativa:** 5-7 horas

### Op√ß√£o 2: Dados Hist√≥ricos BRAPI
- **FASE 25:** Implementar hist√≥rico de pre√ßos
  - Pesquisar endpoints BRAPI
  - Criar tabela historical_prices
  - Implementar endpoint backend
  - Criar componente frontend (gr√°ficos)
- **Estimativa:** 8-10 horas

### Op√ß√£o 3: Melhorias Sistema de Reports
- **FASE 26:** An√°lise de insiders
- **FASE 27:** An√°lise de dividendos
- **FASE 28:** An√°lise macroecon√¥mica

---

## ‚úÖ APROVA√á√ÉO FINAL

### Checklist de Aprova√ß√£o (TODOS devem estar ‚úÖ)

#### Backend
- [ ] 6 scrapers implementados e compilam sem erros
- [ ] ScrapersService injeta e usa 6 scrapers
- [ ] ScrapersController retorna 6 fontes
- [ ] Cross-validation funciona com 6 fontes
- [ ] Logs mostram tentativas de todos 6 scrapers
- [ ] Build backend: 0 erros TypeScript

#### Frontend
- [ ] /data-sources exibe 6 cards
- [ ] Dados s√£o reais (vindos da API)
- [ ] Loading states funcionam
- [ ] Error handling funciona
- [ ] Build frontend: 0 erros TypeScript

#### Testes
- [ ] Playwright: 6 cards confirmados
- [ ] Selenium: 6 elementos DOM confirmados
- [ ] Chrome DevTools: 0 erros console
- [ ] E2E: An√°lise PETR4 usa 6 scrapers
- [ ] Cross-validation testado com dados reais

#### Qualidade
- [ ] TypeScript: 0 erros (backend + frontend)
- [ ] Console: 0 erros cr√≠ticos
- [ ] Responsividade: OK em mobile/tablet/desktop
- [ ] Acessibilidade: Lighthouse ‚â• 90

#### Evid√™ncias
- [ ] 12 screenshots capturados
- [ ] 3 arquivos de logs salvos
- [ ] Dados reais de PETR4 coletados

#### Documenta√ß√£o
- [ ] VALIDACAO_FASE_23_SCRAPERS_COMPLETA.md criado
- [ ] claude.md atualizado (se√ß√£o Fontes de Dados)
- [ ] DOCUMENTACAO_SCRAPERS_COMPLETA.md atualizado

#### Git
- [ ] Git status limpo
- [ ] Commits descritivos (1 por sub-fase)
- [ ] Branch main atualizada com origin/main
- [ ] Push realizado com sucesso

---

**Criado por:** Claude Code (Sonnet 4.5)
**Data:** 2025-11-13
**Status:** üìã **AGUARDANDO EXECU√á√ÉO**
**Estimativa:** 7-9 horas de desenvolvimento meticuloso

ü§ñ Generated with [Claude Code](https://claude.com/claude-code)

Co-Authored-By: Claude <noreply@anthropic.com>
