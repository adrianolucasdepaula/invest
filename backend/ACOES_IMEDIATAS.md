# A√á√ïES IMEDIATAS - B3 AI ANALYSIS PLATFORM

## üö® CHECKLIST DE A√á√ïES CR√çTICAS (4 itens - 20 minutos total)

### 1. INSTALAR CHROME (5 minutos) - CR√çTICO
```bash
# Op√ß√£o A: Google Chrome
sudo apt-get update
sudo apt-get install -y wget
wget -q -O - https://dl.google.com/linux/linux_signing_key.pub | sudo apt-key add -
sudo sh -c 'echo "deb [arch=amd64] http://dl.google.com/linux/chrome/deb/ stable main" >> /etc/apt/sources.list.d/google-chrome.list'
sudo apt-get update
sudo apt-get install -y google-chrome-stable

# Op√ß√£o B: Chromium (alternativa)
sudo apt-get update
sudo apt-get install -y chromium-browser
```
**Impacto:** Sem Chrome, NENHUM dos 27 scrapers funciona!

### 2. INICIAR REDIS (2 minutos) - CR√çTICO
```bash
# Via Docker
docker run -d --name redis -p 6479:6379 redis:alpine

# OU via docker-compose (se existir)
cd /home/user/invest/backend
docker-compose up -d redis
```
**Impacto:** Sem Redis, sem cache = performance muito lenta

### 3. INICIAR POSTGRESQL (2 minutos) - CR√çTICO
```bash
# Via Docker
docker run -d --name postgres \
  -e POSTGRES_USER=invest_user \
  -e POSTGRES_PASSWORD=invest_password \
  -e POSTGRES_DB=invest_db \
  -p 5532:5432 \
  postgres:14-alpine

# OU via docker-compose (se existir)
cd /home/user/invest/backend
docker-compose up -d postgres
```
**Impacto:** Sem PostgreSQL, sem persist√™ncia de dados

### 4. CONFIGURAR GOOGLE OAUTH (10 minutos) - ALTA PRIORIDADE
```bash
cd /home/user/invest/backend/python-scrapers
python3 save_google_cookies.py

# Siga as instru√ß√µes interativas:
# 1. Fazer login no Google
# 2. Aceitar permiss√µes
# 3. Cookies ser√£o salvos automaticamente
```
**Impacto:** Sem OAuth, 13 scrapers (48%) n√£o funcionam

---

## ‚úÖ VERIFICA√á√ÉO DE SUCESSO

### Ap√≥s completar as 4 a√ß√µes, execute:
```bash
# 1. Verificar Chrome
google-chrome --version || chromium-browser --version

# 2. Verificar Redis
redis-cli -p 6479 ping
# Esperado: PONG

# 3. Verificar PostgreSQL
PGPASSWORD=invest_password psql -h localhost -p 5532 -U invest_user -d invest_db -c "SELECT 1"
# Esperado: ?column? 1

# 4. Verificar cookies OAuth
ls -la python-scrapers/browser-profiles/google_cookies.pkl
# Esperado: arquivo existe

# 5. Testar scrapers p√∫blicos
cd python-scrapers
python3 tests/test_public_scrapers.py --ticker VALE3
```

---

## üéØ PR√ìXIMOS PASSOS (Ap√≥s a√ß√µes cr√≠ticas)

### Dia 1 - Testes e Valida√ß√£o
```bash
# 1. Corrigir testes E2E
npm install @nestjs/passport passport passport-jwt
npm run test:e2e

# 2. Corrigir vulnerabilidades
npm audit fix --force

# 3. Testar todos scrapers p√∫blicos
python3 tests/test_public_scrapers.py --detailed
```

### Dia 2 - Scrapers OAuth
```bash
# Testar scrapers com autentica√ß√£o OAuth
python3 test_scrapers.py --oauth --detailed
```

### Dia 3 - Monitoramento
```bash
# Implementar health checks
curl http://localhost:3333/health
```

---

## üìä DASHBOARD DE STATUS

| Componente | Status Atual | Status Ap√≥s A√ß√µes | Comando de Verifica√ß√£o |
|------------|--------------|-------------------|------------------------|
| Chrome | ‚ùå N√£o instalado | ‚úÖ Instalado | `google-chrome --version` |
| Redis | ‚ùå N√£o rodando | ‚úÖ Rodando | `redis-cli -p 6479 ping` |
| PostgreSQL | ‚ùå N√£o rodando | ‚úÖ Rodando | `psql ... -c "SELECT 1"` |
| OAuth Cookies | ‚ùå N√£o configurado | ‚úÖ Configurado | `ls google_cookies.pkl` |
| Backend Build | ‚úÖ OK | ‚úÖ OK | `npm run build` |
| TypeScript | ‚úÖ 0 erros | ‚úÖ 0 erros | `npx tsc --noEmit` |
| Scrapers Import | ‚úÖ 27/27 OK | ‚úÖ 27/27 OK | `python3 validate_setup.py` |
| Testes E2E | ‚ùå Falhando | ‚è≥ A corrigir | `npm run test:e2e` |
| Scrapers P√∫blicos | ‚è∏Ô∏è Bloqueado | ‚úÖ Funcionando | `python3 test_public.py` |
| Scrapers OAuth | ‚è∏Ô∏è Bloqueado | ‚úÖ Funcionando | `python3 test_oauth.py` |

---

## üí° DICAS IMPORTANTES

### Se encontrar problemas:

#### Chrome n√£o instala?
```bash
# Verificar arquitetura
uname -m
# Se for ARM/M1, use Chromium ao inv√©s de Chrome

# Limpar cache apt
sudo apt-get clean
sudo apt-get update
```

#### Redis/PostgreSQL n√£o conectam?
```bash
# Verificar portas em uso
netstat -tulpn | grep -E '6479|5532'

# Usar portas alternativas se necess√°rio
# Editar: python-scrapers/config.py
```

#### OAuth falha?
```bash
# Verificar selenium
python3 -c "from selenium import webdriver; print('OK')"

# Reinstalar se necess√°rio
pip3 install --upgrade selenium
```

---

## üìà M√âTRICAS DE SUCESSO

Ap√≥s completar as 4 a√ß√µes, voc√™ ter√°:
- ‚úÖ **100% dos scrapers p√∫blicos** (9) funcionando
- ‚úÖ **100% dos scrapers OAuth** (13) prontos ap√≥s login
- ‚úÖ **Cache Redis** otimizando performance
- ‚úÖ **PostgreSQL** persistindo dados
- ‚úÖ **Sistema 95% operacional**

**Tempo total estimado:** 20 minutos
**Complexidade:** Baixa (copiar e colar comandos)
**Resultado:** Sistema pronto para produ√ß√£o!

---

## üÜò SUPORTE

Se encontrar problemas:
1. Verifique os logs: `tail -f /var/log/syslog`
2. Consulte a documenta√ß√£o: `python-scrapers/README.md`
3. Execute diagn√≥stico: `python3 validate_setup.py`

---

**Gerado em:** 2025-11-08
**Prioridade:** EXECUTE AGORA - Sistema est√° 87% pronto, faltam apenas estas 4 a√ß√µes!