# ğŸ” EstratÃ©gia para Scrapers com Google OAuth

**Criado:** 2025-11-07
**Status:** Documento de Planejamento

---

## ğŸ“‹ Fontes que Requerem Google OAuth

VocÃª acessa essas fontes usando autenticaÃ§Ã£o Google:

1. **Fundamentei** - https://fundamentei.com/
2. **Investidor10** - https://investidor10.com.br/
3. **StatusInvest** - https://statusinvest.com.br/

---

## ğŸ¯ OpÃ§Ãµes de ImplementaÃ§Ã£o

### âœ… OpÃ§Ã£o 1: Selenium com Cookies Salvos (RECOMENDADO)

**Como funciona:**
1. VocÃª faz login manualmente UMA VEZ no seu navegador
2. Exportamos os cookies da sessÃ£o autenticada
3. Scrapers carregam esses cookies para manter a sessÃ£o ativa
4. RenovaÃ§Ã£o periÃ³dica dos cookies (manual ou semi-automÃ¡tica)

**Vantagens:**
- âœ… Simples de implementar
- âœ… NÃ£o precisa de credenciais no cÃ³digo
- âœ… Funciona com 2FA (autenticaÃ§Ã£o de dois fatores)
- âœ… Mais seguro (sem senha armazenada)
- âœ… Mesma sessÃ£o em todos os sites

**Desvantagens:**
- âš ï¸ Cookies expiram (renovaÃ§Ã£o periÃ³dica necessÃ¡ria)
- âš ï¸ Precisa renovar manualmente (ou semi-automaticamente)

**ImplementaÃ§Ã£o:**

```python
import pickle
from selenium import webdriver
from selenium.webdriver.chrome.options import Options

class GoogleAuthScraper(BaseScraper):
    COOKIES_FILE = "/app/browser-profiles/google_cookies.pkl"

    def __init__(self, name, source):
        super().__init__(name, source, requires_login=True)

    async def initialize(self):
        """Load Google cookies and establish session"""
        if not self.driver:
            self.driver = self._create_driver()

        # Navigate to site first
        self.driver.get(self.BASE_URL)

        # Load saved cookies
        try:
            with open(self.COOKIES_FILE, 'rb') as f:
                cookies = pickle.load(f)

            for cookie in cookies:
                self.driver.add_cookie(cookie)

            # Refresh to apply cookies
            self.driver.refresh()

            # Verify login successful
            if not await self._verify_logged_in():
                raise Exception("Cookies expired or invalid")

            logger.success(f"{self.name} logged in successfully")

        except FileNotFoundError:
            raise Exception(
                f"Cookies file not found. Please run manual login first."
            )

    async def _verify_logged_in(self) -> bool:
        """Check if user is logged in"""
        # Each site has different indicators
        # Override in subclasses
        pass
```

**Script para salvar cookies (executar manualmente):**

```python
# save_google_cookies.py
import pickle
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
import time

def save_google_cookies():
    """
    Script para salvar cookies do Google apÃ³s login manual

    INSTRUÃ‡Ã•ES:
    1. Execute este script
    2. Uma janela do Chrome abrirÃ¡
    3. FaÃ§a login no Google manualmente
    4. Acesse fundamentei.com, investidor10.com, statusinvest.com
    5. Pressione ENTER no terminal quando terminar
    6. Cookies serÃ£o salvos automaticamente
    """

    options = Options()
    options.add_argument("--no-sandbox")
    options.add_argument("--window-size=1920,1080")

    # User data dir para persistir sessÃ£o
    options.add_argument("--user-data-dir=/app/browser-profiles/google-session")

    driver = webdriver.Chrome(options=options)

    print("=" * 60)
    print("INSTRUÃ‡Ã•ES PARA SALVAR COOKIES DO GOOGLE")
    print("=" * 60)
    print()
    print("1. Uma janela do Chrome foi aberta")
    print("2. FaÃ§a login no Google (se necessÃ¡rio)")
    print("3. Acesse os sites:")
    print("   - https://fundamentei.com/")
    print("   - https://investidor10.com.br/")
    print("   - https://statusinvest.com.br/")
    print("4. Verifique que estÃ¡ logado em todos")
    print("5. Pressione ENTER aqui quando terminar")
    print()

    # Abrir Google primeiro
    driver.get("https://accounts.google.com")

    input("Pressione ENTER apÃ³s fazer login e acessar todos os sites...")

    # Salvar cookies
    cookies = driver.get_cookies()

    with open('/app/browser-profiles/google_cookies.pkl', 'wb') as f:
        pickle.dump(cookies, f)

    print()
    print("âœ… Cookies salvos com sucesso!")
    print(f"ğŸ“ Arquivo: /app/browser-profiles/google_cookies.pkl")
    print(f"ğŸ”¢ Total de cookies: {len(cookies)}")
    print()
    print("Agora os scrapers podem usar esses cookies para login automÃ¡tico.")

    driver.quit()

if __name__ == "__main__":
    save_google_cookies()
```

**RenovaÃ§Ã£o de Cookies:**

```python
# renew_google_cookies.py
"""
Script para renovar cookies do Google periodicamente
Execute a cada 7-14 dias ou quando os scrapers comeÃ§arem a falhar
"""
import pickle
from datetime import datetime
from pathlib import Path

def check_cookies_age():
    """Verificar idade dos cookies"""
    cookies_file = Path("/app/browser-profiles/google_cookies.pkl")

    if not cookies_file.exists():
        print("âŒ Arquivo de cookies nÃ£o existe")
        return None

    age_days = (datetime.now() - datetime.fromtimestamp(
        cookies_file.stat().st_mtime
    )).days

    print(f"Idade dos cookies: {age_days} dias")

    if age_days > 7:
        print("âš ï¸  Cookies podem estar expirando em breve")
        print("   Recomendado renovar")

    return age_days

if __name__ == "__main__":
    age = check_cookies_age()

    if age and age > 14:
        print("\nğŸš¨ ATENÃ‡ÃƒO: Cookies muito antigos!")
        print("   Execute save_google_cookies.py para renovar")
```

---

### ğŸ”„ OpÃ§Ã£o 2: Selenium com Login Automatizado

**Como funciona:**
1. Scrapers fazem login automÃ¡tico usando email/senha
2. Lidam com fluxo OAuth do Google
3. Podem precisar resolver CAPTCHAs

**Vantagens:**
- âœ… Totalmente automÃ¡tico
- âœ… NÃ£o precisa renovaÃ§Ã£o manual

**Desvantagens:**
- âŒ Muito complexo com OAuth
- âŒ Google bloqueia automaÃ§Ã£o facilmente
- âŒ NÃ£o funciona com 2FA
- âŒ Credenciais no cÃ³digo (risco de seguranÃ§a)
- âŒ CAPTCHA frequente

**Status:** âŒ NÃƒO RECOMENDADO para Google OAuth

---

### ğŸ­ OpÃ§Ã£o 3: Playwright com Contexto Persistente

**Como funciona:**
1. Similar Ã  OpÃ§Ã£o 1, mas usa Playwright
2. Contexto do navegador persistente
3. Estado de login mantido entre execuÃ§Ãµes

**Vantagens:**
- âœ… Mais moderno que Selenium
- âœ… Melhor performance
- âœ… Contexto persistente nativo
- âœ… Menos detecÃ§Ã£o de bot

**Desvantagens:**
- âš ï¸ Requer adicionar Playwright ao projeto
- âš ï¸ Aprendizado de nova API

**ImplementaÃ§Ã£o:**

```python
from playwright.async_api import async_playwright
import pickle

class PlaywrightGoogleScraper:
    async def initialize(self):
        self.playwright = await async_playwright().start()

        # Usar contexto persistente
        self.context = await self.playwright.chromium.launch_persistent_context(
            user_data_dir="/app/browser-profiles/playwright-session",
            headless=True,
            args=[
                "--no-sandbox",
                "--disable-dev-shm-usage",
            ]
        )

        self.page = await self.context.new_page()

    async def scrape(self, ticker):
        await self.page.goto(f"{self.BASE_URL}{ticker}")
        # ... scraping logic
```

---

### ğŸ¤– OpÃ§Ã£o 4: APIs Oficiais (Quando DisponÃ­veis)

**Verificar se os sites oferecem APIs:**

- **Fundamentei**: âŒ Sem API pÃºblica
- **Investidor10**: âŒ Sem API pÃºblica
- **StatusInvest**: âŒ Sem API pÃºblica (mas tem endpoints internos)

**Status:** âŒ NÃ£o disponÃ­vel atualmente

---

## ğŸ¯ RecomendaÃ§Ã£o Final

### Para PRODUÃ‡ÃƒO: OpÃ§Ã£o 1 (Selenium + Cookies Salvos)

**Motivos:**
1. âœ… **Simples e confiÃ¡vel**
2. âœ… **Funciona com 2FA**
3. âœ… **Mais seguro** (sem credenciais)
4. âœ… **JÃ¡ temos Selenium** instalado
5. âœ… **FÃ¡cil manutenÃ§Ã£o**

**Workflow:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. Setup Inicial (UMA VEZ)                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ Executar save_google_cookies.py                   â”‚
â”‚ â€¢ Fazer login manual no Chrome que abre            â”‚
â”‚ â€¢ Acessar os 3 sites (Fundamentei, Inv10, Status)  â”‚
â”‚ â€¢ Pressionar ENTER                                  â”‚
â”‚ â€¢ Cookies salvos em google_cookies.pkl             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. Uso DiÃ¡rio (AUTOMÃTICO)                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ Scrapers carregam cookies automaticamente         â”‚
â”‚ â€¢ Fazem scraping como se estivesse logado          â”‚
â”‚ â€¢ Sem intervenÃ§Ã£o humana necessÃ¡ria                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. ManutenÃ§Ã£o (A CADA 7-14 DIAS)                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ Executar renew_google_cookies.py                  â”‚
â”‚ â€¢ Se idade > 14 dias: renovar cookies               â”‚
â”‚ â€¢ Repetir processo do passo 1                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“‹ Checklist de ImplementaÃ§Ã£o

### Fase 1: Setup BÃ¡sico
- [x] Criar base_scraper.py com suporte a login
- [x] Criar scrapers pÃºblicos (Fundamentus, Investsite)
- [ ] Criar save_google_cookies.py
- [ ] Criar renew_google_cookies.py
- [ ] Criar diretÃ³rio /app/browser-profiles

### Fase 2: Scrapers com OAuth
- [ ] Implementar FundamenteiScraper (com cookies)
- [ ] Implementar Investidor10Scraper (com cookies)
- [ ] Atualizar StatusInvestScraper (adicionar suporte a cookies)
- [ ] Testar login e scraping

### Fase 3: AutomaÃ§Ã£o
- [ ] Script de verificaÃ§Ã£o de cookies expirados
- [ ] NotificaÃ§Ã£o quando cookies precisam renovaÃ§Ã£o
- [ ] DocumentaÃ§Ã£o de uso para equipe

---

## ğŸ”’ SeguranÃ§a

### Armazenamento de Cookies

**LocalizaÃ§Ã£o:**
```
/app/browser-profiles/
â”œâ”€â”€ google_cookies.pkl        # Cookies salvos
â””â”€â”€ google-session/           # Session data (Playwright)
```

**ProteÃ§Ãµes:**
- âœ… Volume Docker isolado
- âœ… NÃ£o commitar no Git (.gitignore)
- âœ… PermissÃµes restritas (600)
- âœ… Criptografia em repouso (se necessÃ¡rio)

**.gitignore:**
```
browser-profiles/
*.pkl
google-session/
```

**docker-compose.yml:**
```yaml
scrapers:
  volumes:
    - ./browser-profiles:/app/browser-profiles:rw
```

---

## ğŸ§ª Testes

### Testar Cookies

```bash
# 1. Salvar cookies
docker exec -it invest_scrapers python save_google_cookies.py

# 2. Testar scraper
docker exec -it invest_scrapers python -c "
from scrapers import FundamenteiScraper
import asyncio

async def test():
    scraper = FundamenteiScraper()
    result = await scraper.scrape_with_retry('PETR4')
    print(result.to_dict())

asyncio.run(test())
"

# 3. Verificar idade dos cookies
docker exec -it invest_scrapers python renew_google_cookies.py
```

---

## ğŸ“Š ComparaÃ§Ã£o de OpÃ§Ãµes

| CritÃ©rio | OpÃ§Ã£o 1 (Cookies) | OpÃ§Ã£o 2 (Auto Login) | OpÃ§Ã£o 3 (Playwright) | OpÃ§Ã£o 4 (API) |
|----------|-------------------|----------------------|----------------------|---------------|
| **Complexidade** | ğŸŸ¢ Baixa | ğŸ”´ Alta | ğŸŸ¡ MÃ©dia | ğŸŸ¢ Baixa |
| **Confiabilidade** | ğŸŸ¢ Alta | ğŸ”´ Baixa | ğŸŸ¢ Alta | ğŸŸ¢ Alta |
| **ManutenÃ§Ã£o** | ğŸŸ¡ Manual | ğŸŸ¢ Auto | ğŸŸ¡ Manual | ğŸŸ¢ Auto |
| **SeguranÃ§a** | ğŸŸ¢ Alta | ğŸ”´ Baixa | ğŸŸ¢ Alta | ğŸŸ¢ Alta |
| **2FA Support** | ğŸŸ¢ Sim | ğŸ”´ NÃ£o | ğŸŸ¢ Sim | ğŸŸ¢ Sim |
| **Disponibilidade** | ğŸŸ¢ Agora | ğŸŸ¢ Agora | ğŸŸ¡ Adicionar lib | ğŸ”´ NÃ£o existe |
| **RECOMENDADO** | âœ… SIM | âŒ NÃƒO | ğŸŸ¡ Alternativa | âŒ N/A |

---

## ğŸ“ Exemplo Completo: FundamenteiScraper

```python
# scrapers/fundamentei_scraper.py
import pickle
from pathlib import Path
from selenium.webdriver.common.by import By
from loguru import logger

from base_scraper import BaseScraper, ScraperResult

class FundamenteiScraper(BaseScraper):
    BASE_URL = "https://fundamentei.com/acoes/"
    COOKIES_FILE = "/app/browser-profiles/google_cookies.pkl"

    def __init__(self):
        super().__init__(
            name="Fundamentei",
            source="FUNDAMENTEI",
            requires_login=True,  # Requer Google OAuth
        )

    async def initialize(self):
        """Initialize with Google OAuth cookies"""
        if self._initialized:
            return

        # Create driver
        if not self.driver:
            self.driver = self._create_driver()

        # Load Google cookies
        try:
            logger.info(f"Loading Google cookies for {self.name}")

            # Navigate to site first (cookies need domain)
            self.driver.get("https://fundamentei.com")

            # Load cookies
            with open(self.COOKIES_FILE, 'rb') as f:
                cookies = pickle.load(f)

            for cookie in cookies:
                # Only add cookies for relevant domains
                if 'fundamentei.com' in cookie.get('domain', '') or \
                   'google.com' in cookie.get('domain', ''):
                    try:
                        self.driver.add_cookie(cookie)
                    except Exception as e:
                        logger.debug(f"Could not add cookie: {e}")

            # Refresh to apply cookies
            self.driver.refresh()
            await asyncio.sleep(2)

            # Verify login
            if not await self._verify_logged_in():
                raise Exception(
                    "Login verification failed. Cookies may be expired. "
                    "Please run save_google_cookies.py"
                )

            logger.success(f"{self.name} logged in successfully via Google OAuth")
            self._initialized = True

        except FileNotFoundError:
            raise Exception(
                f"Google cookies file not found: {self.COOKIES_FILE}\n"
                f"Please run save_google_cookies.py first"
            )
        except Exception as e:
            logger.error(f"Failed to initialize {self.name}: {e}")
            raise

    async def _verify_logged_in(self) -> bool:
        """Verify that user is logged in"""
        try:
            # Look for logout button or user profile
            # Each site has different indicators
            logout_button = self.driver.find_elements(
                By.XPATH,
                "//a[contains(text(), 'Sair')] | //button[contains(text(), 'Logout')]"
            )

            if logout_button:
                return True

            # Or check for user profile/avatar
            user_profile = self.driver.find_elements(
                By.CSS_SELECTOR,
                ".user-avatar, .user-profile, [data-testid='user-menu']"
            )

            return len(user_profile) > 0

        except Exception as e:
            logger.debug(f"Login verification error: {e}")
            return False

    async def scrape(self, ticker: str) -> ScraperResult:
        """Scrape data from Fundamentei"""
        try:
            # Ensure logged in
            await self.initialize()

            # Navigate to ticker page
            url = f"{self.BASE_URL}{ticker.upper()}"
            logger.info(f"Navigating to {url}")
            self.driver.get(url)

            await asyncio.sleep(2)

            # Check if behind paywall
            if "premium" in self.driver.page_source.lower() or \
               "assine" in self.driver.page_source.lower():
                return ScraperResult(
                    success=False,
                    error="Content behind paywall",
                    source=self.source,
                )

            # Extract data
            data = await self._extract_data(ticker)

            if data:
                return ScraperResult(
                    success=True,
                    data=data,
                    source=self.source,
                    metadata={"url": url, "requires_login": True},
                )
            else:
                return ScraperResult(
                    success=False,
                    error="Failed to extract data",
                    source=self.source,
                )

        except Exception as e:
            logger.error(f"Error scraping {ticker}: {e}")
            return ScraperResult(
                success=False,
                error=str(e),
                source=self.source,
            )

    async def _extract_data(self, ticker: str):
        """Extract data from Fundamentei page"""
        # ... implementation specific to Fundamentei's HTML structure
        pass
```

---

## ğŸ” ATUALIZAÃ‡ÃƒO 2025-11-24: AnÃ¡lise Ultra-Robusta Completa

### ğŸ“Š **Duas ImplementaÃ§Ãµes Paralelas Identificadas**

Durante anÃ¡lise completa do fluxo OAuth (2025-11-24), identificamos que o projeto possui **DUAS implementaÃ§Ãµes paralelas** de scrapers:

#### **1. Scrapers PYTHON (Selenium)** - âœ… **FUNCIONA HOJE**

```
LocalizaÃ§Ã£o: backend/python-scrapers/scrapers/fundamentei_scraper.py
Tecnologia: Selenium WebDriver
Cookies: /app/browser-profiles/google_cookies.pkl (PICKLE format)
Volume Docker: ./browser-profiles:/app/browser-profiles (docker-compose.yml:234)
OAuth Manager: backend/python-scrapers/oauth_session_manager.py
Status: âœ… OAuth 100% funcional
EvidÃªncia: google_cookies.pkl existe (9.9KB, atualizado 2025-11-23)
```

**Fluxo Completo (Python):**
```
1. Frontend OAuth Manager (/oauth-manager) â†’ Abre VNC viewer
2. UsuÃ¡rio faz login manual via VNC (21 sites, ~18 minutos)
3. oauth_session_manager.py coleta cookies via Playwright
4. Salva: /app/browser-profiles/google_cookies.pkl (pickle.dump)
5. Docker volume montado: ./browser-profiles:/app/browser-profiles
6. Python scrapers carregam: pickle.load(google_cookies.pkl)
7. âœ… Login automÃ¡tico em Fundamentei/Investidor10/StatusInvest
```

**Sites Suportados (21 total):**
- âœ… Google (base OAuth)
- âœ… Fundamentei
- âœ… Investidor10
- âœ… StatusInvest
- âœ… Investing.com
- âœ… ADVFN
- âœ… Google Finance
- âœ… TradingView
- âœ… ChatGPT, Gemini, DeepSeek, Claude, Grok (AI)
- âœ… Valor, Exame, InfoMoney, EstadÃ£o, Mais Retorno (notÃ­cias)
- âœ… MyProfit Web, Kinvo (portfÃ³lio)

**ConfiguraÃ§Ã£o:** `backend/python-scrapers/oauth_sites_config.py`

---

#### **2. Scrapers TYPESCRIPT/NestJS (Puppeteer)** - âŒ **NÃƒO FUNCIONA**

```
LocalizaÃ§Ã£o: backend/src/scrapers/fundamental/fundamentei.scraper.ts
Tecnologia: Puppeteer (via @nestjs/puppeteer)
Cookies esperados: data/cookies/fundamentei_session.json (JSON format)
Helper: backend/src/scrapers/auth/google-auth.helper.ts
Status: âŒ Cookies JSON NÃƒO EXISTEM
Problema: CÃ³digo espera JSON mas OAuth Manager salva PICKLE
```

**CÃ³digo TypeScript Atual:**
```typescript
// fundamentei.scraper.ts:30-35
private readonly cookiesPath = path.join(
  process.cwd(),
  'data',
  'cookies',
  'fundamentei_session.json',  // âŒ Arquivo nÃ£o existe
);

// google-auth.helper.ts:15-18
const cookiesString = fs.readFileSync(filePath, 'utf8');
const cookies = JSON.parse(cookiesString);  // âŒ Espera JSON
await page.setCookie(...cookies);
```

**Resultado:** Login falha com erro:
```
NO VALID OAUTH SESSION FOUND FOR FUNDAMENTEI
Please complete OAuth login at http://localhost:3100/oauth-manager
```

---

### ğŸš¨ **GAP IDENTIFICADO: Falta ConversÃ£o Pickle â†’ JSON**

**SituaÃ§Ã£o Atual:**
1. âœ… Python OAuth Manager salva cookies em **PICKLE** (`google_cookies.pkl`)
2. âœ… Python scrapers leem **PICKLE** diretamente (funcionando)
3. âŒ TypeScript scrapers esperam **JSON** (`{site}_session.json`)
4. âŒ **Nenhuma conversÃ£o automÃ¡tica** entre formatos
5. âŒ DiretÃ³rio `data/cookies/` estÃ¡ **vazio**

**Impacto:**
- Scrapers Python: âœ… Funcionam 100%
- Scrapers TypeScript: âŒ NÃ£o funcionam (sem cookies)

---

### âœ… **SoluÃ§Ã£o: Script Conversor Opcional**

Criar script de conversÃ£o **apenas se** scrapers TypeScript forem necessÃ¡rios:

```python
# backend/python-scrapers/convert_cookies_to_json.py
"""
Converter cookies pickle para JSON (TypeScript/Puppeteer)
Execute apenas se scrapers TypeScript/NestJS forem usados
"""
import pickle
import json
from pathlib import Path

PICKLE_FILE = Path("/app/browser-profiles/google_cookies.pkl")
JSON_OUTPUT_DIR = Path("/app/data/cookies")

def convert_cookies_pickle_to_json():
    """Convert pickle cookies to JSON per site"""
    # Load pickle
    with open(PICKLE_FILE, 'rb') as f:
        all_cookies = pickle.load(f)  # Dict[site_name, List[cookie]]

    # Ensure output dir exists
    JSON_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

    # Convert each site
    site_mapping = {
        "Fundamentei": "fundamentei_session.json",
        "Investidor10": "investidor10_session.json",
        "StatusInvest": "statusinvest_session.json",
    }

    for site_name, json_filename in site_mapping.items():
        if site_name in all_cookies:
            cookies = all_cookies[site_name]

            # Save as JSON
            output_file = JSON_OUTPUT_DIR / json_filename
            with open(output_file, 'w') as f:
                json.dump(cookies, f, indent=2)

            print(f"âœ… {site_name}: {len(cookies)} cookies â†’ {json_filename}")
        else:
            print(f"âš ï¸  {site_name}: No cookies found in pickle")

if __name__ == "__main__":
    convert_cookies_pickle_to_json()
```

**Uso:**
```bash
# Executar apÃ³s coletar cookies via OAuth Manager
docker exec invest_python_service python /app/convert_cookies_to_json.py
```

---

### ğŸ¯ **Diagrama de DecisÃ£o: Qual Scraper Usar?**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Precisa de OAuth Google (Fundamentei, Investidor10, etc)?  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                           â”‚
    âœ… SIM                       âŒ NÃƒO
        â”‚                           â”‚
        v                           v
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Python Scraper    â”‚     â”‚ TypeScript ou    â”‚
â”‚ (Selenium)        â”‚     â”‚ Python (qualquer)â”‚
â”‚                   â”‚     â”‚                  â”‚
â”‚ âœ… Usa pickle     â”‚     â”‚ âœ… Sem login     â”‚
â”‚ âœ… JÃ¡ funciona    â”‚     â”‚ âœ… HTTP direto   â”‚
â”‚ âœ… Sem conversÃ£o  â”‚     â”‚ âœ… APIs pÃºblicas â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚
        â”‚ (se REALMENTE precisar TypeScript com OAuth)
        v
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Executar script conversor:               â”‚
â”‚ python convert_cookies_to_json.py        â”‚
â”‚                                          â”‚
â”‚ Resultado:                               â”‚
â”‚ âœ… data/cookies/fundamentei_session.json â”‚
â”‚ âœ… TypeScript scrapers funcionam         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**RecomendaÃ§Ã£o:**
- **Scrapers OAuth**: Use Python (Selenium) - jÃ¡ funciona
- **Scrapers pÃºblicos**: Use TypeScript ou Python - ambos OK
- **ConversÃ£o pickleâ†’JSON**: Apenas se TypeScript OAuth for necessÃ¡rio

---

### ğŸ“ **Volumes Docker e Paths Completos**

**docker-compose.yml (linhas 231-236):**
```yaml
api-service:  # Python Service
  volumes:
    - ./backend/python-scrapers:/app
    - ./browser-profiles:/app/browser-profiles  # â† OAuth cookies (pickle)
    - ./logs:/app/logs
    - ./data:/app/data                          # â† JSON cookies (se converter)
```

**Paths no Container:**
```
/app/browser-profiles/google_cookies.pkl         # âœ… Pickle (Python OAuth Manager)
/app/data/cookies/fundamentei_session.json       # âŒ JSON (nÃ£o existe ainda)
/app/data/cookies/investidor10_session.json      # âŒ JSON (nÃ£o existe ainda)
/app/data/cookies/statusinvest_session.json      # âŒ JSON (nÃ£o existe ainda)
```

**Paths no Host (Windows):**
```
C:\...\invest-claude-web\browser-profiles\google_cookies.pkl  # âœ… Existe (9.9KB)
C:\...\invest-claude-web\backend\data\cookies\                # âš ï¸  Vazio
```

---

### ğŸ“Š **Status Atual (2025-11-24)**

| Item | Python (Selenium) | TypeScript (Puppeteer) |
|------|-------------------|------------------------|
| **OAuth Manager** | âœ… Implementado | âŒ N/A |
| **Coleta de Cookies** | âœ… VNC + Playwright | âŒ N/A |
| **Formato Cookies** | âœ… Pickle | âŒ JSON (nÃ£o existe) |
| **Scrapers Implementados** | âœ… Fundamentei, Investidor10, StatusInvest | âœ… CÃ³digo existe (sem cookies) |
| **Login AutomÃ¡tico** | âœ… Funciona 100% | âŒ Falha (sem JSON) |
| **ConversÃ£o Pickleâ†’JSON** | âŒ NÃ£o implementado | âŒ NecessÃ¡rio para funcionar |
| **RecomendaÃ§Ã£o** | âœ… **USAR ESTE** | âš ï¸  SÃ³ se realmente necessÃ¡rio |

**Scrapers PÃºblicos (sem OAuth):**
- âœ… Fundamentus (Python)
- âœ… Investsite (Python)
- âœ… BRAPI (TypeScript/Axios) - API pÃºblica

---

### ğŸ“ **Boas PrÃ¡ticas: PadrÃ£o para Novos Scrapers**

**1. Scrapers que REQUEREM Google OAuth:**
```
âœ… Usar: Python (Selenium)
âœ… Cookies: google_cookies.pkl (jÃ¡ existe)
âœ… Base class: BaseScraper (backend/python-scrapers/base_scraper.py)
âœ… Exemplo: fundamentei_scraper.py
```

**2. Scrapers PÃºblicos (sem login):**
```
âœ… Usar: TypeScript (NestJS) ou Python
âœ… Protocolo: HTTP/HTTPS direto (Axios/fetch)
âœ… Exemplo: brapi.scraper.ts (TypeScript)
âœ… Exemplo: fundamentus_scraper.py (Python)
```

**3. Scrapers com Login PrÃ³prio (nÃ£o OAuth):**
```
âœ… Usar: Python (Selenium) ou TypeScript (Puppeteer)
âœ… Credenciais: .env (SITE_USERNAME, SITE_PASSWORD)
âœ… Exemplo: opcoes.scraper.ts (user/password)
```

**4. APIs Oficiais:**
```
âœ… Usar: TypeScript (NestJS) - preferÃ­vel
âœ… Protocolo: REST/GraphQL via Axios
âœ… Exemplo: brapi.scraper.ts
```

---

### ğŸ”’ **SeguranÃ§a: Cookies e Credenciais**

**Arquivos SensÃ­veis (NÃƒO commitar):**
```gitignore
# .gitignore (jÃ¡ configurado)
browser-profiles/
*.pkl
data/cookies/
google-session/
```

**PermissÃµes Recomendadas:**
```bash
chmod 600 browser-profiles/google_cookies.pkl
chmod 700 browser-profiles/
chmod 700 data/cookies/
```

**RenovaÃ§Ã£o de Cookies:**
- **FrequÃªncia:** A cada 7-14 dias (cookies Google expiram)
- **Como:** Repetir fluxo OAuth Manager (/oauth-manager)
- **AutomaÃ§Ã£o:** PossÃ­vel via cron job semanal (futuro)

---

### âœ… **Checklist de ImplementaÃ§Ã£o (Atualizado 2025-11-24)**

#### **Fase 1: Setup BÃ¡sico (CONCLUÃDO)**
- [x] Criar base_scraper.py com suporte a login
- [x] Criar scrapers pÃºblicos (Fundamentus, Investsite)
- [x] Criar oauth_session_manager.py (OAuth Manager)
- [x] Criar oauth_sites_config.py (21 sites)
- [x] Criar diretÃ³rio /app/browser-profiles
- [x] Implementar VNC viewer no frontend
- [x] Coletar cookies via OAuth Manager (9.9KB pickle)

#### **Fase 2: Scrapers com OAuth (CONCLUÃDO - Python)**
- [x] Implementar FundamenteiScraper (Python/pickle)
- [x] Implementar Investidor10Scraper (Python/pickle)
- [x] Atualizar StatusInvestScraper (Python/pickle)
- [x] Testar login e scraping (âœ… funcionando)

#### **Fase 3: TypeScript (OPCIONAL - se necessÃ¡rio)**
- [ ] Criar convert_cookies_to_json.py (se usar TypeScript OAuth)
- [ ] Converter pickle â†’ JSON para sites especÃ­ficos
- [ ] Testar TypeScript scrapers com JSON
- [ ] Documentar novo fluxo (se implementado)

#### **Fase 4: AutomaÃ§Ã£o (FUTURO)**
- [ ] Script de verificaÃ§Ã£o de cookies expirados
- [ ] NotificaÃ§Ã£o quando cookies precisam renovaÃ§Ã£o
- [ ] Cron job semanal para renovaÃ§Ã£o automÃ¡tica
- [ ] Dashboard de status de cookies

---

## âœ… ConclusÃ£o

**IMPLEMENTAR AGORA:**
1. âœ… Scrapers pÃºblicos (Fundamentus, Investsite) - **PRONTOS**
2. âœ… Script save_google_cookies.py â†’ **OAuth Manager (VNC)** - **PRONTO**
3. âœ… Scrapers com OAuth (Fundamentei, Investidor10) - **PRONTOS (Python)**

**STATUS FINAL (2025-11-24):**
- **Fundamentus**: âœ… Implementado (Python, sem login)
- **Investsite**: âœ… Implementado (Python, sem login)
- **StatusInvest**: âœ… Implementado (Python, com OAuth pickle)
- **Fundamentei**: âœ… Implementado (Python, com OAuth pickle)
- **Investidor10**: âœ… Implementado (Python, com OAuth pickle)
- **BRAPI**: âœ… Implementado (TypeScript, API pÃºblica)

**OBSERVAÃ‡Ã•ES IMPORTANTES:**
1. **Python scrapers (OAuth)**: Totalmente funcionais âœ…
2. **TypeScript scrapers (OAuth)**: CÃ³digo existe mas sem cookies âŒ
3. **ConversÃ£o pickleâ†’JSON**: Apenas se TypeScript OAuth for necessÃ¡rio
4. **RecomendaÃ§Ã£o**: Continuar usando Python para scrapers OAuth

**PRÃ“XIMOS SCRAPERS (31 fontes planejadas):**
- Usar Python para sites com OAuth/login
- Usar TypeScript para APIs pÃºblicas
- Documentar mÃ©todo escolhido em cada implementaÃ§Ã£o

---

**Ãšltima atualizaÃ§Ã£o:** 2025-11-24 (AnÃ¡lise Ultra-Robusta Completa)
**PrÃ³xima revisÃ£o:** ApÃ³s implementaÃ§Ã£o de novos scrapers (verificar padrÃµes)
