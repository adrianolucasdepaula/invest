# üîê Estrat√©gia para Scrapers com Google OAuth

**Criado:** 2025-11-07
**Status:** Documento de Planejamento

---

## üìã Fontes que Requerem Google OAuth

Voc√™ acessa essas fontes usando autentica√ß√£o Google:

1. **Fundamentei** - https://fundamentei.com/
2. **Investidor10** - https://investidor10.com.br/
3. **StatusInvest** - https://statusinvest.com.br/

---

## üéØ Op√ß√µes de Implementa√ß√£o

### ‚úÖ Op√ß√£o 1: Selenium com Cookies Salvos (RECOMENDADO)

**Como funciona:**
1. Voc√™ faz login manualmente UMA VEZ no seu navegador
2. Exportamos os cookies da sess√£o autenticada
3. Scrapers carregam esses cookies para manter a sess√£o ativa
4. Renova√ß√£o peri√≥dica dos cookies (manual ou semi-autom√°tica)

**Vantagens:**
- ‚úÖ Simples de implementar
- ‚úÖ N√£o precisa de credenciais no c√≥digo
- ‚úÖ Funciona com 2FA (autentica√ß√£o de dois fatores)
- ‚úÖ Mais seguro (sem senha armazenada)
- ‚úÖ Mesma sess√£o em todos os sites

**Desvantagens:**
- ‚ö†Ô∏è Cookies expiram (renova√ß√£o peri√≥dica necess√°ria)
- ‚ö†Ô∏è Precisa renovar manualmente (ou semi-automaticamente)

**Implementa√ß√£o:**

```python
import pickle
from selenium import webdriver
from selenium.webdriver.chrome.options import Options

class GoogleAuthScraper(BaseScraper):
    COOKIES_FILE = "/app/browser-profiles/google_cookies.pkl"

    def __init__(self, name, source):
        super().__init__(name, source, requires_login=True)

    async def initialize(self):
        """Load Google cookies and establish session"""
        if not self.driver:
            self.driver = self._create_driver()

        # Navigate to site first
        self.driver.get(self.BASE_URL)

        # Load saved cookies
        try:
            with open(self.COOKIES_FILE, 'rb') as f:
                cookies = pickle.load(f)

            for cookie in cookies:
                self.driver.add_cookie(cookie)

            # Refresh to apply cookies
            self.driver.refresh()

            # Verify login successful
            if not await self._verify_logged_in():
                raise Exception("Cookies expired or invalid")

            logger.success(f"{self.name} logged in successfully")

        except FileNotFoundError:
            raise Exception(
                f"Cookies file not found. Please run manual login first."
            )

    async def _verify_logged_in(self) -> bool:
        """Check if user is logged in"""
        # Each site has different indicators
        # Override in subclasses
        pass
```

**Script para salvar cookies (executar manualmente):**

```python
# save_google_cookies.py
import pickle
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
import time

def save_google_cookies():
    """
    Script para salvar cookies do Google ap√≥s login manual

    INSTRU√á√ïES:
    1. Execute este script
    2. Uma janela do Chrome abrir√°
    3. Fa√ßa login no Google manualmente
    4. Acesse fundamentei.com, investidor10.com, statusinvest.com
    5. Pressione ENTER no terminal quando terminar
    6. Cookies ser√£o salvos automaticamente
    """

    options = Options()
    options.add_argument("--no-sandbox")
    options.add_argument("--window-size=1920,1080")

    # User data dir para persistir sess√£o
    options.add_argument("--user-data-dir=/app/browser-profiles/google-session")

    driver = webdriver.Chrome(options=options)

    print("=" * 60)
    print("INSTRU√á√ïES PARA SALVAR COOKIES DO GOOGLE")
    print("=" * 60)
    print()
    print("1. Uma janela do Chrome foi aberta")
    print("2. Fa√ßa login no Google (se necess√°rio)")
    print("3. Acesse os sites:")
    print("   - https://fundamentei.com/")
    print("   - https://investidor10.com.br/")
    print("   - https://statusinvest.com.br/")
    print("4. Verifique que est√° logado em todos")
    print("5. Pressione ENTER aqui quando terminar")
    print()

    # Abrir Google primeiro
    driver.get("https://accounts.google.com")

    input("Pressione ENTER ap√≥s fazer login e acessar todos os sites...")

    # Salvar cookies
    cookies = driver.get_cookies()

    with open('/app/browser-profiles/google_cookies.pkl', 'wb') as f:
        pickle.dump(cookies, f)

    print()
    print("‚úÖ Cookies salvos com sucesso!")
    print(f"üìÅ Arquivo: /app/browser-profiles/google_cookies.pkl")
    print(f"üî¢ Total de cookies: {len(cookies)}")
    print()
    print("Agora os scrapers podem usar esses cookies para login autom√°tico.")

    driver.quit()

if __name__ == "__main__":
    save_google_cookies()
```

**Renova√ß√£o de Cookies:**

```python
# renew_google_cookies.py
"""
Script para renovar cookies do Google periodicamente
Execute a cada 7-14 dias ou quando os scrapers come√ßarem a falhar
"""
import pickle
from datetime import datetime
from pathlib import Path

def check_cookies_age():
    """Verificar idade dos cookies"""
    cookies_file = Path("/app/browser-profiles/google_cookies.pkl")

    if not cookies_file.exists():
        print("‚ùå Arquivo de cookies n√£o existe")
        return None

    age_days = (datetime.now() - datetime.fromtimestamp(
        cookies_file.stat().st_mtime
    )).days

    print(f"Idade dos cookies: {age_days} dias")

    if age_days > 7:
        print("‚ö†Ô∏è  Cookies podem estar expirando em breve")
        print("   Recomendado renovar")

    return age_days

if __name__ == "__main__":
    age = check_cookies_age()

    if age and age > 14:
        print("\nüö® ATEN√á√ÉO: Cookies muito antigos!")
        print("   Execute save_google_cookies.py para renovar")
```

---

### üîÑ Op√ß√£o 2: Selenium com Login Automatizado

**Como funciona:**
1. Scrapers fazem login autom√°tico usando email/senha
2. Lidam com fluxo OAuth do Google
3. Podem precisar resolver CAPTCHAs

**Vantagens:**
- ‚úÖ Totalmente autom√°tico
- ‚úÖ N√£o precisa renova√ß√£o manual

**Desvantagens:**
- ‚ùå Muito complexo com OAuth
- ‚ùå Google bloqueia automa√ß√£o facilmente
- ‚ùå N√£o funciona com 2FA
- ‚ùå Credenciais no c√≥digo (risco de seguran√ßa)
- ‚ùå CAPTCHA frequente

**Status:** ‚ùå N√ÉO RECOMENDADO para Google OAuth

---

### üé≠ Op√ß√£o 3: Playwright com Contexto Persistente

**Como funciona:**
1. Similar √† Op√ß√£o 1, mas usa Playwright
2. Contexto do navegador persistente
3. Estado de login mantido entre execu√ß√µes

**Vantagens:**
- ‚úÖ Mais moderno que Selenium
- ‚úÖ Melhor performance
- ‚úÖ Contexto persistente nativo
- ‚úÖ Menos detec√ß√£o de bot

**Desvantagens:**
- ‚ö†Ô∏è Requer adicionar Playwright ao projeto
- ‚ö†Ô∏è Aprendizado de nova API

**Implementa√ß√£o:**

```python
from playwright.async_api import async_playwright
import pickle

class PlaywrightGoogleScraper:
    async def initialize(self):
        self.playwright = await async_playwright().start()

        # Usar contexto persistente
        self.context = await self.playwright.chromium.launch_persistent_context(
            user_data_dir="/app/browser-profiles/playwright-session",
            headless=True,
            args=[
                "--no-sandbox",
                "--disable-dev-shm-usage",
            ]
        )

        self.page = await self.context.new_page()

    async def scrape(self, ticker):
        await self.page.goto(f"{self.BASE_URL}{ticker}")
        # ... scraping logic
```

---

### ü§ñ Op√ß√£o 4: APIs Oficiais (Quando Dispon√≠veis)

**Verificar se os sites oferecem APIs:**

- **Fundamentei**: ‚ùå Sem API p√∫blica
- **Investidor10**: ‚ùå Sem API p√∫blica
- **StatusInvest**: ‚ùå Sem API p√∫blica (mas tem endpoints internos)

**Status:** ‚ùå N√£o dispon√≠vel atualmente

---

## üéØ Recomenda√ß√£o Final

### Para PRODU√á√ÉO: Op√ß√£o 1 (Selenium + Cookies Salvos)

**Motivos:**
1. ‚úÖ **Simples e confi√°vel**
2. ‚úÖ **Funciona com 2FA**
3. ‚úÖ **Mais seguro** (sem credenciais)
4. ‚úÖ **J√° temos Selenium** instalado
5. ‚úÖ **F√°cil manuten√ß√£o**

**Workflow:**

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 1. Setup Inicial (UMA VEZ)                         ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ ‚Ä¢ Executar save_google_cookies.py                   ‚îÇ
‚îÇ ‚Ä¢ Fazer login manual no Chrome que abre            ‚îÇ
‚îÇ ‚Ä¢ Acessar os 3 sites (Fundamentei, Inv10, Status)  ‚îÇ
‚îÇ ‚Ä¢ Pressionar ENTER                                  ‚îÇ
‚îÇ ‚Ä¢ Cookies salvos em google_cookies.pkl             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                      ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 2. Uso Di√°rio (AUTOM√ÅTICO)                         ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ ‚Ä¢ Scrapers carregam cookies automaticamente         ‚îÇ
‚îÇ ‚Ä¢ Fazem scraping como se estivesse logado          ‚îÇ
‚îÇ ‚Ä¢ Sem interven√ß√£o humana necess√°ria                ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                      ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ 3. Manuten√ß√£o (A CADA 7-14 DIAS)                   ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ ‚Ä¢ Executar renew_google_cookies.py                  ‚îÇ
‚îÇ ‚Ä¢ Se idade > 14 dias: renovar cookies               ‚îÇ
‚îÇ ‚Ä¢ Repetir processo do passo 1                       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üìã Checklist de Implementa√ß√£o

### Fase 1: Setup B√°sico
- [x] Criar base_scraper.py com suporte a login
- [x] Criar scrapers p√∫blicos (Fundamentus, Investsite)
- [ ] Criar save_google_cookies.py
- [ ] Criar renew_google_cookies.py
- [ ] Criar diret√≥rio /app/browser-profiles

### Fase 2: Scrapers com OAuth
- [ ] Implementar FundamenteiScraper (com cookies)
- [ ] Implementar Investidor10Scraper (com cookies)
- [ ] Atualizar StatusInvestScraper (adicionar suporte a cookies)
- [ ] Testar login e scraping

### Fase 3: Automa√ß√£o
- [ ] Script de verifica√ß√£o de cookies expirados
- [ ] Notifica√ß√£o quando cookies precisam renova√ß√£o
- [ ] Documenta√ß√£o de uso para equipe

---

## üîí Seguran√ßa

### Armazenamento de Cookies

**Localiza√ß√£o:**
```
/app/browser-profiles/
‚îú‚îÄ‚îÄ google_cookies.pkl        # Cookies salvos
‚îî‚îÄ‚îÄ google-session/           # Session data (Playwright)
```

**Prote√ß√µes:**
- ‚úÖ Volume Docker isolado
- ‚úÖ N√£o commitar no Git (.gitignore)
- ‚úÖ Permiss√µes restritas (600)
- ‚úÖ Criptografia em repouso (se necess√°rio)

**.gitignore:**
```
browser-profiles/
*.pkl
google-session/
```

**docker-compose.yml:**
```yaml
scrapers:
  volumes:
    - ./browser-profiles:/app/browser-profiles:rw
```

---

## üß™ Testes

### Testar Cookies

```bash
# 1. Salvar cookies
docker exec -it invest_scrapers python save_google_cookies.py

# 2. Testar scraper
docker exec -it invest_scrapers python -c "
from scrapers import FundamenteiScraper
import asyncio

async def test():
    scraper = FundamenteiScraper()
    result = await scraper.scrape_with_retry('PETR4')
    print(result.to_dict())

asyncio.run(test())
"

# 3. Verificar idade dos cookies
docker exec -it invest_scrapers python renew_google_cookies.py
```

---

## üìä Compara√ß√£o de Op√ß√µes

| Crit√©rio | Op√ß√£o 1 (Cookies) | Op√ß√£o 2 (Auto Login) | Op√ß√£o 3 (Playwright) | Op√ß√£o 4 (API) |
|----------|-------------------|----------------------|----------------------|---------------|
| **Complexidade** | üü¢ Baixa | üî¥ Alta | üü° M√©dia | üü¢ Baixa |
| **Confiabilidade** | üü¢ Alta | üî¥ Baixa | üü¢ Alta | üü¢ Alta |
| **Manuten√ß√£o** | üü° Manual | üü¢ Auto | üü° Manual | üü¢ Auto |
| **Seguran√ßa** | üü¢ Alta | üî¥ Baixa | üü¢ Alta | üü¢ Alta |
| **2FA Support** | üü¢ Sim | üî¥ N√£o | üü¢ Sim | üü¢ Sim |
| **Disponibilidade** | üü¢ Agora | üü¢ Agora | üü° Adicionar lib | üî¥ N√£o existe |
| **RECOMENDADO** | ‚úÖ SIM | ‚ùå N√ÉO | üü° Alternativa | ‚ùå N/A |

---

## üéì Exemplo Completo: FundamenteiScraper

```python
# scrapers/fundamentei_scraper.py
import pickle
from pathlib import Path
from selenium.webdriver.common.by import By
from loguru import logger

from base_scraper import BaseScraper, ScraperResult

class FundamenteiScraper(BaseScraper):
    BASE_URL = "https://fundamentei.com/acoes/"
    COOKIES_FILE = "/app/browser-profiles/google_cookies.pkl"

    def __init__(self):
        super().__init__(
            name="Fundamentei",
            source="FUNDAMENTEI",
            requires_login=True,  # Requer Google OAuth
        )

    async def initialize(self):
        """Initialize with Google OAuth cookies"""
        if self._initialized:
            return

        # Create driver
        if not self.driver:
            self.driver = self._create_driver()

        # Load Google cookies
        try:
            logger.info(f"Loading Google cookies for {self.name}")

            # Navigate to site first (cookies need domain)
            self.driver.get("https://fundamentei.com")

            # Load cookies
            with open(self.COOKIES_FILE, 'rb') as f:
                cookies = pickle.load(f)

            for cookie in cookies:
                # Only add cookies for relevant domains
                if 'fundamentei.com' in cookie.get('domain', '') or \
                   'google.com' in cookie.get('domain', ''):
                    try:
                        self.driver.add_cookie(cookie)
                    except Exception as e:
                        logger.debug(f"Could not add cookie: {e}")

            # Refresh to apply cookies
            self.driver.refresh()
            await asyncio.sleep(2)

            # Verify login
            if not await self._verify_logged_in():
                raise Exception(
                    "Login verification failed. Cookies may be expired. "
                    "Please run save_google_cookies.py"
                )

            logger.success(f"{self.name} logged in successfully via Google OAuth")
            self._initialized = True

        except FileNotFoundError:
            raise Exception(
                f"Google cookies file not found: {self.COOKIES_FILE}\n"
                f"Please run save_google_cookies.py first"
            )
        except Exception as e:
            logger.error(f"Failed to initialize {self.name}: {e}")
            raise

    async def _verify_logged_in(self) -> bool:
        """Verify that user is logged in"""
        try:
            # Look for logout button or user profile
            # Each site has different indicators
            logout_button = self.driver.find_elements(
                By.XPATH,
                "//a[contains(text(), 'Sair')] | //button[contains(text(), 'Logout')]"
            )

            if logout_button:
                return True

            # Or check for user profile/avatar
            user_profile = self.driver.find_elements(
                By.CSS_SELECTOR,
                ".user-avatar, .user-profile, [data-testid='user-menu']"
            )

            return len(user_profile) > 0

        except Exception as e:
            logger.debug(f"Login verification error: {e}")
            return False

    async def scrape(self, ticker: str) -> ScraperResult:
        """Scrape data from Fundamentei"""
        try:
            # Ensure logged in
            await self.initialize()

            # Navigate to ticker page
            url = f"{self.BASE_URL}{ticker.upper()}"
            logger.info(f"Navigating to {url}")
            self.driver.get(url)

            await asyncio.sleep(2)

            # Check if behind paywall
            if "premium" in self.driver.page_source.lower() or \
               "assine" in self.driver.page_source.lower():
                return ScraperResult(
                    success=False,
                    error="Content behind paywall",
                    source=self.source,
                )

            # Extract data
            data = await self._extract_data(ticker)

            if data:
                return ScraperResult(
                    success=True,
                    data=data,
                    source=self.source,
                    metadata={"url": url, "requires_login": True},
                )
            else:
                return ScraperResult(
                    success=False,
                    error="Failed to extract data",
                    source=self.source,
                )

        except Exception as e:
            logger.error(f"Error scraping {ticker}: {e}")
            return ScraperResult(
                success=False,
                error=str(e),
                source=self.source,
            )

    async def _extract_data(self, ticker: str):
        """Extract data from Fundamentei page"""
        # ... implementation specific to Fundamentei's HTML structure
        pass
```

---

## ‚úÖ Conclus√£o

**IMPLEMENTAR AGORA:**
1. ‚úÖ Scrapers p√∫blicos (Fundamentus, Investsite) - PRONTOS
2. üîÑ Script save_google_cookies.py - A CRIAR
3. üîÑ Scrapers com OAuth (Fundamentei, Investidor10) - A CRIAR

**STATUS:**
- Fundamentus: ‚úÖ Implementado (sem login)
- Investsite: ‚úÖ Implementado (sem login)
- StatusInvest: üü° B√°sico implementado (pode melhorar com cookies)
- Fundamentei: ‚è≥ Aguardando cookies
- Investidor10: ‚è≥ Aguardando cookies

---

**√öltima atualiza√ß√£o:** 2025-11-07
**Pr√≥xima revis√£o:** Ap√≥s testes com cookies
