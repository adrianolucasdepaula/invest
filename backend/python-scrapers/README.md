# Python Scrapers Service

ServiÃ§o de scrapers Python para coletar dados financeiros de mÃºltiplas fontes.

## ğŸ“‹ VisÃ£o Geral

Este serviÃ§o complementa os scrapers TypeScript do backend NestJS, oferecendo:
- **Selenium WebDriver** para sites com JavaScript pesado
- **ExecuÃ§Ã£o assÃ­ncrona** com retry automÃ¡tico
- **IntegraÃ§Ã£o Redis** para job queues
- **PostgreSQL** para persistÃªncia de dados
- **Logging estruturado** com Loguru

## ğŸ—ï¸ Arquitetura

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Backend   â”‚â”€â”€â”€â”€â–¶â”‚    Redis    â”‚â—€â”€â”€â”€â”€â”‚   Python    â”‚
â”‚  (NestJS)   â”‚     â”‚   (Queue)   â”‚     â”‚  Scrapers   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                    â”‚                    â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                      â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
                      â”‚ PostgreSQL â”‚
                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Fluxo de Dados

1. **Backend NestJS** envia job para Redis: `lpush scraper:jobs <job>`
2. **Python Scrapers** escuta a fila e processa jobs
3. **Scraper executa** com retry automÃ¡tico
4. **Resultado salvo** no PostgreSQL
5. **Evento publicado** no Redis: `publish scraper:results <result>`

## ğŸ“ Estrutura de Arquivos

```
backend/python-scrapers/
â”œâ”€â”€ Dockerfile              # Imagem Docker com Python + Chrome
â”œâ”€â”€ requirements.txt        # DependÃªncias Python
â”œâ”€â”€ config.py              # ConfiguraÃ§Ãµes (env vars)
â”œâ”€â”€ database.py            # Cliente PostgreSQL
â”œâ”€â”€ redis_client.py        # Cliente Redis
â”œâ”€â”€ base_scraper.py        # Classe base abstrata
â”œâ”€â”€ main.py                # ServiÃ§o principal
â””â”€â”€ scrapers/              # Scrapers especÃ­ficos
    â”œâ”€â”€ __init__.py
    â””â”€â”€ statusinvest_scraper.py
```

## ğŸš€ Como Usar

### Executar com Docker Compose (Recomendado)

```bash
# Build e start de todos os serviÃ§os
docker-compose up -d --build

# Ver logs dos scrapers
docker-compose logs -f scrapers

# Status
docker-compose ps scrapers
```

### Executar Localmente (Desenvolvimento)

```bash
# 1. Instalar dependÃªncias
pip install -r requirements.txt

# 2. Configurar variÃ¡veis de ambiente
export DB_HOST=localhost
export DB_PORT=5532
export REDIS_HOST=localhost
export REDIS_PORT=6479

# 3. Executar
python main.py
```

## ğŸ“¤ Enviar Job de Scraping

### Via Redis CLI

```bash
redis-cli -p 6479
LPUSH scraper:jobs '{"ticker":"PETR4","source":"STATUSINVEST","job_id":"123"}'
```

### Via Python

```python
import redis
import json

client = redis.Redis(host='localhost', port=6479, decode_responses=True)

job = {
    "ticker": "PETR4",
    "source": "STATUSINVEST",
    "job_id": "uuid-here",
    "timestamp": "2024-01-01T00:00:00"
}

client.lpush("scraper:jobs", json.dumps(job))
```

### Via Backend NestJS

```typescript
@Injectable()
export class ScrapersService {
  constructor(@InjectQueue('scrapers') private queue: Queue) {}

  async scrapeStock(ticker: string) {
    await this.queue.add('scrape', {
      ticker,
      source: 'STATUSINVEST',
      job_id: uuidv4(),
      timestamp: new Date().toISOString(),
    });
  }
}
```

## ğŸ” Scrapers DisponÃ­veis

### StatusInvest

Coleta dados fundamentalistas:
- PreÃ§o atual
- Dividend Yield (DY)
- P/L (Price/Earnings)
- P/VP (Price/Book Value)
- ROE, ROIC
- Liquidez
- Valor de mercado

**Uso:**
```json
{
  "ticker": "PETR4",
  "source": "STATUSINVEST"
}
```

## ğŸ› ï¸ Adicionar Novo Scraper

### 1. Criar classe do scraper

```python
# scrapers/meu_scraper.py
from base_scraper import BaseScraper, ScraperResult

class MeuScraper(BaseScraper):
    def __init__(self):
        super().__init__(
            name="MeuScraper",
            source="MEU_SOURCE",
            requires_login=False
        )

    async def scrape(self, ticker: str) -> ScraperResult:
        # Criar driver se necessÃ¡rio
        if not self.driver:
            self.driver = self._create_driver()

        # Navegar e extrair dados
        url = f"https://example.com/{ticker}"
        self.driver.get(url)

        # ... lÃ³gica de extraÃ§Ã£o ...

        return ScraperResult(
            success=True,
            data={"price": 10.50},
            source=self.source
        )
```

### 2. Registrar no service

```python
# main.py
from scrapers.meu_scraper import MeuScraper

class ScraperService:
    def _register_scrapers(self):
        self.scrapers["STATUSINVEST"] = StatusInvestScraper
        self.scrapers["MEU_SOURCE"] = MeuScraper  # Adicionar aqui
```

### 3. Rebuild e testar

```bash
docker-compose up -d --build scrapers
docker-compose logs -f scrapers
```

## âš™ï¸ ConfiguraÃ§Ã£o

VariÃ¡veis de ambiente disponÃ­veis:

| VariÃ¡vel | PadrÃ£o | DescriÃ§Ã£o |
|----------|--------|-----------|
| `DB_HOST` | `localhost` | Host do PostgreSQL |
| `DB_PORT` | `5432` | Porta do PostgreSQL |
| `DB_USERNAME` | `invest_user` | UsuÃ¡rio do banco |
| `DB_PASSWORD` | `invest_password` | Senha do banco |
| `DB_DATABASE` | `invest_db` | Nome do banco |
| `REDIS_HOST` | `localhost` | Host do Redis |
| `REDIS_PORT` | `6379` | Porta do Redis |
| `CHROME_HEADLESS` | `true` | Chrome em modo headless |
| `SCRAPER_TIMEOUT` | `30000` | Timeout em ms |
| `SCRAPER_MAX_RETRIES` | `3` | MÃ¡ximo de tentativas |
| `LOG_LEVEL` | `INFO` | NÃ­vel de log |

## ğŸ“Š Logs

Logs sÃ£o salvos em:
- **Console:** stdout (colorido)
- **Arquivo:** `/app/logs/scrapers.log` (rotaÃ§Ã£o 10MB, 7 dias)

Formato:
```
2024-01-01 12:00:00 | INFO     | scraper:scrape - [StatusInvest] Scraping PETR4...
2024-01-01 12:00:05 | SUCCESS  | scraper:scrape - Successfully scraped PETR4 in 5.2s
```

## ğŸ› Debugging

### Ver logs em tempo real

```bash
docker-compose logs -f scrapers
```

### Executar comando dentro do container

```bash
docker-compose exec scrapers bash
python -c "from config import settings; print(settings.database_url)"
```

### Testar scraper manualmente

```python
import asyncio
from scrapers import StatusInvestScraper

async def test():
    scraper = StatusInvestScraper()
    result = await scraper.scrape_with_retry("PETR4")
    print(result.to_dict())

asyncio.run(test())
```

## ğŸš¨ Troubleshooting

### Chrome nÃ£o inicia

```bash
# Verificar se Chrome estÃ¡ instalado
docker-compose exec scrapers google-chrome --version

# Verificar ChromeDriver
docker-compose exec scrapers chromedriver --version
```

### Erro de conexÃ£o com PostgreSQL/Redis

```bash
# Verificar se serviÃ§os estÃ£o rodando
docker-compose ps postgres redis

# Testar conexÃ£o manualmente
docker-compose exec scrapers python -c "from database import db; db.connect(); print('OK')"
docker-compose exec scrapers python -c "from redis_client import redis_client; redis_client.connect(); print('OK')"
```

### Timeout ao fazer scraping

- Aumentar `SCRAPER_TIMEOUT` (em ms)
- Verificar velocidade da internet
- Verificar se site estÃ¡ acessÃ­vel

## ğŸ“ˆ Performance

ConfiguraÃ§Ãµes recomendadas para produÃ§Ã£o:

```yaml
# docker-compose.yml
scrapers:
  deploy:
    resources:
      limits:
        cpus: '2.0'
        memory: 2G
      reservations:
        cpus: '0.5'
        memory: 512M
  environment:
    - SCRAPER_CONCURRENT_JOBS=3  # Jobs simultÃ¢neos
    - SCRAPER_MAX_RETRIES=3
    - CHROME_HEADLESS=true
```

## ğŸ“ LicenÃ§a

Parte do projeto B3 AI Analysis Platform.
