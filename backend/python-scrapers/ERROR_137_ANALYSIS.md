# An√°lise Completa: Exit Code 137 - Python Scrapers

**Data:** 2025-11-28
**Container:** `invest_scrapers`
**Contexto:** Migra√ß√£o Selenium ‚Üí Playwright

---

## üîç O que √© Exit Code 137?

**Exit Code 137 = 128 + 9 (SIGKILL)**

Significa que o processo foi **for√ßadamente terminado** pelo sistema operacional com o sinal SIGKILL (n√£o pode ser capturado pelo processo).

---

## üìã Causas Poss√≠veis

### 1. OOM Killer (Out of Memory) - MAIS PROV√ÅVEL ‚ö†Ô∏è

**O que √©:** Kernel Linux mata processos quando mem√≥ria RAM acaba

**Como identificar:**
```bash
# No host (n√£o no container)
dmesg | grep -i oom
dmesg | grep -i "killed process"

# Logs do Docker
docker inspect invest_scrapers | grep -i memory

# Stats em tempo real
watch -n 1 'docker stats invest_scrapers --no-stream'
```

**Sintomas:**
- ‚úÖ Processo morre abruptamente (sem erro de Python)
- ‚úÖ Sempre no mesmo ponto (durante navega√ß√£o/extra√ß√£o)
- ‚úÖ Exit code exatamente 137

**Evid√™ncias no nosso caso:**
- Chrome √© memory-intensive (pode usar 200-500MB por inst√¢ncia)
- Container tem limite de 2GB (`mem_limit: 2g`)
- M√∫ltiplos scrapers rodando = m√∫ltiplos browsers

---

### 2. Timeout do Docker/Sistema

**O que √©:** Container ou processo atinge tempo m√°ximo de execu√ß√£o

**Como identificar:**
```bash
docker inspect invest_scrapers | grep -i timeout
```

**Sintomas:**
- Processo sempre morre ap√≥s X segundos
- Tempo consistente entre execu√ß√µes

**Evid√™ncias no nosso caso:**
- ‚ùå Tempo varia entre execu√ß√µes
- ‚ùå Nenhum timeout configurado explicitamente

**Conclus√£o:** IMPROV√ÅVEL

---

### 3. cgroup Resource Limits

**O que √©:** Limites de recursos do container (CPU, mem√≥ria, I/O)

**Como identificar:**
```bash
docker inspect invest_scrapers | grep -i "Memory\|Cpu"
cat /sys/fs/cgroup/memory/docker/*/memory.limit_in_bytes
```

**Sintomas:**
- Container atinge limite e √© terminado
- Pode ser mem√≥ria, CPU throttling extremo, etc

**Evid√™ncias no nosso caso:**
- ‚úÖ Container tem `mem_limit: 2g` (confirmado em docker-compose.yml)
- ‚ö†Ô∏è Pode estar excedendo este limite

---

### 4. Chrome Crash

**O que √©:** Google Chrome trava e √© morto pelo sistema

**Como identificar:**
- Verificar se Chrome deixa core dump
- Logs do Chrome/Playwright

**Sintomas:**
- Chrome espec√≠fico, n√£o Python
- Pode ser segfault, assertion failure

**Evid√™ncias no nosso caso:**
- ‚úÖ Chrome standalone funciona (testamos `google-chrome --headless`)
- ‚úÖ Playwright com p√°ginas simples funciona (example.com)
- ‚ö†Ô∏è S√≥ falha com Fundamentus (p√°gina complexa)

---

## üß™ Testes Realizados

### ‚úÖ Teste 1: Python B√°sico

```bash
docker exec invest_scrapers python -c "print('Python OK')"
```

**Resultado:** ‚úÖ SUCESSO

---

### ‚úÖ Teste 2: Import do Scraper

```python
from scrapers.fundamentus_scraper import FundamentusScraper
print("Import OK")
```

**Resultado:** ‚úÖ SUCESSO

---

### ‚úÖ Teste 3: Cria√ß√£o do Objeto Scraper

```python
scraper = FundamentusScraper()
print(f"Scraper criado: {scraper.name}")
```

**Resultado:** ‚úÖ SUCESSO (ap√≥s fix do asyncio.Lock no __init__)

---

### ‚úÖ Teste 4: Chrome Standalone

```bash
google-chrome --headless --dump-dom https://www.google.com
```

**Resultado:** ‚úÖ SUCESSO - Chrome funciona

---

### ‚úÖ Teste 5: Playwright + P√°gina Simples

```python
await page.goto('http://example.com')
print(await page.title())  # "Example Domain"
```

**Resultado:** ‚úÖ SUCESSO - Playwright funciona

---

### ‚ö†Ô∏è Teste 6: Playwright + Fundamentus (completo)

```python
await page.goto('https://www.fundamentus.com.br/detalhes.php?papel=PETR4')
data = await scraper._extract_data('PETR4')
```

**Resultado:** ‚ö†Ô∏è PARCIAL
- ‚úÖ Browser criado
- ‚úÖ Page criada
- ‚úÖ Navega√ß√£o iniciada
- ‚úÖ Extra√ß√£o PARCIAL (consegue extrair alguns campos):
  - PETR4
  - PETROBRAS PN
  - Setor: Petr√≥leo. G√°s e Biocombust√≠veis
  - Min 52 sem: 28.3
  - Max 52 sem: 35.88
  - Vol $ m√©d (2m): 1104810000.0
  - Valor de mercado: 417595000000.0
- ‚ùå **KILLED (exit 137)** ap√≥s ~10 segundos de extra√ß√£o

---

## üìä Logs Detalhados do Erro

```
2025-11-28 11:29:34.150 | INFO | base_scraper:scrape_with_retry:236 - [Fundamentus] Scraping PETR4 (attempt 1/3)
2025-11-28 11:29:34.150 | INFO | base_scraper:initialize:139 - [INIT QUEUE] Initializing Fundamentus...
2025-11-28 11:29:34.616 | DEBUG | base_scraper:_create_browser_and_page:98 - Playwright browser created for Fundamentus
2025-11-28 11:29:34.899 | DEBUG | base_scraper:_create_browser_and_page:114 - Playwright page created for Fundamentus
2025-11-28 11:29:34.899 | INFO | base_scraper:initialize:150 - [INIT QUEUE] ‚úÖ Fundamentus initialized successfully
2025-11-28 11:29:36.901 | INFO | scrapers.fundamentus_scraper:scrape:61 - Navigating to https://www.fundamentus.com.br/detalhes.php?papel=PETR4
[... dados sendo extra√≠dos ...]
2025-11-28 11:29:42.044 | DEBUG | scrapers.fundamentus_scraper:_map_field:304 - Unmapped Fundamentus field: 'valor de mercado' = 417595000000.0
[PROCESSO MORTO - EXIT CODE 137]
```

**Timeline:**
- T+0s: In√≠cio do scraping
- T+0.5s: Browser criado
- T+0.7s: Page criada
- T+0.9s: Inicializa√ß√£o completa
- T+2.9s: Navega√ß√£o inicia
- T+6.8s: P√°gina carregada, extra√ß√£o inicia
- T+8.0s: ~10 campos extra√≠dos
- T+8.0s: **KILLED** ‚ùå

---

## üí° Hip√≥tese Principal

**OOM (Out of Memory) Killer matando o processo Chrome**

### C√°lculo de Mem√≥ria Estimado:

```
Container limit: 2048 MB (2GB)

Uso base do container:
- Python runtime: ~50 MB
- Sistema (fluxbox, VNC, etc): ~100 MB
- Total base: ~150 MB

Chrome durante scraping:
- Browser process: ~100-200 MB
- Renderer process: ~100-300 MB
- GPU process: ~50-100 MB (mesmo com --disable-gpu, pode alocar)
- Network process: ~50 MB
- Utilit√°rios: ~50 MB
- Total Chrome: ~350-700 MB

Pico durante navega√ß√£o:
- P√°gina HTML grande: ~20 MB
- JavaScript execution: ~50-100 MB
- Imagens/assets: ~30 MB
- DOM tree: ~20-50 MB
- Total navega√ß√£o: ~120-200 MB

TOTAL ESTIMADO: 150 + 700 + 200 = ~1050 MB (no pico)
```

**Dentro do limite?** Teoricamente SIM (1050MB < 2048MB)

**MAS:**
- Picos tempor√°rios podem ultrapassar
- Python asyncio overhead
- Playwright overhead
- Cheerio parsing (se usado)
- M√∫ltiplos browsers se rodando concorrentemente

---

## üîß Solu√ß√µes Propostas

### Solu√ß√£o 1: Aumentar Limite de Mem√≥ria ‚≠ê RECOMENDADO

```yaml
# docker-compose.yml
services:
  scrapers:
    mem_limit: 4g  # Aumentar de 2g para 4g
    memswap_limit: 4g
```

**Pr√≥s:**
- ‚úÖ Solu√ß√£o direta
- ‚úÖ Sem mudan√ßas no c√≥digo
- ‚úÖ Backend TypeScript funciona com recursos suficientes

**Contras:**
- ‚ùå Usa mais RAM do host
- ‚ùå N√£o resolve problema de efici√™ncia

---

### Solu√ß√£o 2: Otimizar Uso de Mem√≥ria

```python
# Reduzir tamanho da p√°gina carregada
await page.set_extra_http_headers({
    'Accept': 'text/html',  # N√£o baixar imagens/css/js desnecess√°rios
})

# Bloquear recursos pesados
await page.route("**/*", lambda route: (
    route.abort() if route.request.resource_type in ["image", "stylesheet", "font"]
    else route.continue_()
))

# Limpar recursos ap√≥s extra√ß√£o
await page.evaluate("() => { document.body.innerHTML = ''; }")
```

**Pr√≥s:**
- ‚úÖ Reduz uso de mem√≥ria
- ‚úÖ Melhora performance

**Contras:**
- ‚ùå Pode quebrar sites que dependem de JS
- ‚ùå Requer testes extensivos

---

### Solu√ß√£o 3: Usar Playwright Chromium (n√£o sistema Chrome)

```bash
# Instalar browsers do Playwright
docker exec invest_scrapers playwright install chromium
```

```python
# N√£o usar Chrome do sistema
executable_path = None  # Usa Playwright's Chromium
```

**Pr√≥s:**
- ‚úÖ Chromium headless_shell √© mais leve

**Contras:**
- ‚ùå Requer download de ~160MB
- ‚ùå Dockerfile install falhou antes

---

### Solu√ß√£o 4: Simplificar Extra√ß√£o

```python
# Extrair apenas campos essenciais (n√£o todos os 50+ campos)
ESSENTIAL_FIELDS = ['cotacao', 'pl', 'pvp', 'dividendYield', 'roe', 'roic']
```

**Pr√≥s:**
- ‚úÖ Reduz processamento
- ‚úÖ Pode evitar OOM

**Contras:**
- ‚ùå Perde dados
- ‚ùå N√£o √© solu√ß√£o real

---

## üìù Pr√≥ximos Passos

### Imediato (agora):

1. ‚úÖ **Monitorar mem√≥ria durante execu√ß√£o**
   ```bash
   watch -n 0.5 'docker stats invest_scrapers --no-stream'
   ```

2. ‚è≥ **Verificar OOM no host**
   ```bash
   dmesg | grep -i oom | tail -20
   ```

3. ‚è≥ **Tentar com limite maior**
   - Editar `docker-compose.yml`: `mem_limit: 4g`
   - `docker-compose up -d --force-recreate scrapers`
   - Re-testar

### Curto prazo (hoje):

4. ‚è≥ **Implementar otimiza√ß√µes de mem√≥ria** (Solu√ß√£o 2)

5. ‚è≥ **Comparar com backend TypeScript**
   - Backend usa mesma config mas funciona
   - Verificar diferen√ßas de consumo de mem√≥ria

### M√©dio prazo (esta semana):

6. ‚è≥ **Validar scrapers j√° migrados**
   - bcb_scraper.py
   - fundamentus_scraper.py (ap√≥s resolver OOM)

7. ‚è≥ **Continuar migra√ß√£o em massa**
   - Pr√≥ximo: advfn_scraper.py ou outro scraper cr√≠tico

---

## üìö Refer√™ncias T√©cnicas

- [Docker Memory Limits](https://docs.docker.com/config/containers/resource_constraints/)
- [Linux OOM Killer](https://www.kernel.org/doc/gorman/html/understand/understand016.html)
- [Playwright Python - Reducing Memory](https://playwright.dev/python/docs/ci)
- [Chrome Memory Usage](https://www.chromium.org/developers/design-documents/multi-process-architecture/)

---

**√öltima atualiza√ß√£o:** 2025-11-28 11:35 BRT
**Pr√≥xima a√ß√£o:** Aumentar mem_limit e re-testar
