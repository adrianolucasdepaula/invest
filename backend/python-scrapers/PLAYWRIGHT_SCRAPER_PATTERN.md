# Playwright Scraper Pattern - Padr√£o Standardizado

**Data:** 2025-11-28
**Status:** ‚úÖ **DEFINITIVO** - Validado e aprovado para produ√ß√£o
**Vers√£o:** 1.0

---

## üìã Resumo Executivo

Este documento define o **padr√£o standardizado** para todos os scrapers Python que usam Playwright, baseado na solu√ß√£o definitiva do **Exit Code 137** e nas melhores pr√°ticas do Playwright 2025.

**Problema resolvido:** Exit Code 137 (SIGKILL) causado por m√∫ltiplas opera√ß√µes `await` lentas durante extra√ß√£o de dados.

**Solu√ß√£o:** Single HTML fetch + BeautifulSoup local parsing (~10x mais r√°pido).

---

## üéØ Princ√≠pios Fundamentais

### 1. Single HTML Fetch (N√£o Multiple Awaits)

**‚ùå ERRADO** (padr√£o antigo - Selenium):
```python
# M√∫ltiplas opera√ß√µes await (140ms cada)
tables = await page.query_selector_all("table")  # await #1
for table in tables:
    rows = await table.query_selector_all("tr")  # await #2
    for row in rows:
        cells = await row.query_selector_all("td")  # await #3
        label = await cells[0].text_content()  # await #4
        value = await cells[1].text_content()  # await #5
        # 50 campos √ó 5 awaits √ó 140ms = ~35 segundos!
```

**‚úÖ CORRETO** (padr√£o novo - Playwright + BeautifulSoup):
```python
from bs4 import BeautifulSoup

# UMA √öNICA opera√ß√£o await
html_content = await page.content()  # await #1 (√öNICO)
soup = BeautifulSoup(html_content, 'html.parser')

# Todas opera√ß√µes locais (SEM await)
tables = soup.select("table")  # local, sem await
for table in tables:
    rows = table.select("tr")  # local, sem await
    for row in rows:
        cells = row.select("td")  # local, sem await
        label = cells[0].get_text()  # local, sem await
        value = cells[1].get_text()  # local, sem await
        # 50 campos √ó 0 awaits √ó 0ms = instant√¢neo!
```

**Resultado:** ~10x mais r√°pido (7.72s vs 35s+ ou timeout)

---

### 2. Browser Individual (N√£o Compartilhado)

**Padr√£o do backend TypeScript** (a ser seguido):

```python
class BaseScraper:
    def __init__(self):
        # Cada scraper tem SEU PR√ìPRIO browser
        self.playwright: Optional[Playwright] = None  # Individual
        self.browser: Optional[Browser] = None         # Individual
        self.page: Optional[Page] = None               # Individual
```

**‚ùå N√ÉO fazer** (browser compartilhado):
```python
# Errado - compartilhado entre scrapers
_browser_instance: Browser = None
_playwright_instance: Playwright = None
```

**Refer√™ncia:** `backend/src/scrapers/base/abstract-scraper.ts` (backend TypeScript)

---

### 3. Wait Strategy: 'load' (N√£o 'networkidle')

```python
# ‚úÖ CORRETO: Aguarda apenas DOM load (r√°pido)
await page.goto(url, wait_until='load', timeout=60000)

# ‚ùå EVITAR: Aguarda todos requests de rede (analytics lentos = timeout)
# await page.goto(url, wait_until='networkidle')  # Pode causar timeout
```

**Justificativa:** Sites modernos t√™m analytics lentos que nunca completam `networkidle`.

---

### 4. Cleanup Completo (page + browser + playwright)

```python
async def cleanup(self):
    """Cleanup resources (page, browser, playwright)"""
    try:
        if self.page:
            await self.page.close()
            self.page = None

        if self.browser:
            await self.browser.close()
            self.browser = None

        if self.playwright:
            await self.playwright.stop()  # Python-specific (TypeScript n√£o precisa)
            self.playwright = None

        self._initialized = False

    except Exception as e:
        logger.error(f"Error during cleanup: {e}")
```

---

## üìù Template Completo de Scraper

### scraper_template.py

```python
"""
[Nome do Scraper] - [Descri√ß√£o]
Fonte: [URL]
[Requer login? SIM/N√ÉO]

MIGRATED TO PLAYWRIGHT - [Data]
"""
import asyncio
from typing import Dict, Any, Optional
from loguru import logger
from bs4 import BeautifulSoup
import re

from base_scraper import BaseScraper, ScraperResult


class [Nome]Scraper(BaseScraper):
    """
    Scraper para [descri√ß√£o]

    Dados extra√≠dos:
    - Campo 1
    - Campo 2
    - Campo N
    """

    BASE_URL = "https://[site].com.br"

    def __init__(self):
        super().__init__(
            name="[NOME_FONTE]",
            source="[NOME_FONTE]",
            requires_login=False,  # ou True se requer login
        )

    async def scrape(self, ticker: str) -> ScraperResult:
        """
        Scrape data for specific ticker

        Args:
            ticker: Stock ticker (e.g., 'PETR4')

        Returns:
            ScraperResult with scraped data or error
        """
        try:
            # Ensure page is initialized (Playwright)
            if not self.page:
                await self.initialize()

            # Build URL
            url = f"{self.BASE_URL}/[endpoint]?ticker={ticker.upper()}"
            logger.info(f"Navigating to {url}")

            # Navigate (Playwright)
            # Using 'load' instead of 'networkidle' to avoid timeout issues
            await self.page.goto(url, wait_until="load", timeout=60000)

            # Optional: Small delay for JS execution
            await asyncio.sleep(1)

            # Check if ticker exists
            page_source = (await self.page.content()).lower()
            if "n√£o encontrado" in page_source or "not found" in page_source:
                return ScraperResult(
                    success=False,
                    error=f"Ticker {ticker} not found",
                    source=self.source,
                )

            # Extract data
            data = await self._extract_data(ticker)

            if data and data.get("ticker"):
                return ScraperResult(
                    success=True,
                    data=data,
                    source=self.source,
                    metadata={
                        "url": url,
                        "requires_login": self.requires_login,
                    },
                )
            else:
                return ScraperResult(
                    success=False,
                    error="Failed to extract data",
                    source=self.source,
                )

        except Exception as e:
            logger.error(f"Error scraping {ticker}: {e}")
            return ScraperResult(
                success=False,
                error=str(e),
                source=self.source,
            )

    async def _extract_data(self, ticker: str) -> Optional[Dict[str, Any]]:
        """
        Extract data from page

        OPTIMIZED: Uses single HTML fetch + local parsing (BeautifulSoup)
        instead of multiple await calls. ~10x faster!
        """
        try:
            from bs4 import BeautifulSoup

            data = {
                "ticker": ticker.upper(),
                "field1": None,
                "field2": None,
                # ... todos os campos
            }

            # OPTIMIZATION: Get HTML content once and parse locally
            html_content = await self.page.content()
            soup = BeautifulSoup(html_content, 'html.parser')

            # Extract company name (example)
            try:
                name_elem = soup.select_one("h1.company-name")
                if name_elem:
                    data["company_name"] = name_elem.get_text().strip()
            except Exception as e:
                logger.debug(f"Could not extract company name: {e}")

            # Extract table data (example)
            try:
                tables = soup.select("table.data-table")

                for table in tables:
                    rows = table.select("tr")

                    for row in rows:
                        cells = row.select("td")

                        # Processar c√©lulas em pares (label, value)
                        for i in range(0, len(cells) - 1, 2):
                            try:
                                label_elem = cells[i].select_one(".label")
                                value_elem = cells[i + 1].select_one(".value")

                                if not label_elem or not value_elem:
                                    continue

                                label = label_elem.get_text().strip()
                                value = value_elem.get_text().strip()

                                # Parse value
                                parsed_value = self._parse_value(value)

                                # Map to data fields
                                self._map_field(data, label, parsed_value)

                            except Exception as e:
                                continue

            except Exception as e:
                logger.error(f"Error extracting table data: {e}")

            logger.debug(f"Extracted data for {ticker}: {data}")
            return data

        except Exception as e:
            logger.error(f"Error in _extract_data: {e}")
            return None

    def _parse_value(self, value_text: str) -> Optional[float]:
        """
        Parse numeric value from text
        Handles Brazilian number format (comma as decimal separator)
        """
        if not value_text or value_text == "-":
            return None

        try:
            # Remove common prefixes
            value_text = value_text.replace("R$", "").strip()

            # Check for percentage
            is_percent = "%" in value_text
            value_text = value_text.replace("%", "").strip()

            # Replace Brazilian decimal separator
            value_text = value_text.replace(".", "").replace(",", ".")

            # Parse number
            parsed = float(value_text)

            return parsed

        except Exception as e:
            logger.debug(f"Could not parse value '{value_text}': {e}")
            return None

    def _map_field(self, data: dict, label: str, value: Optional[float]):
        """Map field labels to data dictionary keys"""

        # Normalize label
        label = label.lower().strip().replace("?", "")

        # Mapping dictionary
        field_map = {
            "cota√ß√£o": "price",
            "p/l": "p_l",
            # ... adicionar todos os mapeamentos
        }

        # Find matching field
        for key, field in field_map.items():
            if key in label:
                data[field] = value
                return

        # Log unmapped fields for future improvement
        logger.debug(f"Unmapped field: '{label}' = {value}")


# Example usage
async def test_[nome]():
    """Test [Nome] scraper"""
    scraper = [Nome]Scraper()

    try:
        # Test with PETR4
        result = await scraper.scrape_with_retry("PETR4")

        if result.success:
            print("‚úÖ Success!")
            print(f"Data: {result.data}")
        else:
            print(f"‚ùå Error: {result.error}")

    finally:
        await scraper.cleanup()


if __name__ == "__main__":
    asyncio.run(test_[nome]())
```

---

## ‚úÖ Checklist de Migra√ß√£o

Para cada scraper a ser migrado:

### Fase 1: Prepara√ß√£o
- [ ] Fazer backup do scraper Selenium: `[nome]_scraper.py.bak`
- [ ] Ler documenta√ß√£o do site alvo
- [ ] Identificar seletores CSS necess√°rios
- [ ] Verificar se requer login/autentica√ß√£o

### Fase 2: Implementa√ß√£o
- [ ] Copiar template acima
- [ ] Implementar `__init__()` (definir BASE_URL, source, requires_login)
- [ ] Implementar `scrape()` (navega√ß√£o + verifica√ß√£o de ticker)
- [ ] Implementar `_extract_data()` usando **BeautifulSoup local parsing**
- [ ] Implementar `_parse_value()` (formato brasileiro: v√≠rgula decimal)
- [ ] Implementar `_map_field()` (mapeamento label ‚Üí campo)

### Fase 3: Valida√ß√£o
- [ ] Criar arquivo `test_[nome].py`
- [ ] Testar com ticker v√°lido (ex: PETR4)
- [ ] Testar com ticker inv√°lido
- [ ] Verificar todos os campos extra√≠dos
- [ ] Comparar valores com site original
- [ ] Medir tempo de execu√ß√£o (deve ser <10s)
- [ ] Verificar cleanup correto (page, browser, playwright)

### Fase 4: Integra√ß√£o
- [ ] Adicionar import em `scrapers/__init__.py`
- [ ] Adicionar import em `main.py`
- [ ] Adicionar registro em `main.py::_register_scrapers()`
- [ ] Testar servi√ßo completo (`docker-compose restart scrapers`)
- [ ] Verificar logs (sem erros)

### Fase 5: Documenta√ß√£o
- [ ] Atualizar `MIGRATION_REPORT.md`
- [ ] Atualizar `VALIDACAO_MIGRACAO_PLAYWRIGHT.md`
- [ ] Commit com mensagem descritiva

---

## üìä Scrapers Validados

### ‚úÖ fundamentus_scraper.py
- **Status:** ‚úÖ Validado e em produ√ß√£o
- **Tempo:** 7.72s
- **Campos:** 30 extra√≠dos com sucesso
- **Teste:** PETR4 - Todos valores corretos
- **Padr√£o:** BeautifulSoup single fetch ‚úÖ

### ‚úÖ bcb_scraper.py
- **Status:** ‚úÖ Validado e em produ√ß√£o
- **M√©todo prim√°rio:** API BCB (SGS) - 17 indicadores
- **Fallback web:** BeautifulSoup local parsing
- **Tempo:** <1s (API)
- **Padr√£o:** BeautifulSoup single fetch ‚úÖ

---

## üöß Pr√≥ximos Scrapers (24 pendentes)

**Ordem sugerida de migra√ß√£o:**

### Prioridade ALTA (p√∫blico, sem login):
1. `statusinvest_scraper.py` - Fundamental analysis
2. `investsite_scraper.py` - Fundamental analysis
3. `b3_scraper.py` - Official data
4. `googlenews_scraper.py` - News aggregator

### Prioridade M√âDIA (requer login/OAuth):
5. `advfn_scraper.py` - Market analysis (Google OAuth)
6. `fundamentei_scraper.py` - Fundamental (Google OAuth)
7. `investidor10_scraper.py` - Fundamental (Google OAuth)
8. `tradingview_scraper.py` - Technical analysis (Google OAuth)

### Prioridade BAIXA (especializado):
9-24. Demais scrapers (AI assistants, institutional reports, etc)

---

## üõ†Ô∏è Troubleshooting

### Exit Code 137 (SIGKILL)

**Causa:** Opera√ß√µes `await` m√∫ltiplas e lentas.

**Solu√ß√£o:** Aplicar padr√£o BeautifulSoup (single HTML fetch).

**Como evitar:**
- ‚úÖ Usar `await page.content()` UMA VEZ
- ‚úÖ Todo parsing em BeautifulSoup local
- ‚ùå NUNCA usar m√∫ltiplos `await query_selector()`

### Container Restarting

**Causa:** Imports de scrapers n√£o migrados no `main.py`.

**Solu√ß√£o:**
1. Comentar imports n√£o migrados em `main.py` (linha ~14-56)
2. Comentar registros n√£o migrados em `_register_scrapers()` (linha ~67-114)
3. `docker-compose restart scrapers`

### Timeout na Navega√ß√£o

**Causa:** `wait_until='networkidle'` aguardando analytics lentos.

**Solu√ß√£o:** Usar `wait_until='load'`:

```python
await page.goto(url, wait_until='load', timeout=60000)
```

---

## üìö Refer√™ncias

- **Backend TypeScript:** `backend/src/scrapers/base/abstract-scraper.ts`
- **Playwright Python Docs:** https://playwright.dev/python/docs/intro
- **Playwright Best Practices 2025:** Pesquisa web realizada em 2025-11-28
- **BeautifulSoup Docs:** https://www.crummy.com/software/BeautifulSoup/bs4/doc/

---

## üìù Li√ß√µes Aprendidas

### 1. Sempre Seguir Padr√£o do Backend

**Erro inicial:** Implementei browser compartilhado (otimiza√ß√£o prematura).

**Corre√ß√£o:** Backend TypeScript usa browser individual - seguir mesmo padr√£o.

**Li√ß√£o:** **Sempre alinhar com backend funcional antes de "otimizar"**.

### 2. asyncio.Lock Requer Async Context

**Erro:** Criar `asyncio.Lock()` em `__init__()` (s√≠ncrono).

**Corre√ß√£o:** Criar lazily no primeiro uso async.

**Li√ß√£o:** **Python async tem regras estritas - sempre verificar event loop**.

### 3. networkidle vs load

**Situa√ß√£o:** Sites t√™m analytics lentos que nunca completam `networkidle`.

**Decis√£o:** Usar `wait_until='load'` ao inv√©s de `'networkidle'`.

**Li√ß√£o:** **Adaptar wait strategy por site - analytics != conte√∫do**.

### 4. Exit 137 √© Trai√ßoeiro

**Sintoma:** Processo morre sem mensagem de erro Python.

**Causa:** Opera√ß√µes lentas (n√£o OOM como inicialmente suspeitado).

**Debug:** Timeline de eventos, medir tempo de cada opera√ß√£o.

**Li√ß√£o:** **Monitorar performance, n√£o apenas mem√≥ria**.

---

**√öltima atualiza√ß√£o:** 2025-11-28 12:30 BRT
**Pr√≥xima revis√£o:** Ap√≥s cada novo scraper migrado
**Respons√°vel:** Claude Code

**Aprova√ß√£o para produ√ß√£o:** ‚úÖ **APROVADO** - Padr√£o validado e funcional
