# BRScraper v3: Especifica√ß√£o T√©cnica Definitiva

## üìã Documento de Controle

| Campo | Valor |
|-------|-------|
| **Vers√£o** | 3.0.0 (Definitiva) |
| **Data** | 2025-01-XX |
| **Autor** | Claude + Adriano |
| **Status** | Pronto para Implementa√ß√£o |

---

## üéØ Vis√£o Geral do Projeto

### Objetivo Principal
Sistema completo de coleta, consolida√ß√£o e an√°lise de dados financeiros brasileiros para suporte a decis√µes de trading em m√∫ltiplos horizontes temporais.

### Pipeline de Dados
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                                                                             ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ COLETA  ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ PROCESS ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ CONSOL. ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ AN√ÅLISE ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇRELAT√ìRIO‚îÇ  ‚îÇ
‚îÇ  ‚îÇ (Scrape)‚îÇ    ‚îÇ (Parse) ‚îÇ    ‚îÇ (Merge) ‚îÇ    ‚îÇ (LLMs)  ‚îÇ    ‚îÇ (Output)‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îÇ       ‚îÇ              ‚îÇ              ‚îÇ              ‚îÇ              ‚îÇ        ‚îÇ
‚îÇ       ‚ñº              ‚ñº              ‚ñº              ‚ñº              ‚ñº        ‚îÇ
‚îÇ   raw/*.html    processed/    consolidated/   analysis/     reports/      ‚îÇ
‚îÇ   raw/*.json      *.md         {TICKER}.md    {TICKER}/     daily.md      ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Especifica√ß√µes T√©cnicas

| Par√¢metro | Valor |
|-----------|-------|
| Volume di√°rio | ~50 p√°ginas |
| Frequ√™ncia | 1x/dia √†s 09:00 BRT |
| Custo | R$ 0 (100% self-hosted) |
| Runtime | Python 3.11+ |
| Container | Docker + docker-compose |
| Storage | SQLite + JSON/Markdown |

---

## üìä Invent√°rio Completo de Sites

### Legenda de Tipos de Acesso

| C√≥digo | Descri√ß√£o | Fetcher |
|--------|-----------|---------|
| üü¢ API | API REST direta | `APIFetcher` |
| üîµ OPEN | Acesso direto sem login | `SimpleFetcher` ou `BrowserFetcher` |
| üü° GOOGLE | Login via Google OAuth | `GoogleAuthFetcher` |
| üü† CREDS | Login com credenciais | `CredentialsFetcher` |
| üî¥ 2FA | Requer 2FA (sess√£o manual) | `Session2FAFetcher` |
| üü£ LLM | Interface de LLM | `LLMInterfaceFetcher` |
| ‚ö´ SKIP | Ignorar/N√£o implementar | - |

---

### 1. AN√ÅLISE DE FUNDAMENTOS (7 sites)

| # | Site | Tipo | URL Base | P√°ginas a Scrapar | Parser | Delay | Notas |
|---|------|------|----------|-------------------|--------|-------|-------|
| 1 | BRAPI | üü¢ API | brapi.dev | `/api/quote/{ticker}` | `brapi` | 1s | Token em .env |
| 2 | Fundamentus | üîµ OPEN | fundamentus.com.br | `/detalhes.php?papel={ticker}`, `/resultado.php` | `fundamentus` | 5s | HTML est√°tico |
| 3 | InvestSite | üîµ OPEN | investsite.com.br | `/principais_indicadores.php?cod_ativo={ticker}` | `investsite` | 10s | JS leve |
| 4 | Oceans14 | üîµ OPEN | oceans14.com.br | `/acoes/{ticker}` | `oceans14` | 10s | JS leve |
| 5 | Fundamentei | üü° GOOGLE | fundamentei.com | `/acao/{ticker}` | `fundamentei` | 15s | Login Google |
| 6 | Investidor10 | üü° GOOGLE | investidor10.com.br | `/acoes/{ticker}` | `investidor10` | 15s | Login Google |
| 7 | StatusInvest | üü° GOOGLE | statusinvest.com.br | `/acoes/{ticker}` | `statusinvest` | 30s | Cloudflare + Login |

---

### 2. AN√ÅLISE GERAL DO MERCADO (4 sites)

| # | Site | Tipo | URL Base | P√°ginas a Scrapar | Parser | Delay | Notas |
|---|------|------|----------|-------------------|--------|-------|-------|
| 8 | Investing.com BR | üü° GOOGLE | br.investing.com | `/equities/{ticker}`, `/indices/ibovespa` | `investing` | 30s | Heavy protection |
| 9 | ADVFN | üü° GOOGLE | br.advfn.com | `/bolsa-de-valores/bovespa/{ticker}/cotacao` | `advfn` | 20s | Login Google |
| 10 | Google Finance | üü° GOOGLE | google.com/finance | `/quote/{ticker}:BVMF` | `google_finance` | 15s | Login Google |
| 11 | Yahoo Finance | üü° GOOGLE | finance.yahoo.com | `/quote/{ticker}.SA` | `yahoo_finance` | 20s | Login Google |

---

### 3. AN√ÅLISE GR√ÅFICA/T√âCNICA (1 site)

| # | Site | Tipo | URL Base | P√°ginas a Scrapar | Parser | Delay | Notas |
|---|------|------|----------|-------------------|--------|-------|-------|
| 12 | TradingView | üü° GOOGLE | br.tradingview.com | `/symbols/BMFBOVESPA-{ticker}/technicals/` | `tradingview` | 60s | Heavy protection |

---

### 4. AN√ÅLISE DE OP√á√ïES (2 sites)

| # | Site | Tipo | URL Base | P√°ginas a Scrapar | Parser | Delay | Notas |
|---|------|------|----------|-------------------|--------|-------|-------|
| 13 | Op√ß√µes.net.br | üü† CREDS | opcoes.net.br | `/acoes/{ticker}` | `opcoes_net` | 15s | Login CPF/senha |
| 14 | OpLab | üîµ OPEN | opcoes.oplab.com.br | `/mercado-de-opcoes` | `oplab` | 15s | Acesso direto |

---

### 5. AN√ÅLISE DE CRIPTOMOEDAS (2 fontes)

| # | Site | Tipo | URL Base | P√°ginas a Scrapar | Parser | Delay | Notas |
|---|------|------|----------|-------------------|--------|-------|-------|
| 15 | CoinGecko | üü¢ API | api.coingecko.com | `/api/v3/coins/{id}` | `coingecko` | 2s | Prefer√≠vel ao CMC |
| 16 | CoinMarketCap | üîµ OPEN | coinmarketcap.com | `/currencies/{coin}/` | `coinmarketcap` | 20s | Fallback apenas |

---

### 6. AN√ÅLISE DE INSIDERS (1 site)

| # | Site | Tipo | URL Base | P√°ginas a Scrapar | Parser | Delay | Notas |
|---|------|------|----------|-------------------|--------|-------|-------|
| 17 | Griffin | üü° GOOGLE | griffin.app.br | `/empresa/{ticker}` | `griffin` | 20s | Dados de insiders |

---

### 7. RELAT√ìRIOS INSTITUCIONAIS (4 sites)

| # | Site | Tipo | URL Base | P√°ginas a Scrapar | Parser | Delay | Notas |
|---|------|------|----------|-------------------|--------|-------|-------|
| 18 | BTG Research | üî¥ 2FA | content.btgpactual.com | `/research/` | `btg` | 30s | 2FA celular - sess√£o manual |
| 19 | XP Conte√∫dos | üî¥ 2FA | conteudos.xpi.com.br | `/` | `xp` | 30s | 2FA celular - sess√£o manual |
| 20 | E-Investidor | üü° GOOGLE | einvestidor.estadao.com.br | `/` | `einvestidor` | 15s | Login Google |
| 21 | Mais Retorno | üü° GOOGLE | maisretorno.com | `/` | `maisretorno` | 15s | Login Google |

---

### 8. DADOS OFICIAIS (3 sites)

| # | Site | Tipo | URL Base | P√°ginas a Scrapar | Parser | Delay | Notas |
|---|------|------|----------|-------------------|--------|-------|-------|
| 22 | B3 | üü° GOOGLE | b3.com.br | `/pt_br/produtos-e-servicos/negociacao/` | `b3` | 15s | Login Google |
| 23 | BCB | üü¢ API | api.bcb.gov.br | SGS series | `bcb` | 1s | API oficial |
| 24 | IBGE | üü¢ API | api.sidra.ibge.gov.br | SIDRA tables | `ibge` | 1s | API oficial |

---

### 9. NOT√çCIAS FINANCEIRAS (6 sites)

| # | Site | Tipo | URL Base | P√°ginas a Scrapar | Parser | Delay | Notas |
|---|------|------|----------|-------------------|--------|-------|-------|
| 25 | Google News | üü° GOOGLE | news.google.com | `/search?q={ticker}+a√ß√£o` | `google_news` | 10s | Login Google |
| 26 | Bloomberg L√≠nea | üîµ OPEN | bloomberglinea.com.br | `/mercados/` | `bloomberg` | 10s | Sem login |
| 27 | Investing News | üü° GOOGLE | br.investing.com/news | `/` | `investing_news` | 20s | Login Google |
| 28 | Valor Econ√¥mico | üü° GOOGLE | valor.globo.com | `/` | `valor` | 20s | Pode ter paywall |
| 29 | Exame | üü° GOOGLE | exame.com | `/invest/` | `exame` | 20s | Login Google |
| 30 | InfoMoney | üü° GOOGLE | infomoney.com.br | `/` | `infomoney` | 20s | Login Google |

---

### 10. LLMs PARA AN√ÅLISE (6 interfaces)

| # | Site | Tipo | URL Base | Fun√ß√£o | Seletores | Delay | Notas |
|---|------|------|----------|--------|-----------|-------|-------|
| 31 | ChatGPT | üü£ LLM | chatgpt.com | An√°lise prim√°ria | Ver tabela seletores | 30s | Login Google |
| 32 | Claude | üü£ LLM | claude.ai/new | An√°lise secund√°ria | Ver tabela seletores | 30s | Login Google |
| 33 | Gemini | üü£ LLM | gemini.google.com/app | Pesquisa + an√°lise | Ver tabela seletores | 30s | Login Google |
| 34 | Perplexity | üü£ LLM | perplexity.ai | Pesquisa web | Ver tabela seletores | 30s | Login Google |
| 35 | Grok | üü£ LLM | grok.com | An√°lise alternativa | Ver tabela seletores | 30s | Login Google |
| 36 | DeepSeek | üü£ LLM | deepseek.com | An√°lise t√©cnica | Ver tabela seletores | 30s | Login Google |

---

### 11. BUSCADORES GERAIS (ignorar como fonte de dados)

| # | Site | Tipo | Motivo |
|---|------|------|--------|
| 37 | Google Search | ‚ö´ SKIP | Usar Google News ou APIs espec√≠ficas |

---

## üìä Resumo de Cobertura

| Categoria | Total | API | Open | Google | Creds | 2FA | LLM |
|-----------|-------|-----|------|--------|-------|-----|-----|
| Fundamentos | 7 | 1 | 3 | 3 | 0 | 0 | 0 |
| Mercado | 4 | 0 | 0 | 4 | 0 | 0 | 0 |
| T√©cnica | 1 | 0 | 0 | 1 | 0 | 0 | 0 |
| Op√ß√µes | 2 | 0 | 1 | 0 | 1 | 0 | 0 |
| Crypto | 2 | 1 | 1 | 0 | 0 | 0 | 0 |
| Insiders | 1 | 0 | 0 | 1 | 0 | 0 | 0 |
| Research | 4 | 0 | 0 | 2 | 0 | 2 | 0 |
| Oficiais | 3 | 2 | 0 | 1 | 0 | 0 | 0 |
| Not√≠cias | 6 | 0 | 1 | 5 | 0 | 0 | 0 |
| LLMs | 6 | 0 | 0 | 0 | 0 | 0 | 6 |
| **TOTAL** | **36** | **4** | **6** | **17** | **1** | **2** | **6** |

---

## üèóÔ∏è Arquitetura Detalhada

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                              BRScraper v3                                       ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                                 ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ                         CAMADA DE INTERFACE                               ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  FastAPI    ‚îÇ  ‚îÇ  Scheduler  ‚îÇ  ‚îÇ    CLI      ‚îÇ  ‚îÇ  Dashboard  ‚îÇ      ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  REST API   ‚îÇ  ‚îÇ APScheduler ‚îÇ  ‚îÇ   Typer     ‚îÇ  ‚îÇ   (Rich)    ‚îÇ      ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  :8000      ‚îÇ  ‚îÇ  09:00 BRT  ‚îÇ  ‚îÇ             ‚îÇ  ‚îÇ             ‚îÇ      ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îÇ                                       ‚îÇ                                         ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ                         CAMADA DE ORQUESTRA√á√ÉO                            ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ   Redis     ‚îÇ  ‚îÇ  Job Queue  ‚îÇ  ‚îÇ   Session   ‚îÇ  ‚îÇ   Error     ‚îÇ      ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ   Cache     ‚îÇ  ‚îÇ    (RQ)     ‚îÇ  ‚îÇ   Manager   ‚îÇ  ‚îÇ   Handler   ‚îÇ      ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  :6379      ‚îÇ  ‚îÇ             ‚îÇ  ‚îÇ  (Cookies)  ‚îÇ  ‚îÇ  (Retry)    ‚îÇ      ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îÇ                                       ‚îÇ                                         ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ                         CAMADA DE FETCHERS                                ‚îÇ ‚îÇ
‚îÇ  ‚îÇ                                                                           ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ    API    ‚îÇ ‚îÇ  Simple   ‚îÇ ‚îÇ  Google   ‚îÇ ‚îÇ   Creds   ‚îÇ ‚îÇ    LLM    ‚îÇ  ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  Fetcher  ‚îÇ ‚îÇ  Fetcher  ‚îÇ ‚îÇ   Auth    ‚îÇ ‚îÇ  Fetcher  ‚îÇ ‚îÇ Interface ‚îÇ  ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ           ‚îÇ ‚îÇ           ‚îÇ ‚îÇ  Fetcher  ‚îÇ ‚îÇ           ‚îÇ ‚îÇ  Fetcher  ‚îÇ  ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  BRAPI    ‚îÇ ‚îÇFundamentus‚îÇ ‚îÇ Invest10  ‚îÇ ‚îÇopcoes.net ‚îÇ ‚îÇ  ChatGPT  ‚îÇ  ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  BCB      ‚îÇ ‚îÇ OpLab     ‚îÇ ‚îÇ StatusInv ‚îÇ ‚îÇ           ‚îÇ ‚îÇ  Claude   ‚îÇ  ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  IBGE     ‚îÇ ‚îÇ Bloomberg ‚îÇ ‚îÇ TradingV  ‚îÇ ‚îÇ           ‚îÇ ‚îÇ  Gemini   ‚îÇ  ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ CoinGecko ‚îÇ ‚îÇ           ‚îÇ ‚îÇ  +15 more ‚îÇ ‚îÇ           ‚îÇ ‚îÇ  +3 more  ‚îÇ  ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ ‚îÇ
‚îÇ  ‚îÇ                                                                           ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ                     Session 2FA (Manual)                          ‚îÇ   ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  BTG Research, XP Conte√∫dos - Requer login manual pr√©vio          ‚îÇ   ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îÇ                                       ‚îÇ                                         ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ                         CAMADA DE PROCESSAMENTO                           ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ   Parsers   ‚îÇ  ‚îÇ  HTML‚ÜíMD    ‚îÇ  ‚îÇ    Data     ‚îÇ  ‚îÇ  Validator  ‚îÇ      ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ (30+ sites) ‚îÇ  ‚îÇ  Converter  ‚îÇ  ‚îÇ Normalizer  ‚îÇ  ‚îÇ  (Schema)   ‚îÇ      ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îÇ                                       ‚îÇ                                         ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ                         CAMADA DE AN√ÅLISE                                 ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ    Data     ‚îÇ  ‚îÇ   Prompt    ‚îÇ  ‚îÇ     LLM     ‚îÇ  ‚îÇ   Report    ‚îÇ      ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  ‚îÇConsolidator ‚îÇ  ‚îÇ  Builder    ‚îÇ  ‚îÇ  Analyzer   ‚îÇ  ‚îÇ  Generator  ‚îÇ      ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ             ‚îÇ  ‚îÇ (templates) ‚îÇ  ‚îÇ             ‚îÇ  ‚îÇ             ‚îÇ      ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îÇ                                       ‚îÇ                                         ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ                         CAMADA DE STORAGE                                 ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ   SQLite    ‚îÇ  ‚îÇ    JSON     ‚îÇ  ‚îÇ  Markdown   ‚îÇ  ‚îÇ   Reports   ‚îÇ      ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ  (metadata) ‚îÇ  ‚îÇ   (raw)     ‚îÇ  ‚îÇ (processed) ‚îÇ  ‚îÇ  (output)   ‚îÇ      ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îÇ                                                                                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üìÅ Estrutura de Diret√≥rios Definitiva

```
brscraper/
‚îÇ
‚îú‚îÄ‚îÄ üìÑ docker-compose.yml          # Orquestra√ß√£o de containers
‚îú‚îÄ‚îÄ üìÑ Dockerfile                  # Build da imagem
‚îú‚îÄ‚îÄ üìÑ Dockerfile.worker           # Build do worker
‚îú‚îÄ‚îÄ üìÑ requirements.txt            # Depend√™ncias Python
‚îú‚îÄ‚îÄ üìÑ requirements-dev.txt        # Depend√™ncias de desenvolvimento
‚îú‚îÄ‚îÄ üìÑ .env.example                # Template de vari√°veis de ambiente
‚îú‚îÄ‚îÄ üìÑ .env                        # ‚ö†Ô∏è N√ÉO COMMITAR - credenciais
‚îú‚îÄ‚îÄ üìÑ .gitignore                  # Arquivos ignorados
‚îú‚îÄ‚îÄ üìÑ README.md                   # Documenta√ß√£o principal
‚îú‚îÄ‚îÄ üìÑ CHANGELOG.md                # Hist√≥rico de mudan√ßas
‚îú‚îÄ‚îÄ üìÑ LICENSE                     # Licen√ßa MIT
‚îÇ
‚îú‚îÄ‚îÄ üìÅ src/                        # C√≥digo fonte
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ main.py                 # FastAPI entry point
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ cli.py                  # CLI com Typer
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ config.py               # Configura√ß√µes Pydantic
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ constants.py            # Constantes do sistema
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ api/                    # API REST
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ routes/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ __init__.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ scrape.py       # /scrape endpoints
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ analyze.py      # /analyze endpoints
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ reports.py      # /reports endpoints
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üìÑ health.py       # /health endpoints
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ schemas.py          # Pydantic models
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ dependencies.py     # FastAPI dependencies
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üìÑ middleware.py       # Middlewares
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ core/                   # Componentes centrais
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ scheduler.py        # APScheduler
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ queue.py            # Redis Queue
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ worker.py           # RQ Worker
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ session_manager.py  # Gerenciador de sess√µes/cookies
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ storage.py          # SQLite + File storage
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ cache.py            # Redis cache
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üìÑ error_handler.py    # Tratamento centralizado de erros
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ fetchers/               # Coletores de dados
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ base.py             # BaseFetcher abstrato
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ api_fetcher.py      # Para APIs REST
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ simple_fetcher.py   # requests + BeautifulSoup
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ browser_fetcher.py  # nodriver base
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ google_auth_fetcher.py    # nodriver + cookies Google
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ credentials_fetcher.py    # nodriver + user/pass
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ session_2fa_fetcher.py    # Para sites com 2FA
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üìÑ llm_interface_fetcher.py  # Automa√ß√£o de LLMs
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ parsers/                # Processadores de conte√∫do
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ base.py             # BaseParser abstrato
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ html_to_markdown.py # Conversor HTML‚ÜíMD
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ data_extractor.py   # Extra√ß√£o estruturada
‚îÇ   ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÅ fundamentos/        # Parsers de fundamentos
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ __init__.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ brapi.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ fundamentus.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ investidor10.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ statusinvest.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ fundamentei.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ investsite.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üìÑ oceans14.py
‚îÇ   ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÅ mercado/            # Parsers de mercado
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ __init__.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ investing.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ advfn.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ google_finance.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üìÑ yahoo_finance.py
‚îÇ   ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÅ tecnica/            # Parsers de an√°lise t√©cnica
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ __init__.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üìÑ tradingview.py
‚îÇ   ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÅ opcoes/             # Parsers de op√ß√µes
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ __init__.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ opcoes_net.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üìÑ oplab.py
‚îÇ   ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÅ crypto/             # Parsers de crypto
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ __init__.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ coingecko.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üìÑ coinmarketcap.py
‚îÇ   ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÅ insiders/           # Parsers de insiders
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ __init__.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üìÑ griffin.py
‚îÇ   ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÅ research/           # Parsers de research
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ __init__.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ btg.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ xp.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ einvestidor.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üìÑ maisretorno.py
‚îÇ   ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÅ oficiais/           # Parsers oficiais
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ __init__.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ bcb.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ ibge.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üìÑ b3.py
‚îÇ   ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üìÅ noticias/           # Parsers de not√≠cias
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ üìÑ __init__.py
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ üìÑ google_news.py
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ üìÑ bloomberg.py
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ üìÑ investing_news.py
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ üìÑ valor.py
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ üìÑ exame.py
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ üìÑ infomoney.py
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ analyzers/              # Analisadores
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ consolidator.py     # Consolida dados de m√∫ltiplas fontes
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ llm_analyzer.py     # Envia para LLMs
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ report_generator.py # Gera relat√≥rios
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üìÑ scoring.py          # Sistema de scoring
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ prompts/                # Templates de prompts
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ base.py             # PromptBuilder base
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ daytrade.py         # Prompts day trade
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ swingtrade.py       # Prompts swing trade
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ position.py         # Prompts position
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ market_overview.py  # Vis√£o geral do mercado
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üìÑ sector_analysis.py  # An√°lise setorial
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ sites/                  # Configura√ß√£o de sites
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ registry.py         # Registro central de sites
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ selectors.py        # Seletores CSS/XPath
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üìÑ llm_selectors.py    # Seletores espec√≠ficos de LLMs
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ models/                 # Modelos de dados
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ database.py         # SQLAlchemy models
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ ticker.py           # TickerData model
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ analysis.py         # Analysis model
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üìÑ report.py           # Report model
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ üìÅ utils/                  # Utilit√°rios
‚îÇ       ‚îú‚îÄ‚îÄ üìÑ __init__.py
‚îÇ       ‚îú‚îÄ‚îÄ üìÑ user_agents.py      # Rota√ß√£o de UA
‚îÇ       ‚îú‚îÄ‚îÄ üìÑ delays.py           # Delays humanizados
‚îÇ       ‚îú‚îÄ‚îÄ üìÑ retry.py            # Retry com backoff
‚îÇ       ‚îú‚îÄ‚îÄ üìÑ crypto.py           # Criptografia
‚îÇ       ‚îú‚îÄ‚îÄ üìÑ validators.py       # Validadores
‚îÇ       ‚îú‚îÄ‚îÄ üìÑ formatters.py       # Formatadores
‚îÇ       ‚îî‚îÄ‚îÄ üìÑ logger.py           # Logging estruturado
‚îÇ
‚îú‚îÄ‚îÄ üìÅ data/                       # Dados (gitignored)
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ raw/                    # Dados brutos
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üìÅ YYYY-MM-DD/         # Por data
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ üìÑ brapi.json
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ üìÑ fundamentus.html
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ üìÑ ...
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ processed/              # Dados processados
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üìÅ YYYY-MM-DD/
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ üìÑ brapi.md
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ üìÑ fundamentus.md
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ üìÑ ...
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ consolidated/           # Dados consolidados por ticker
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üìÅ YYYY-MM-DD/
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ üìÑ PETR4.json
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ üìÑ PETR4.md
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ üìÑ ...
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ analysis/               # An√°lises dos LLMs
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üìÅ YYYY-MM-DD/
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ üìÅ PETR4/
‚îÇ   ‚îÇ           ‚îú‚îÄ‚îÄ üìÑ daytrade.md
‚îÇ   ‚îÇ           ‚îú‚îÄ‚îÄ üìÑ swingtrade.md
‚îÇ   ‚îÇ           ‚îî‚îÄ‚îÄ üìÑ position.md
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ reports/                # Relat√≥rios finais
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üìÅ YYYY-MM-DD/
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ üìÑ daily_report.md
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ üìÑ market_overview.md
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ üìÑ alerts.md
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ cookies/                # Cookies do sistema externo
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üìÑ google_session.json
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ cache/                  # Cache de requisi√ß√µes
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ logs/                   # Logs do sistema
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üìÑ brscraper.log
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ üìÑ brscraper.db            # SQLite database
‚îÇ
‚îú‚îÄ‚îÄ üìÅ tests/                      # Testes
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ conftest.py             # Fixtures pytest
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ unit/                   # Testes unit√°rios
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ test_fetchers.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ test_parsers.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ test_analyzers.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üìÑ test_utils.py
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ üìÅ integration/            # Testes de integra√ß√£o
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ test_api.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ üìÑ test_workflow.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ üìÑ test_llm.py
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ üìÅ e2e/                    # Testes end-to-end
‚îÇ       ‚îú‚îÄ‚îÄ üìÑ test_full_pipeline.py
‚îÇ       ‚îî‚îÄ‚îÄ üìÑ test_sites.py
‚îÇ
‚îú‚îÄ‚îÄ üìÅ scripts/                    # Scripts utilit√°rios
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ run_once.py             # Executar scraping manualmente
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ test_site.py            # Testar site espec√≠fico
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ analyze_ticker.py       # Analisar ticker espec√≠fico
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ export_data.py          # Exportar dados
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ validate_cookies.py     # Validar cookies
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ refresh_session.py      # Atualizar sess√£o 2FA
‚îÇ   ‚îî‚îÄ‚îÄ üìÑ cleanup_old_data.py     # Limpar dados antigos
‚îÇ
‚îú‚îÄ‚îÄ üìÅ docs/                       # Documenta√ß√£o
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ SETUP.md                # Guia de instala√ß√£o
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ USAGE.md                # Guia de uso
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ API.md                  # Documenta√ß√£o da API
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ COOKIES.md              # Guia de configura√ß√£o de cookies
‚îÇ   ‚îú‚îÄ‚îÄ üìÑ TROUBLESHOOTING.md      # Solu√ß√£o de problemas
‚îÇ   ‚îî‚îÄ‚îÄ üìÑ CONTRIBUTING.md         # Guia de contribui√ß√£o
‚îÇ
‚îî‚îÄ‚îÄ üìÅ config/                     # Configura√ß√µes adicionais
    ‚îú‚îÄ‚îÄ üìÑ tickers.json            # Lista de tickers a monitorar
    ‚îú‚îÄ‚îÄ üìÑ schedule.json           # Configura√ß√£o de agendamento
    ‚îî‚îÄ‚îÄ üìÑ alerts.json             # Configura√ß√£o de alertas
```

---

## üîê Configura√ß√£o de Ambiente

### .env.example (COMPLETO)

```env
# =============================================================================
# BRScraper v3 - Configura√ß√£o de Ambiente
# =============================================================================
# IMPORTANTE: Copie este arquivo para .env e preencha os valores
# NUNCA commite o arquivo .env no git!
# =============================================================================

# -----------------------------------------------------------------------------
# APLICA√á√ÉO
# -----------------------------------------------------------------------------
APP_NAME=BRScraper
APP_VERSION=3.0.0
DEBUG=false
LOG_LEVEL=INFO
# Op√ß√µes: DEBUG, INFO, WARNING, ERROR, CRITICAL

# -----------------------------------------------------------------------------
# SERVIDOR
# -----------------------------------------------------------------------------
API_HOST=0.0.0.0
API_PORT=8000
WORKERS=1

# -----------------------------------------------------------------------------
# REDIS
# -----------------------------------------------------------------------------
REDIS_URL=redis://localhost:6379
REDIS_DB=0
REDIS_MAX_CONNECTIONS=10

# -----------------------------------------------------------------------------
# DATABASE
# -----------------------------------------------------------------------------
DATABASE_URL=sqlite:///./data/brscraper.db
# Para PostgreSQL (produ√ß√£o):
# DATABASE_URL=postgresql://user:pass@localhost:5432/brscraper

# -----------------------------------------------------------------------------
# STORAGE
# -----------------------------------------------------------------------------
DATA_DIR=./data
RAW_DATA_DIR=./data/raw
PROCESSED_DATA_DIR=./data/processed
CONSOLIDATED_DATA_DIR=./data/consolidated
ANALYSIS_DATA_DIR=./data/analysis
REPORTS_DATA_DIR=./data/reports
COOKIES_DIR=./data/cookies
CACHE_DIR=./data/cache
LOGS_DIR=./data/logs

# -----------------------------------------------------------------------------
# SCHEDULER
# -----------------------------------------------------------------------------
SCHEDULER_ENABLED=true
DAILY_RUN_HOUR=9
DAILY_RUN_MINUTE=0
TIMEZONE=America/Sao_Paulo

# -----------------------------------------------------------------------------
# SCRAPING - CONFIGURA√á√ïES GERAIS
# -----------------------------------------------------------------------------
DEFAULT_DELAY_SECONDS=10
MAX_RETRIES=3
REQUEST_TIMEOUT_SECONDS=30
MAX_CONCURRENT_REQUESTS=3

# Headless mode (false = abre janela do browser, melhor para debug)
BROWSER_HEADLESS=true

# -----------------------------------------------------------------------------
# APIs EXTERNAS
# -----------------------------------------------------------------------------

# BRAPI - Dados B3
# Obter em: https://brapi.dev/
BRAPI_TOKEN=

# CoinGecko (opcional, aumenta rate limit)
# COINGECKO_API_KEY=

# -----------------------------------------------------------------------------
# CREDENCIAIS DE SITES
# -----------------------------------------------------------------------------

# Op√ß√µes.net.br
# ATEN√á√ÉO: Use seu CPF e senha reais
OPCOES_NET_USER=
OPCOES_NET_PASS=

# -----------------------------------------------------------------------------
# SISTEMA DE COOKIES (Integra√ß√£o com sistema externo)
# -----------------------------------------------------------------------------

# OP√á√ÉO 1: Arquivo JSON exportado do seu sistema
COOKIES_SOURCE=file
COOKIES_FILE_PATH=./data/cookies/google_session.json

# OP√á√ÉO 2: API do seu sistema de cookies
# COOKIES_SOURCE=api
# COOKIES_API_URL=http://localhost:5000/api/cookies
# COOKIES_API_KEY=

# OP√á√ÉO 3: Perfil do Chrome (usa perfil existente)
# COOKIES_SOURCE=chrome_profile
# CHROME_USER_DATA_DIR=/home/adriano/.config/google-chrome
# CHROME_PROFILE=Default

# -----------------------------------------------------------------------------
# CONFIGURA√á√ÉO DOS LLMs
# -----------------------------------------------------------------------------

# LLM prim√°rio para an√°lise
PRIMARY_LLM=chatgpt
# Op√ß√µes: chatgpt, claude, gemini, perplexity, grok, deepseek

# LLMs de backup (em ordem de prefer√™ncia, separados por v√≠rgula)
BACKUP_LLMS=perplexity,claude,gemini

# Timeout para resposta do LLM (segundos)
LLM_RESPONSE_TIMEOUT=120

# Tamanho m√°ximo do prompt (caracteres)
LLM_MAX_PROMPT_SIZE=50000

# -----------------------------------------------------------------------------
# TICKERS MONITORADOS
# -----------------------------------------------------------------------------
# Lista de tickers padr√£o (separados por v√≠rgula)
# Pode ser sobrescrito via config/tickers.json
DEFAULT_TICKERS=PETR4,VALE3,ITUB4,BBDC4,WEGE3,RENT3,MGLU3,ABEV3,B3SA3,LREN3

# -----------------------------------------------------------------------------
# ALERTAS E NOTIFICA√á√ïES
# -----------------------------------------------------------------------------
# ALERTS_ENABLED=false
# TELEGRAM_BOT_TOKEN=
# TELEGRAM_CHAT_ID=
# EMAIL_SMTP_HOST=
# EMAIL_SMTP_PORT=
# EMAIL_USER=
# EMAIL_PASS=
# EMAIL_TO=

# -----------------------------------------------------------------------------
# MONITORAMENTO
# -----------------------------------------------------------------------------
# SENTRY_DSN=
# PROMETHEUS_ENABLED=false
# PROMETHEUS_PORT=9090

# -----------------------------------------------------------------------------
# DESENVOLVIMENTO
# -----------------------------------------------------------------------------
# Modo de desenvolvimento (mais logs, recarrega automaticamente)
DEV_MODE=false

# Salvar screenshots de debug
SAVE_DEBUG_SCREENSHOTS=false
DEBUG_SCREENSHOTS_DIR=./data/debug/screenshots
```

---

## üì¶ Depend√™ncias Completas

### requirements.txt

```txt
# =============================================================================
# BRScraper v3 - Depend√™ncias de Produ√ß√£o
# =============================================================================

# -----------------------------------------------------------------------------
# Core Framework
# -----------------------------------------------------------------------------
fastapi==0.109.2
uvicorn[standard]==0.27.1
pydantic==2.6.1
pydantic-settings==2.2.1
python-dotenv==1.0.1
typer[all]==0.9.0

# -----------------------------------------------------------------------------
# Browser Automation
# -----------------------------------------------------------------------------
nodriver==0.38
playwright==1.41.2
# crawl4ai>=0.3.0  # Opcional, adicionar se necess√°rio

# -----------------------------------------------------------------------------
# HTTP Clients
# -----------------------------------------------------------------------------
httpx==0.27.0
aiohttp==3.9.3
curl_cffi==0.6.2
requests==2.31.0

# -----------------------------------------------------------------------------
# HTML Parsing
# -----------------------------------------------------------------------------
selectolax==0.3.21
beautifulsoup4==4.12.3
lxml==5.1.0
html5lib==1.1

# -----------------------------------------------------------------------------
# Markdown
# -----------------------------------------------------------------------------
markdownify==0.11.6
trafilatura==1.8.1

# -----------------------------------------------------------------------------
# APIs Brasileiras
# -----------------------------------------------------------------------------
python-bcb==0.2.0
sidrapy==0.1.5
yfinance==0.2.36

# -----------------------------------------------------------------------------
# Crypto APIs
# -----------------------------------------------------------------------------
pycoingecko==3.1.0

# -----------------------------------------------------------------------------
# Queue & Background Tasks
# -----------------------------------------------------------------------------
redis==5.0.1
rq==1.16.0
apscheduler==3.10.4
celery==5.3.6  # Alternativa ao RQ se precisar de mais recursos

# -----------------------------------------------------------------------------
# Database
# -----------------------------------------------------------------------------
sqlalchemy==2.0.25
aiosqlite==0.19.0
alembic==1.13.1  # Migra√ß√µes de banco

# -----------------------------------------------------------------------------
# Async
# -----------------------------------------------------------------------------
asyncio==3.4.3
aiofiles==23.2.1

# -----------------------------------------------------------------------------
# Utilities
# -----------------------------------------------------------------------------
fake-useragent==1.4.0
tenacity==8.2.3
python-dateutil==2.8.2
pytz==2024.1
orjson==3.9.14  # JSON r√°pido

# -----------------------------------------------------------------------------
# Logging & Monitoring
# -----------------------------------------------------------------------------
structlog==24.1.0
rich==13.7.0
loguru==0.7.2

# -----------------------------------------------------------------------------
# Security
# -----------------------------------------------------------------------------
cryptography==42.0.2
python-jose[cryptography]==3.3.0

# -----------------------------------------------------------------------------
# Validation
# -----------------------------------------------------------------------------
email-validator==2.1.0.post1

# -----------------------------------------------------------------------------
# CLI
# -----------------------------------------------------------------------------
click==8.1.7
questionary==2.0.1  # Para prompts interativos
```

### requirements-dev.txt

```txt
# =============================================================================
# BRScraper v3 - Depend√™ncias de Desenvolvimento
# =============================================================================

-r requirements.txt

# -----------------------------------------------------------------------------
# Testing
# -----------------------------------------------------------------------------
pytest==8.0.0
pytest-asyncio==0.23.4
pytest-cov==4.1.0
pytest-mock==3.12.0
pytest-xdist==3.5.0  # Testes paralelos
httpx-mock==0.0.12
respx==0.20.2

# -----------------------------------------------------------------------------
# Code Quality
# -----------------------------------------------------------------------------
black==24.1.1
ruff==0.2.0
mypy==1.8.0
isort==5.13.2
pre-commit==3.6.0

# -----------------------------------------------------------------------------
# Documentation
# -----------------------------------------------------------------------------
mkdocs==1.5.3
mkdocs-material==9.5.6
mkdocstrings[python]==0.24.0

# -----------------------------------------------------------------------------
# Debugging
# -----------------------------------------------------------------------------
ipython==8.21.0
ipdb==0.13.13
debugpy==1.8.0

# -----------------------------------------------------------------------------
# Type Stubs
# -----------------------------------------------------------------------------
types-requests==2.31.0.20240125
types-redis==4.6.0.20240106
types-python-dateutil==2.8.19.20240106
```

---

## üîß Implementa√ß√£o dos Componentes Principais

### 1. config.py (Configura√ß√£o Robusta)

```python
"""
Configura√ß√£o centralizada do BRScraper usando Pydantic Settings.
"""

from functools import lru_cache
from pathlib import Path
from typing import List, Optional, Literal
from pydantic import Field, field_validator, model_validator
from pydantic_settings import BaseSettings, SettingsConfigDict


class Settings(BaseSettings):
    """Configura√ß√µes do BRScraper carregadas de vari√°veis de ambiente."""
    
    model_config = SettingsConfigDict(
        env_file=".env",
        env_file_encoding="utf-8",
        case_sensitive=False,
        extra="ignore"
    )
    
    # -------------------------------------------------------------------------
    # Aplica√ß√£o
    # -------------------------------------------------------------------------
    app_name: str = "BRScraper"
    app_version: str = "3.0.0"
    debug: bool = False
    log_level: Literal["DEBUG", "INFO", "WARNING", "ERROR", "CRITICAL"] = "INFO"
    
    # -------------------------------------------------------------------------
    # Servidor
    # -------------------------------------------------------------------------
    api_host: str = "0.0.0.0"
    api_port: int = 8000
    workers: int = 1
    
    # -------------------------------------------------------------------------
    # Redis
    # -------------------------------------------------------------------------
    redis_url: str = "redis://localhost:6379"
    redis_db: int = 0
    redis_max_connections: int = 10
    
    # -------------------------------------------------------------------------
    # Database
    # -------------------------------------------------------------------------
    database_url: str = "sqlite:///./data/brscraper.db"
    
    # -------------------------------------------------------------------------
    # Storage
    # -------------------------------------------------------------------------
    data_dir: Path = Path("./data")
    raw_data_dir: Optional[Path] = None
    processed_data_dir: Optional[Path] = None
    consolidated_data_dir: Optional[Path] = None
    analysis_data_dir: Optional[Path] = None
    reports_data_dir: Optional[Path] = None
    cookies_dir: Optional[Path] = None
    cache_dir: Optional[Path] = None
    logs_dir: Optional[Path] = None
    
    # -------------------------------------------------------------------------
    # Scheduler
    # -------------------------------------------------------------------------
    scheduler_enabled: bool = True
    daily_run_hour: int = Field(default=9, ge=0, le=23)
    daily_run_minute: int = Field(default=0, ge=0, le=59)
    timezone: str = "America/Sao_Paulo"
    
    # -------------------------------------------------------------------------
    # Scraping
    # -------------------------------------------------------------------------
    default_delay_seconds: int = Field(default=10, ge=1, le=300)
    max_retries: int = Field(default=3, ge=1, le=10)
    request_timeout_seconds: int = Field(default=30, ge=5, le=120)
    max_concurrent_requests: int = Field(default=3, ge=1, le=10)
    browser_headless: bool = True
    
    # -------------------------------------------------------------------------
    # APIs
    # -------------------------------------------------------------------------
    brapi_token: Optional[str] = None
    coingecko_api_key: Optional[str] = None
    
    # -------------------------------------------------------------------------
    # Credenciais de Sites
    # -------------------------------------------------------------------------
    opcoes_net_user: Optional[str] = None
    opcoes_net_pass: Optional[str] = None
    
    # -------------------------------------------------------------------------
    # Sistema de Cookies
    # -------------------------------------------------------------------------
    cookies_source: Literal["file", "api", "chrome_profile"] = "file"
    cookies_file_path: Optional[Path] = None
    cookies_api_url: Optional[str] = None
    cookies_api_key: Optional[str] = None
    chrome_user_data_dir: Optional[str] = None
    chrome_profile: str = "Default"
    
    # -------------------------------------------------------------------------
    # LLMs
    # -------------------------------------------------------------------------
    primary_llm: Literal["chatgpt", "claude", "gemini", "perplexity", "grok", "deepseek"] = "chatgpt"
    backup_llms: str = "perplexity,claude,gemini"
    llm_response_timeout: int = Field(default=120, ge=30, le=300)
    llm_max_prompt_size: int = Field(default=50000, ge=1000, le=100000)
    
    # -------------------------------------------------------------------------
    # Tickers
    # -------------------------------------------------------------------------
    default_tickers: str = "PETR4,VALE3,ITUB4,BBDC4,WEGE3"
    
    # -------------------------------------------------------------------------
    # Desenvolvimento
    # -------------------------------------------------------------------------
    dev_mode: bool = False
    save_debug_screenshots: bool = False
    debug_screenshots_dir: Optional[Path] = None
    
    # -------------------------------------------------------------------------
    # Validators
    # -------------------------------------------------------------------------
    
    @model_validator(mode="after")
    def set_default_paths(self) -> "Settings":
        """Define paths padr√£o baseados em data_dir."""
        if self.raw_data_dir is None:
            self.raw_data_dir = self.data_dir / "raw"
        if self.processed_data_dir is None:
            self.processed_data_dir = self.data_dir / "processed"
        if self.consolidated_data_dir is None:
            self.consolidated_data_dir = self.data_dir / "consolidated"
        if self.analysis_data_dir is None:
            self.analysis_data_dir = self.data_dir / "analysis"
        if self.reports_data_dir is None:
            self.reports_data_dir = self.data_dir / "reports"
        if self.cookies_dir is None:
            self.cookies_dir = self.data_dir / "cookies"
        if self.cache_dir is None:
            self.cache_dir = self.data_dir / "cache"
        if self.logs_dir is None:
            self.logs_dir = self.data_dir / "logs"
        if self.cookies_file_path is None:
            self.cookies_file_path = self.cookies_dir / "google_session.json"
        if self.debug_screenshots_dir is None:
            self.debug_screenshots_dir = self.data_dir / "debug" / "screenshots"
        return self
    
    @field_validator("backup_llms", mode="before")
    @classmethod
    def parse_backup_llms(cls, v: str) -> str:
        """Valida lista de LLMs de backup."""
        if isinstance(v, str):
            valid_llms = {"chatgpt", "claude", "gemini", "perplexity", "grok", "deepseek"}
            llms = [l.strip().lower() for l in v.split(",")]
            for llm in llms:
                if llm and llm not in valid_llms:
                    raise ValueError(f"LLM inv√°lido: {llm}")
        return v
    
    @field_validator("default_tickers", mode="before")
    @classmethod
    def parse_tickers(cls, v: str) -> str:
        """Valida e normaliza lista de tickers."""
        if isinstance(v, str):
            tickers = [t.strip().upper() for t in v.split(",")]
            return ",".join(tickers)
        return v
    
    # -------------------------------------------------------------------------
    # Properties
    # -------------------------------------------------------------------------
    
    @property
    def backup_llms_list(self) -> List[str]:
        """Retorna lista de LLMs de backup."""
        return [l.strip() for l in self.backup_llms.split(",") if l.strip()]
    
    @property
    def default_tickers_list(self) -> List[str]:
        """Retorna lista de tickers padr√£o."""
        return [t.strip() for t in self.default_tickers.split(",") if t.strip()]
    
    def ensure_directories(self) -> None:
        """Cria todos os diret√≥rios necess√°rios."""
        dirs = [
            self.data_dir,
            self.raw_data_dir,
            self.processed_data_dir,
            self.consolidated_data_dir,
            self.analysis_data_dir,
            self.reports_data_dir,
            self.cookies_dir,
            self.cache_dir,
            self.logs_dir,
        ]
        for d in dirs:
            if d:
                d.mkdir(parents=True, exist_ok=True)


@lru_cache
def get_settings() -> Settings:
    """Retorna inst√¢ncia singleton das configura√ß√µes."""
    settings = Settings()
    settings.ensure_directories()
    return settings


# Alias para acesso r√°pido
settings = get_settings()
```

---

### 2. Session Manager (Integra√ß√£o Robusta com Cookies)

```python
"""
Gerenciador de sess√µes com suporte a m√∫ltiplas fontes de cookies.
"""

import json
import sqlite3
from abc import ABC, abstractmethod
from dataclasses import dataclass, field
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Any
import httpx
import structlog

from ..config import settings

logger = structlog.get_logger()


@dataclass
class Cookie:
    """Representa um cookie HTTP."""
    name: str
    value: str
    domain: str
    path: str = "/"
    expires: Optional[float] = None
    http_only: bool = False
    secure: bool = False
    same_site: str = "Lax"
    
    def is_expired(self) -> bool:
        """Verifica se o cookie expirou."""
        if self.expires is None:
            return False  # Session cookie
        return datetime.now().timestamp() > self.expires
    
    def to_dict(self) -> Dict[str, Any]:
        """Converte para dicion√°rio."""
        return {
            "name": self.name,
            "value": self.value,
            "domain": self.domain,
            "path": self.path,
            "expires": self.expires,
            "httpOnly": self.http_only,
            "secure": self.secure,
            "sameSite": self.same_site,
        }


class CookieSource(ABC):
    """Interface abstrata para fonte de cookies."""
    
    @abstractmethod
    async def load_cookies(self, domain: str) -> List[Cookie]:
        """Carrega cookies para um dom√≠nio."""
        pass
    
    @abstractmethod
    async def is_available(self) -> bool:
        """Verifica se a fonte est√° dispon√≠vel."""
        pass


class FileCookieSource(CookieSource):
    """Carrega cookies de arquivo JSON."""
    
    def __init__(self, file_path: Path):
        self.file_path = file_path
        self._cache: Dict[str, List[Cookie]] = {}
        self._last_load: Optional[datetime] = None
    
    async def is_available(self) -> bool:
        return self.file_path.exists()
    
    async def load_cookies(self, domain: str) -> List[Cookie]:
        # Recarregar se arquivo foi modificado
        if self._should_reload():
            await self._load_file()
        
        return self._cache.get(domain, [])
    
    def _should_reload(self) -> bool:
        if self._last_load is None:
            return True
        if not self.file_path.exists():
            return False
        mtime = datetime.fromtimestamp(self.file_path.stat().st_mtime)
        return mtime > self._last_load
    
    async def _load_file(self) -> None:
        try:
            with open(self.file_path, "r", encoding="utf-8") as f:
                data = json.load(f)
            
            # Suporta m√∫ltiplos formatos de arquivo
            cookies_list = self._normalize_format(data)
            
            # Indexar por dom√≠nio
            self._cache.clear()
            for cookie_data in cookies_list:
                cookie = self._parse_cookie(cookie_data)
                if cookie and not cookie.is_expired():
                    domain = cookie.domain.lstrip(".")
                    if domain not in self._cache:
                        self._cache[domain] = []
                    self._cache[domain].append(cookie)
                    
                    # Tamb√©m indexar por dom√≠nio pai
                    # Ex: .google.com ‚Üí google.com
                    parts = domain.split(".")
                    if len(parts) >= 2:
                        parent = ".".join(parts[-2:])
                        if parent not in self._cache:
                            self._cache[parent] = []
                        self._cache[parent].append(cookie)
            
            self._last_load = datetime.now()
            logger.info("cookies_loaded_from_file", 
                       file=str(self.file_path),
                       domains=list(self._cache.keys()))
            
        except Exception as e:
            logger.error("cookies_file_load_error", error=str(e))
    
    def _normalize_format(self, data: Any) -> List[Dict]:
        """Normaliza diferentes formatos de arquivo de cookies."""
        # Formato 1: Lista direta
        if isinstance(data, list):
            return data
        
        # Formato 2: {"cookies": [...]}
        if isinstance(data, dict) and "cookies" in data:
            return data["cookies"]
        
        # Formato 3: {"domain": [...], ...}
        if isinstance(data, dict):
            result = []
            for domain, cookies in data.items():
                if isinstance(cookies, list):
                    result.extend(cookies)
            return result
        
        return []
    
    def _parse_cookie(self, data: Dict) -> Optional[Cookie]:
        """Parse de dicion√°rio para Cookie."""
        try:
            return Cookie(
                name=data.get("name", ""),
                value=data.get("value", ""),
                domain=data.get("domain", data.get("host", "")),
                path=data.get("path", "/"),
                expires=data.get("expires") or data.get("expirationDate"),
                http_only=data.get("httpOnly", data.get("http_only", False)),
                secure=data.get("secure", False),
                same_site=data.get("sameSite", data.get("same_site", "Lax")),
            )
        except Exception as e:
            logger.debug("cookie_parse_error", error=str(e), data=data)
            return None


class APICookieSource(CookieSource):
    """Carrega cookies de API externa."""
    
    def __init__(self, api_url: str, api_key: Optional[str] = None):
        self.api_url = api_url.rstrip("/")
        self.api_key = api_key
    
    async def is_available(self) -> bool:
        try:
            async with httpx.AsyncClient(timeout=5) as client:
                response = await client.get(f"{self.api_url}/health")
                return response.status_code == 200
        except:
            return False
    
    async def load_cookies(self, domain: str) -> List[Cookie]:
        try:
            headers = {}
            if self.api_key:
                headers["Authorization"] = f"Bearer {self.api_key}"
            
            async with httpx.AsyncClient(timeout=10) as client:
                response = await client.get(
                    f"{self.api_url}/cookies",
                    params={"domain": domain},
                    headers=headers
                )
                
                if response.status_code == 200:
                    data = response.json()
                    cookies = []
                    for c in data.get("cookies", []):
                        cookie = Cookie(
                            name=c["name"],
                            value=c["value"],
                            domain=c.get("domain", domain),
                            path=c.get("path", "/"),
                            expires=c.get("expires"),
                            http_only=c.get("httpOnly", False),
                            secure=c.get("secure", False),
                        )
                        if not cookie.is_expired():
                            cookies.append(cookie)
                    return cookies
                    
        except Exception as e:
            logger.error("cookies_api_error", error=str(e))
        
        return []


class ChromeProfileCookieSource(CookieSource):
    """Carrega cookies do perfil do Chrome (SQLite)."""
    
    def __init__(self, user_data_dir: str, profile: str = "Default"):
        self.user_data_dir = Path(user_data_dir)
        self.profile = profile
        self.cookies_db = self.user_data_dir / profile / "Cookies"
    
    async def is_available(self) -> bool:
        return self.cookies_db.exists()
    
    async def load_cookies(self, domain: str) -> List[Cookie]:
        # NOTA: Chrome precisa estar fechado para ler o arquivo
        # Em produ√ß√£o, √© melhor usar a op√ß√£o de arquivo JSON
        logger.warning("chrome_profile_not_recommended",
                      message="Usar perfil do Chrome diretamente n√£o √© recomendado. "
                              "Considere exportar cookies para JSON.")
        return []


class SessionManager:
    """
    Gerenciador de sess√µes que unifica m√∫ltiplas fontes de cookies.
    
    Suporta:
    - Arquivo JSON (recomendado)
    - API externa
    - Perfil do Chrome (experimental)
    """
    
    def __init__(self):
        self._sources: List[CookieSource] = []
        self._cache: Dict[str, List[Cookie]] = {}
        self._setup_sources()
    
    def _setup_sources(self) -> None:
        """Configura fontes de cookies baseado nas settings."""
        
        if settings.cookies_source == "file":
            if settings.cookies_file_path and settings.cookies_file_path.exists():
                self._sources.append(
                    FileCookieSource(settings.cookies_file_path)
                )
                logger.info("cookie_source_configured", 
                           type="file", 
                           path=str(settings.cookies_file_path))
        
        elif settings.cookies_source == "api":
            if settings.cookies_api_url:
                self._sources.append(
                    APICookieSource(
                        settings.cookies_api_url,
                        settings.cookies_api_key
                    )
                )
                logger.info("cookie_source_configured", 
                           type="api", 
                           url=settings.cookies_api_url)
        
        elif settings.cookies_source == "chrome_profile":
            if settings.chrome_user_data_dir:
                self._sources.append(
                    ChromeProfileCookieSource(
                        settings.chrome_user_data_dir,
                        settings.chrome_profile
                    )
                )
                logger.info("cookie_source_configured", 
                           type="chrome_profile")
        
        if not self._sources:
            logger.warning("no_cookie_source_configured")
    
    async def get_cookies(self, domain: str) -> List[Cookie]:
        """
        Obt√©m cookies para um dom√≠nio.
        
        Args:
            domain: Dom√≠nio alvo (ex: "google.com", "investidor10.com.br")
            
        Returns:
            Lista de cookies v√°lidos para o dom√≠nio
        """
        # Normalizar dom√≠nio
        domain = domain.lower().lstrip(".")
        
        # Buscar em todas as fontes
        all_cookies: List[Cookie] = []
        
        for source in self._sources:
            if await source.is_available():
                cookies = await source.load_cookies(domain)
                all_cookies.extend(cookies)
        
        # Deduplificar por nome
        unique: Dict[str, Cookie] = {}
        for cookie in all_cookies:
            key = f"{cookie.domain}:{cookie.name}"
            if key not in unique or (cookie.expires and 
                                     unique[key].expires and 
                                     cookie.expires > unique[key].expires):
                unique[key] = cookie
        
        result = list(unique.values())
        logger.debug("cookies_retrieved", domain=domain, count=len(result))
        return result
    
    async def get_google_cookies(self) -> List[Cookie]:
        """Obt√©m cookies do Google (para OAuth)."""
        google_domains = ["google.com", "accounts.google.com", "google.com.br"]
        all_cookies: List[Cookie] = []
        
        for domain in google_domains:
            cookies = await self.get_cookies(domain)
            all_cookies.extend(cookies)
        
        return all_cookies
    
    async def inject_cookies(self, browser, domain: str) -> int:
        """
        Injeta cookies em uma inst√¢ncia de browser (nodriver).
        
        Args:
            browser: Inst√¢ncia do nodriver browser
            domain: Dom√≠nio para carregar cookies
            
        Returns:
            N√∫mero de cookies injetados
        """
        cookies = await self.get_cookies(domain)
        
        # Se √© um site que usa Google OAuth, injetar cookies do Google tamb√©m
        google_auth_domains = [
            "fundamentei.com", "investidor10.com.br", "statusinvest.com.br",
            "br.investing.com", "br.advfn.com", "br.tradingview.com",
            "chatgpt.com", "claude.ai", "gemini.google.com"
        ]
        
        if any(d in domain for d in google_auth_domains):
            google_cookies = await self.get_google_cookies()
            cookies.extend(google_cookies)
        
        injected = 0
        for cookie in cookies:
            try:
                await browser.cookies.set(
                    name=cookie.name,
                    value=cookie.value,
                    domain=cookie.domain,
                    path=cookie.path,
                    secure=cookie.secure,
                    httpOnly=cookie.http_only
                )
                injected += 1
            except Exception as e:
                logger.debug("cookie_inject_failed", 
                           name=cookie.name, 
                           error=str(e))
        
        logger.info("cookies_injected", domain=domain, count=injected)
        return injected
    
    def get_nodriver_args(self) -> List[str]:
        """Retorna argumentos extras para nodriver se usando perfil Chrome."""
        args = []
        
        if settings.cookies_source == "chrome_profile":
            if settings.chrome_user_data_dir:
                args.append(f"--user-data-dir={settings.chrome_user_data_dir}")
            if settings.chrome_profile:
                args.append(f"--profile-directory={settings.chrome_profile}")
        
        return args


# Singleton
_session_manager: Optional[SessionManager] = None


def get_session_manager() -> SessionManager:
    """Retorna inst√¢ncia singleton do SessionManager."""
    global _session_manager
    if _session_manager is None:
        _session_manager = SessionManager()
    return _session_manager
```

---

### 3. Site Registry Completo

```python
"""
Registro completo de todos os sites com configura√ß√µes detalhadas.
"""

from dataclasses import dataclass, field
from enum import Enum, auto
from typing import Dict, List, Optional, Any


class FetcherType(Enum):
    """Tipos de fetchers dispon√≠veis."""
    API = auto()                    # API REST direta
    SIMPLE = auto()                 # requests + BeautifulSoup
    BROWSER_NO_LOGIN = auto()       # nodriver sem autentica√ß√£o
    BROWSER_GOOGLE = auto()         # nodriver com cookies Google
    BROWSER_CREDENTIALS = auto()    # nodriver com user/pass
    BROWSER_2FA = auto()            # Requer login manual com 2FA
    LLM_INTERFACE = auto()          # Automa√ß√£o de interface LLM
    SKIP = auto()                   # Ignorar


class Category(Enum):
    """Categorias de sites."""
    FUNDAMENTOS = "fundamentos"
    MERCADO = "mercado"
    ANALISE_TECNICA = "analise_tecnica"
    OPCOES = "opcoes"
    CRYPTO = "crypto"
    INSIDERS = "insiders"
    RESEARCH = "research"
    OFICIAIS = "oficiais"
    NOTICIAS = "noticias"
    LLM = "llm"


@dataclass
class SiteConfig:
    """Configura√ß√£o completa de um site."""
    
    # Identifica√ß√£o
    key: str
    name: str
    url: str
    
    # Comportamento
    fetcher_type: FetcherType
    category: Category
    delay_seconds: int = 10
    enabled: bool = True
    priority: int = 5  # 1-10, menor = maior prioridade
    
    # Autentica√ß√£o
    requires_cookies: bool = False
    requires_login: bool = False
    login_url: Optional[str] = None
    credentials_env_user: Optional[str] = None
    credentials_env_pass: Optional[str] = None
    
    # Scraping
    pages: List[str] = field(default_factory=list)  # P√°ginas espec√≠ficas a scrapar
    parser: Optional[str] = None
    wait_for_selector: Optional[str] = None  # Aguardar este elemento
    scroll_page: bool = True  # Fazer scroll para lazy loading
    
    # API
    api_endpoint: Optional[str] = None
    api_params: Dict[str, Any] = field(default_factory=dict)
    
    # LLM
    llm_input_selector: Optional[str] = None
    llm_submit_selector: Optional[str] = None
    llm_response_selector: Optional[str] = None
    
    # Metadados
    notes: str = ""
    fallback_to: Optional[str] = None  # Site alternativo se falhar
    
    def get_pages_for_ticker(self, ticker: str) -> List[str]:
        """Retorna URLs das p√°ginas para um ticker espec√≠fico."""
        return [
            page.format(ticker=ticker, TICKER=ticker.upper())
            for page in self.pages
        ]


# =============================================================================
# REGISTRO COMPLETO DE SITES
# =============================================================================

SITES: Dict[str, SiteConfig] = {
    
    # =========================================================================
    # FUNDAMENTOS (7 sites)
    # =========================================================================
    
    "brapi": SiteConfig(
        key="brapi",
        name="BRAPI",
        url="https://brapi.dev",
        fetcher_type=FetcherType.API,
        category=Category.FUNDAMENTOS,
        delay_seconds=1,
        priority=1,
        api_endpoint="https://brapi.dev/api/quote/{ticker}",
        api_params={"fundamental": True, "dividends": True},
        parser="brapi",
        notes="API principal para cota√ß√µes B3"
    ),
    
    "fundamentus": SiteConfig(
        key="fundamentus",
        name="Fundamentus",
        url="https://www.fundamentus.com.br",
        fetcher_type=FetcherType.SIMPLE,
        category=Category.FUNDAMENTOS,
        delay_seconds=5,
        priority=2,
        pages=[
            "/detalhes.php?papel={ticker}",
            "/resultado.php"
        ],
        parser="fundamentus",
        notes="HTML est√°tico, sem prote√ß√£o"
    ),
    
    "investsite": SiteConfig(
        key="investsite",
        name="InvestSite",
        url="https://www.investsite.com.br",
        fetcher_type=FetcherType.BROWSER_NO_LOGIN,
        category=Category.FUNDAMENTOS,
        delay_seconds=10,
        priority=4,
        pages=["/principais_indicadores.php?cod_ativo={ticker}"],
        parser="investsite",
        notes="Sem login, JS leve"
    ),
    
    "oceans14": SiteConfig(
        key="oceans14",
        name="Oceans14",
        url="https://www.oceans14.com.br",
        fetcher_type=FetcherType.BROWSER_NO_LOGIN,
        category=Category.FUNDAMENTOS,
        delay_seconds=10,
        priority=4,
        pages=["/acoes/{ticker}"],
        parser="oceans14",
        notes="Sem login"
    ),
    
    "fundamentei": SiteConfig(
        key="fundamentei",
        name="Fundamentei",
        url="https://fundamentei.com",
        fetcher_type=FetcherType.BROWSER_GOOGLE,
        category=Category.FUNDAMENTOS,
        delay_seconds=15,
        priority=3,
        requires_cookies=True,
        pages=["/acao/{ticker}"],
        parser="fundamentei",
        wait_for_selector="[data-ticker]",
        notes="Login Google"
    ),
    
    "investidor10": SiteConfig(
        key="investidor10",
        name="Investidor10",
        url="https://investidor10.com.br",
        fetcher_type=FetcherType.BROWSER_GOOGLE,
        category=Category.FUNDAMENTOS,
        delay_seconds=15,
        priority=2,
        requires_cookies=True,
        pages=["/acoes/{ticker}"],
        parser="investidor10",
        wait_for_selector=".stock-info",
        notes="Login Google"
    ),
    
    "statusinvest": SiteConfig(
        key="statusinvest",
        name="StatusInvest",
        url="https://statusinvest.com.br",
        fetcher_type=FetcherType.BROWSER_GOOGLE,
        category=Category.FUNDAMENTOS,
        delay_seconds=30,
        priority=3,
        requires_cookies=True,
        pages=["/acoes/{ticker}"],
        parser="statusinvest",
        wait_for_selector="#main-header",
        notes="Cloudflare + Login Google",
        fallback_to="investidor10"
    ),
    
    # =========================================================================
    # MERCADO GERAL (4 sites)
    # =========================================================================
    
    "investing": SiteConfig(
        key="investing",
        name="Investing.com BR",
        url="https://br.investing.com",
        fetcher_type=FetcherType.BROWSER_GOOGLE,
        category=Category.MERCADO,
        delay_seconds=30,
        priority=3,
        requires_cookies=True,
        pages=[
            "/equities/{ticker}",
            "/indices/ibovespa"
        ],
        parser="investing",
        notes="Heavy protection"
    ),
    
    "advfn": SiteConfig(
        key="advfn",
        name="ADVFN",
        url="https://br.advfn.com",
        fetcher_type=FetcherType.BROWSER_GOOGLE,
        category=Category.MERCADO,
        delay_seconds=20,
        priority=4,
        requires_cookies=True,
        pages=["/bolsa-de-valores/bovespa/{ticker}/cotacao"],
        parser="advfn",
        notes="Login Google"
    ),
    
    "google_finance": SiteConfig(
        key="google_finance",
        name="Google Finance",
        url="https://www.google.com/finance",
        fetcher_type=FetcherType.BROWSER_GOOGLE,
        category=Category.MERCADO,
        delay_seconds=15,
        priority=4,
        requires_cookies=True,
        pages=["/quote/{ticker}:BVMF"],
        parser="google_finance",
        notes="Login Google"
    ),
    
    "yahoo_finance": SiteConfig(
        key="yahoo_finance",
        name="Yahoo Finance",
        url="https://finance.yahoo.com",
        fetcher_type=FetcherType.BROWSER_GOOGLE,
        category=Category.MERCADO,
        delay_seconds=20,
        priority=4,
        requires_cookies=True,
        pages=["/quote/{ticker}.SA"],
        parser="yahoo_finance",
        notes="Login Google, usar yfinance como alternativa"
    ),
    
    # =========================================================================
    # AN√ÅLISE T√âCNICA (1 site)
    # =========================================================================
    
    "tradingview": SiteConfig(
        key="tradingview",
        name="TradingView",
        url="https://br.tradingview.com",
        fetcher_type=FetcherType.BROWSER_GOOGLE,
        category=Category.ANALISE_TECNICA,
        delay_seconds=60,
        priority=2,
        requires_cookies=True,
        pages=["/symbols/BMFBOVESPA-{ticker}/technicals/"],
        parser="tradingview",
        wait_for_selector=".tv-symbol-header",
        notes="Heavy protection, considerar tvdatafeed"
    ),
    
    # =========================================================================
    # OP√á√ïES (2 sites)
    # =========================================================================
    
    "opcoes_net": SiteConfig(
        key="opcoes_net",
        name="Op√ß√µes.net.br",
        url="https://opcoes.net.br",
        fetcher_type=FetcherType.BROWSER_CREDENTIALS,
        category=Category.OPCOES,
        delay_seconds=15,
        priority=2,
        requires_login=True,
        login_url="https://opcoes.net.br/login",
        credentials_env_user="OPCOES_NET_USER",
        credentials_env_pass="OPCOES_NET_PASS",
        pages=["/acoes/{ticker}"],
        parser="opcoes_net",
        notes="Login com CPF/senha"
    ),
    
    "oplab": SiteConfig(
        key="oplab",
        name="OpLab",
        url="https://opcoes.oplab.com.br",
        fetcher_type=FetcherType.BROWSER_NO_LOGIN,
        category=Category.OPCOES,
        delay_seconds=15,
        priority=3,
        pages=["/mercado-de-opcoes"],
        parser="oplab",
        notes="Acesso direto"
    ),
    
    # =========================================================================
    # CRYPTO (2 fontes)
    # =========================================================================
    
    "coingecko": SiteConfig(
        key="coingecko",
        name="CoinGecko",
        url="https://api.coingecko.com",
        fetcher_type=FetcherType.API,
        category=Category.CRYPTO,
        delay_seconds=2,
        priority=1,
        api_endpoint="https://api.coingecko.com/api/v3/coins/{coin}",
        api_params={"localization": False, "tickers": False},
        parser="coingecko",
        notes="API gratuita, prefer√≠vel ao CoinMarketCap"
    ),
    
    "coinmarketcap": SiteConfig(
        key="coinmarketcap",
        name="CoinMarketCap",
        url="https://coinmarketcap.com",
        fetcher_type=FetcherType.BROWSER_NO_LOGIN,
        category=Category.CRYPTO,
        delay_seconds=20,
        priority=5,
        enabled=False,  # Desabilitado por padr√£o, usar CoinGecko
        pages=["/currencies/{coin}/"],
        parser="coinmarketcap",
        notes="Fallback apenas"
    ),
    
    # =========================================================================
    # INSIDERS (1 site)
    # =========================================================================
    
    "griffin": SiteConfig(
        key="griffin",
        name="Griffin",
        url="https://griffin.app.br",
        fetcher_type=FetcherType.BROWSER_GOOGLE,
        category=Category.INSIDERS,
        delay_seconds=20,
        priority=3,
        requires_cookies=True,
        pages=["/empresa/{ticker}"],
        parser="griffin",
        notes="Dados de insiders"
    ),
    
    # =========================================================================
    # RESEARCH (4 sites)
    # =========================================================================
    
    "btg_research": SiteConfig(
        key="btg_research",
        name="BTG Research",
        url="https://content.btgpactual.com",
        fetcher_type=FetcherType.BROWSER_2FA,
        category=Category.RESEARCH,
        delay_seconds=30,
        priority=2,
        enabled=True,
        pages=["/research/"],
        parser="btg",
        notes="2FA celular - manter sess√£o ativa manualmente"
    ),
    
    "xpi_conteudos": SiteConfig(
        key="xpi_conteudos",
        name="XP Conte√∫dos",
        url="https://conteudos.xpi.com.br",
        fetcher_type=FetcherType.BROWSER_2FA,
        category=Category.RESEARCH,
        delay_seconds=30,
        priority=2,
        enabled=True,
        pages=["/"],
        parser="xp",
        notes="2FA celular - manter sess√£o ativa manualmente"
    ),
    
    "einvestidor": SiteConfig(
        key="einvestidor",
        name="E-Investidor",
        url="https://einvestidor.estadao.com.br",
        fetcher_type=FetcherType.BROWSER_GOOGLE,
        category=Category.RESEARCH,
        delay_seconds=15,
        priority=3,
        requires_cookies=True,
        pages=["/"],
        parser="einvestidor",
        notes="Login Google"
    ),
    
    "maisretorno": SiteConfig(
        key="maisretorno",
        name="Mais Retorno",
        url="https://maisretorno.com",
        fetcher_type=FetcherType.BROWSER_GOOGLE,
        category=Category.RESEARCH,
        delay_seconds=15,
        priority=3,
        requires_cookies=True,
        pages=["/"],
        parser="maisretorno",
        notes="Login Google"
    ),
    
    # =========================================================================
    # OFICIAIS (3 sites)
    # =========================================================================
    
    "bcb": SiteConfig(
        key="bcb",
        name="Banco Central",
        url="https://www.bcb.gov.br",
        fetcher_type=FetcherType.API,
        category=Category.OFICIAIS,
        delay_seconds=1,
        priority=1,
        api_endpoint="https://api.bcb.gov.br/dados/serie",
        parser="bcb",
        notes="API oficial - Selic, IPCA, PTAX"
    ),
    
    "ibge": SiteConfig(
        key="ibge",
        name="IBGE",
        url="https://www.ibge.gov.br",
        fetcher_type=FetcherType.API,
        category=Category.OFICIAIS,
        delay_seconds=1,
        priority=1,
        api_endpoint="https://api.sidra.ibge.gov.br",
        parser="ibge",
        notes="API oficial - PIB, estat√≠sticas"
    ),
    
    "b3": SiteConfig(
        key="b3",
        name="B3",
        url="https://www.b3.com.br",
        fetcher_type=FetcherType.BROWSER_GOOGLE,
        category=Category.OFICIAIS,
        delay_seconds=15,
        priority=3,
        requires_cookies=True,
        pages=["/pt_br/produtos-e-servicos/negociacao/"],
        parser="b3",
        notes="Login Google"
    ),
    
    # =========================================================================
    # NOT√çCIAS (6 sites)
    # =========================================================================
    
    "google_news": SiteConfig(
        key="google_news",
        name="Google News",
        url="https://news.google.com",
        fetcher_type=FetcherType.BROWSER_GOOGLE,
        category=Category.NOTICIAS,
        delay_seconds=10,
        priority=2,
        requires_cookies=True,
        pages=["/search?q={ticker}+a√ß√£o+bovespa"],
        parser="google_news",
        notes="Login Google"
    ),
    
    "bloomberglinea": SiteConfig(
        key="bloomberglinea",
        name="Bloomberg L√≠nea",
        url="https://www.bloomberglinea.com.br",
        fetcher_type=FetcherType.BROWSER_NO_LOGIN,
        category=Category.NOTICIAS,
        delay_seconds=10,
        priority=3,
        pages=["/mercados/"],
        parser="bloomberg",
        notes="Sem login"
    ),
    
    "investing_news": SiteConfig(
        key="investing_news",
        name="Investing.com News",
        url="https://br.investing.com",
        fetcher_type=FetcherType.BROWSER_GOOGLE,
        category=Category.NOTICIAS,
        delay_seconds=20,
        priority=4,
        requires_cookies=True,
        pages=["/news/"],
        parser="investing_news",
        notes="Login Google"
    ),
    
    "valor": SiteConfig(
        key="valor",
        name="Valor Econ√¥mico",
        url="https://valor.globo.com",
        fetcher_type=FetcherType.BROWSER_GOOGLE,
        category=Category.NOTICIAS,
        delay_seconds=20,
        priority=4,
        requires_cookies=True,
        pages=["/"],
        parser="valor",
        notes="Login Google - pode ter paywall"
    ),
    
    "exame": SiteConfig(
        key="exame",
        name="Exame",
        url="https://exame.com",
        fetcher_type=FetcherType.BROWSER_GOOGLE,
        category=Category.NOTICIAS,
        delay_seconds=20,
        priority=4,
        requires_cookies=True,
        pages=["/invest/"],
        parser="exame",
        notes="Login Google"
    ),
    
    "infomoney": SiteConfig(
        key="infomoney",
        name="InfoMoney",
        url="https://www.infomoney.com.br",
        fetcher_type=FetcherType.BROWSER_GOOGLE,
        category=Category.NOTICIAS,
        delay_seconds=20,
        priority=3,
        requires_cookies=True,
        pages=["/"],
        parser="infomoney",
        notes="Login Google"
    ),
    
    # =========================================================================
    # LLMs (6 interfaces)
    # =========================================================================
    
    "chatgpt": SiteConfig(
        key="chatgpt",
        name="ChatGPT",
        url="https://chatgpt.com",
        fetcher_type=FetcherType.LLM_INTERFACE,
        category=Category.LLM,
        delay_seconds=30,
        priority=1,
        requires_cookies=True,
        llm_input_selector="textarea[data-id='root'], #prompt-textarea",
        llm_submit_selector="button[data-testid='send-button']",
        llm_response_selector="[data-message-author-role='assistant']",
        notes="LLM prim√°rio para an√°lise"
    ),
    
    "claude": SiteConfig(
        key="claude",
        name="Claude",
        url="https://claude.ai/new",
        fetcher_type=FetcherType.LLM_INTERFACE,
        category=Category.LLM,
        delay_seconds=30,
        priority=2,
        requires_cookies=True,
        llm_input_selector="[contenteditable='true'], div.ProseMirror",
        llm_submit_selector="button[aria-label='Send message']",
        llm_response_selector="[data-testid='assistant-message']",
        notes="LLM backup"
    ),
    
    "gemini": SiteConfig(
        key="gemini",
        name="Gemini",
        url="https://gemini.google.com/app",
        fetcher_type=FetcherType.LLM_INTERFACE,
        category=Category.LLM,
        delay_seconds=30,
        priority=3,
        requires_cookies=True,
        llm_input_selector="rich-textarea, .ql-editor",
        llm_submit_selector="button[aria-label='Send message']",
        llm_response_selector=".model-response-text",
        notes="Pesquisa + an√°lise"
    ),
    
    "perplexity": SiteConfig(
        key="perplexity",
        name="Perplexity",
        url="https://www.perplexity.ai",
        fetcher_type=FetcherType.LLM_INTERFACE,
        category=Category.LLM,
        delay_seconds=30,
        priority=2,
        requires_cookies=True,
        llm_input_selector="textarea[placeholder*='Ask']",
        llm_submit_selector="button[aria-label='Submit']",
        llm_response_selector=".prose",
        notes="Melhor para pesquisa com fontes"
    ),
    
    "grok": SiteConfig(
        key="grok",
        name="Grok",
        url="https://grok.com",
        fetcher_type=FetcherType.LLM_INTERFACE,
        category=Category.LLM,
        delay_seconds=30,
        priority=4,
        requires_cookies=True,
        llm_input_selector="textarea",
        llm_submit_selector="button[type='submit']",
        llm_response_selector=".message-content",
        notes="An√°lise alternativa"
    ),
    
    "deepseek": SiteConfig(
        key="deepseek",
        name="DeepSeek",
        url="https://www.deepseek.com",
        fetcher_type=FetcherType.LLM_INTERFACE,
        category=Category.LLM,
        delay_seconds=30,
        priority=4,
        requires_cookies=True,
        llm_input_selector="textarea",
        llm_submit_selector="button[type='submit']",
        llm_response_selector=".markdown-body",
        notes="An√°lise t√©cnica"
    ),
}


# =============================================================================
# FUN√á√ïES DE ACESSO
# =============================================================================

def get_all_sites() -> Dict[str, SiteConfig]:
    """Retorna todos os sites."""
    return SITES


def get_site(key: str) -> Optional[SiteConfig]:
    """Retorna configura√ß√£o de um site."""
    return SITES.get(key)


def get_enabled_sites() -> List[SiteConfig]:
    """Retorna sites habilitados."""
    return [s for s in SITES.values() if s.enabled]


def get_sites_by_category(category: Category) -> List[SiteConfig]:
    """Retorna sites de uma categoria."""
    return [s for s in SITES.values() if s.category == category and s.enabled]


def get_sites_by_fetcher(fetcher_type: FetcherType) -> List[SiteConfig]:
    """Retorna sites por tipo de fetcher."""
    return [s for s in SITES.values() if s.fetcher_type == fetcher_type and s.enabled]


def get_api_sites() -> List[SiteConfig]:
    """Retorna sites com API."""
    return get_sites_by_fetcher(FetcherType.API)


def get_browser_sites() -> List[SiteConfig]:
    """Retorna sites que precisam de browser."""
    browser_types = [
        FetcherType.SIMPLE,
        FetcherType.BROWSER_NO_LOGIN,
        FetcherType.BROWSER_GOOGLE,
        FetcherType.BROWSER_CREDENTIALS,
        FetcherType.BROWSER_2FA,
    ]
    return [s for s in SITES.values() if s.fetcher_type in browser_types and s.enabled]


def get_llm_sites() -> List[SiteConfig]:
    """Retorna interfaces de LLM."""
    return get_sites_by_fetcher(FetcherType.LLM_INTERFACE)


def get_sites_requiring_cookies() -> List[SiteConfig]:
    """Retorna sites que precisam de cookies."""
    return [s for s in SITES.values() if s.requires_cookies and s.enabled]
```

---

## üìù Prompts de An√°lise Completos

### src/prompts/daytrade.py

```python
"""
Templates de prompts para an√°lise de Day Trade.
"""

DAYTRADE_ANALYSIS_PROMPT = """
# üìä AN√ÅLISE PARA DAY TRADE

**Ativo:** {ticker}
**Data:** {date}
**Hor√°rio da an√°lise:** {time}

---

## üìà DADOS DO MERCADO

{market_data}

---

## üìä INDICADORES FUNDAMENTALISTAS

{fundamentals}

---

## üìâ AN√ÅLISE T√âCNICA

{technicals}

---

## üì∞ NOT√çCIAS RECENTES

{news}

---

## üéØ OP√á√ïES (se dispon√≠vel)

{options}

---

# INSTRU√á√ïES PARA AN√ÅLISE

Voc√™ √© um trader profissional especializado em **day trade** no mercado brasileiro (B3).

Com base nos dados acima, forne√ßa uma an√°lise **objetiva e acion√°vel** seguindo EXATAMENTE este formato:

---

## 1. VI√âS DO DIA

**[ ] COMPRA** | **[ ] VENDA** | **[ ] NEUTRO/AGUARDAR**

**Justificativa:** (m√°ximo 3 frases)

---

## 2. N√çVEIS OPERACIONAIS

| N√≠vel | Pre√ßo (R$) | Observa√ß√£o |
|-------|------------|------------|
| Resist√™ncia 2 | | |
| Resist√™ncia 1 | | |
| **Pre√ßo Atual** | | |
| Suporte 1 | | |
| Suporte 2 | | |

---

## 3. SETUP DE ENTRADA

**Tipo de entrada:** (rompimento / pullback / revers√£o)

**Gatilho:** (condi√ß√£o espec√≠fica para entrar)

**Pre√ßo de entrada ideal:** R$ ____

**Confirma√ß√£o necess√°ria:** (volume / candle / indicador)

---

## 4. GEST√ÉO DE RISCO

| Par√¢metro | Valor |
|-----------|-------|
| Stop Loss | R$ ____ (____%) |
| Take Profit 1 (parcial) | R$ ____ (____%) |
| Take Profit 2 (final) | R$ ____ (____%) |
| Rela√ß√£o Risco/Retorno | 1:____ |

---

## 5. RISCOS DO DIA

- **Evento 1:** ____
- **Evento 2:** ____
- **Hor√°rios de aten√ß√£o:** ____

---

## 6. SCORE DE CONFIAN√áA

**[____/10]**

Justificativa: (1 frase)

---

## 7. RECOMENDA√á√ÉO FINAL

(M√°ximo 2 frases diretas e acion√°veis)

---

**IMPORTANTE:**
- Seja OBJETIVO e DIRETO
- Foque em opera√ß√µes de MINUTOS a HORAS
- N√£o use linguagem vaga ou inconclusiva
- Se n√£o houver setup claro, diga "AGUARDAR"
"""


DAYTRADE_QUICK_PROMPT = """
An√°lise r√°pida para day trade de {ticker}:

Cota√ß√£o: R$ {price} ({change}%)
Volume: {volume}
Tend√™ncia: {trend}

Em NO M√ÅXIMO 5 linhas, responda:
1. Vi√©s do dia (COMPRA/VENDA/NEUTRO)
2. Pre√ßo de entrada
3. Stop loss
4. Alvo
5. Score de confian√ßa (0-10)
"""
```

### src/prompts/swingtrade.py

```python
"""
Templates de prompts para an√°lise de Swing Trade.
"""

SWING_ANALYSIS_PROMPT = """
# üìä AN√ÅLISE PARA SWING TRADE

**Ativo:** {ticker}
**Data:** {date}
**Per√≠odo sugerido:** 3 a 15 dias √∫teis

---

## üìà DADOS CONSOLIDADOS

{consolidated_data}

---

# INSTRU√á√ïES PARA AN√ÅLISE

Voc√™ √© um trader profissional especializado em **swing trade** no mercado brasileiro.

Analise os dados e forne√ßa uma an√°lise completa seguindo EXATAMENTE este formato:

---

## 1. TEND√äNCIA PRINCIPAL

**Di√°rio:** [ ] ALTA | [ ] BAIXA | [ ] LATERAL
**Semanal:** [ ] ALTA | [ ] BAIXA | [ ] LATERAL

**Observa√ß√£o:** (m√°ximo 2 frases)

---

## 2. SETUP IDENTIFICADO

**Padr√£o gr√°fico:** ____

**Indicadores de confirma√ß√£o:**
- RSI: ____ (sobrecomprado/sobrevendido/neutro)
- MACD: ____ (cruzamento alta/baixa)
- M√©dias: ____ (pre√ßo acima/abaixo)

---

## 3. ZONA DE ENTRADA

**Tipo:** (rompimento / pullback / revers√£o)

**Faixa de entrada:** R$ ____ a R$ ____

**Gatilho:** ____

---

## 4. GEST√ÉO DE RISCO

| Par√¢metro | Pre√ßo (R$) | % |
|-----------|------------|---|
| Stop Loss | | |
| Alvo 1 (50%) | | |
| Alvo 2 (50%) | | |

**Rela√ß√£o R/R:** 1:____

---

## 5. HORIZONTE TEMPORAL

**Dura√ß√£o estimada:** ____ dias

**Checkpoints:**
- Dia 3: ____
- Dia 7: ____
- Dia 10: ____

---

## 6. AN√ÅLISE FUNDAMENTALISTA

**O fundamento suporta a opera√ß√£o?** [ ] SIM | [ ] N√ÉO | [ ] PARCIALMENTE

**Catalisadores pr√≥ximos:**
- ____
- ____

**Riscos corporativos:**
- ____

---

## 7. SCORE DE CONFIAN√áA

**[____/10]**

| Crit√©rio | Peso | Nota |
|----------|------|------|
| Tend√™ncia | 25% | /10 |
| Setup t√©cnico | 25% | /10 |
| Volume | 20% | /10 |
| Fundamentos | 15% | /10 |
| Contexto macro | 15% | /10 |

---

## 8. RECOMENDA√á√ÉO FINAL

**[ ] MONTAR POSI√á√ÉO** | **[ ] AGUARDAR** | **[ ] EVITAR**

(M√°ximo 3 frases)

---

**IMPORTANTE:**
- Foque em opera√ß√µes de DIAS a SEMANAS
- Considere o contexto macroecon√¥mico
- Avalie o momento setorial
"""
```

### src/prompts/position.py

```python
"""
Templates de prompts para an√°lise de Position Trade / Investimento.
"""

POSITION_ANALYSIS_PROMPT = """
# üìä AN√ÅLISE PARA POSITION TRADE / INVESTIMENTO

**Ativo:** {ticker}
**Setor:** {sector}
**Data:** {date}
**Horizonte:** 1 m√™s a 1+ ano

---

## üìà DADOS CONSOLIDADOS

{consolidated_data}

---

# INSTRU√á√ïES PARA AN√ÅLISE

Voc√™ √© um analista fundamentalista especializado em investimentos de m√©dio/longo prazo no mercado brasileiro.

Forne√ßa uma an√°lise completa seguindo EXATAMENTE este formato:

---

## 1. RECOMENDA√á√ÉO

**[ ] COMPRAR** | **[ ] MANTER** | **[ ] VENDER** | **[ ] AGUARDAR**

**Pre√ßo atual:** R$ ____
**Pre√ßo-alvo 12 meses:** R$ ____
**Upside/Downside:** ____%

---

## 2. AN√ÅLISE FUNDAMENTALISTA

### Indicadores de Valuation

| Indicador | Valor | vs. Setor | Interpreta√ß√£o |
|-----------|-------|-----------|---------------|
| P/L | | | |
| P/VP | | | |
| EV/EBITDA | | | |
| P/Receita | | | |

### Indicadores de Rentabilidade

| Indicador | Valor | Tend√™ncia |
|-----------|-------|-----------|
| ROE | | ‚Üë‚Üì‚Üí |
| ROIC | | ‚Üë‚Üì‚Üí |
| Margem L√≠quida | | ‚Üë‚Üì‚Üí |
| Margem EBITDA | | ‚Üë‚Üì‚Üí |

### Indicadores de Endividamento

| Indicador | Valor | Risco |
|-----------|-------|-------|
| D√≠v. L√≠quida/EBITDA | | Alto/M√©dio/Baixo |
| D√≠v. L√≠quida/PL | | Alto/M√©dio/Baixo |

---

## 3. DIVIDENDOS

**Dividend Yield atual:** ____%
**Payout ratio:** ____%
**Hist√≥rico:** (crescente/est√°vel/decrescente)
**Pr√≥ximo pagamento:** ____

---

## 4. AN√ÅLISE T√âCNICA DE LONGO PRAZO

**Tend√™ncia prim√°ria:** [ ] ALTA | [ ] BAIXA | [ ] LATERAL

**Suportes relevantes:** R$ ____, R$ ____
**Resist√™ncias relevantes:** R$ ____, R$ ____

**Momento atual no ciclo:** (acumula√ß√£o / alta / distribui√ß√£o / baixa)

---

## 5. TESE DE INVESTIMENTO

### Catalisadores de Valoriza√ß√£o
1. ____
2. ____
3. ____

### Principais Riscos
1. ____
2. ____
3. ____

### Vantagens Competitivas (Moat)
- ____

---

## 6. VALUATION

**Metodologia:** (DCF / M√∫ltiplos / DDM)

**Premissas principais:**
- Crescimento receita: ___% a.a.
- Margem EBITDA: ___%
- Taxa de desconto: ___%

**Pre√ßo justo calculado:** R$ ____

**Margem de seguran√ßa:** ____%

---

## 7. ALOCA√á√ÉO SUGERIDA

**% da carteira recomendado:** ____%

**Estrat√©gia de montagem:**
- ____% √† vista
- ____% em quedas de ___%
- ____% em quedas de ___%

---

## 8. COMPARATIVO SETORIAL

| Empresa | P/L | ROE | DY | Recomenda√ß√£o |
|---------|-----|-----|----|----|
| {ticker} | | | | |
| Peer 1 | | | | |
| Peer 2 | | | | |

---

## 9. SCORE DE CONFIAN√áA

**[____/10]**

| Crit√©rio | Peso | Nota |
|----------|------|------|
| Qualidade do neg√≥cio | 25% | /10 |
| Valuation | 25% | /10 |
| Dividendos | 15% | /10 |
| Governan√ßa | 15% | /10 |
| Momento setorial | 10% | /10 |
| T√©cnico LP | 10% | /10 |

---

## 10. RESUMO EXECUTIVO

(M√°ximo 5 frases resumindo a tese)

---

**IMPORTANTE:**
- Foque em FUNDAMENTOS e perspectivas de LONGO PRAZO
- Considere o contexto macroecon√¥mico brasileiro
- Avalie a qualidade da gest√£o e governan√ßa
- Seja conservador nas premissas de valuation
"""
```

---

## üê≥ Docker Configuration

### docker-compose.yml

```yaml
version: '3.9'

services:
  # ===========================================================================
  # Redis - Cache e Filas
  # ===========================================================================
  redis:
    image: redis:7-alpine
    container_name: brscraper-redis
    restart: unless-stopped
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    command: redis-server --appendonly yes --maxmemory 256mb --maxmemory-policy allkeys-lru
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - brscraper-net

  # ===========================================================================
  # API - FastAPI Server
  # ===========================================================================
  api:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: brscraper-api
    restart: unless-stopped
    ports:
      - "8000:8000"
    environment:
      - REDIS_URL=redis://redis:6379
      - DATABASE_URL=sqlite:///./data/brscraper.db
      - DATA_DIR=/app/data
    env_file:
      - .env
    volumes:
      - ./data:/app/data
      - ./config:/app/config:ro
    depends_on:
      redis:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    networks:
      - brscraper-net
    command: uvicorn src.main:app --host 0.0.0.0 --port 8000 --workers 1

  # ===========================================================================
  # Worker - Processamento de Jobs
  # ===========================================================================
  worker:
    build:
      context: .
      dockerfile: Dockerfile.worker
    container_name: brscraper-worker
    restart: unless-stopped
    environment:
      - REDIS_URL=redis://redis:6379
      - DATABASE_URL=sqlite:///./data/brscraper.db
      - DATA_DIR=/app/data
      - BROWSER_HEADLESS=true
    env_file:
      - .env
    volumes:
      - ./data:/app/data
      - ./config:/app/config:ro
      # Compartilhar X11 para browser (se n√£o headless)
      # - /tmp/.X11-unix:/tmp/.X11-unix
    depends_on:
      redis:
        condition: service_healthy
    deploy:
      resources:
        limits:
          memory: 4G  # Browser precisa de mem√≥ria
        reservations:
          memory: 2G
    shm_size: 2gb  # Importante para Chrome
    networks:
      - brscraper-net
    command: rq worker --url redis://redis:6379 --with-scheduler

  # ===========================================================================
  # Scheduler - Agendamento (opcional, pode rodar no API)
  # ===========================================================================
  scheduler:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: brscraper-scheduler
    restart: unless-stopped
    environment:
      - REDIS_URL=redis://redis:6379
      - DATABASE_URL=sqlite:///./data/brscraper.db
      - DATA_DIR=/app/data
    env_file:
      - .env
    volumes:
      - ./data:/app/data
      - ./config:/app/config:ro
    depends_on:
      redis:
        condition: service_healthy
    networks:
      - brscraper-net
    command: python -m src.core.scheduler

networks:
  brscraper-net:
    driver: bridge

volumes:
  redis_data:
```

### Dockerfile

```dockerfile
# ===========================================================================
# BRScraper - API Dockerfile
# ===========================================================================

FROM python:3.11-slim

# Metadados
LABEL maintainer="Adriano"
LABEL version="3.0.0"
LABEL description="BRScraper API Server"

# Vari√°veis de ambiente
ENV PYTHONDONTWRITEBYTECODE=1 \
    PYTHONUNBUFFERED=1 \
    PIP_NO_CACHE_DIR=1 \
    PIP_DISABLE_PIP_VERSION_CHECK=1

# Diret√≥rio de trabalho
WORKDIR /app

# Instalar depend√™ncias do sistema
RUN apt-get update && apt-get install -y --no-install-recommends \
    curl \
    && rm -rf /var/lib/apt/lists/*

# Copiar requirements e instalar depend√™ncias Python
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copiar c√≥digo fonte
COPY src/ ./src/
COPY config/ ./config/

# Criar diret√≥rios de dados
RUN mkdir -p /app/data/raw /app/data/processed /app/data/consolidated \
    /app/data/analysis /app/data/reports /app/data/cookies \
    /app/data/cache /app/data/logs

# Expor porta
EXPOSE 8000

# Health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \
    CMD curl -f http://localhost:8000/health || exit 1

# Comando padr√£o
CMD ["uvicorn", "src.main:app", "--host", "0.0.0.0", "--port", "8000"]
```

### Dockerfile.worker

```dockerfile
# ===========================================================================
# BRScraper - Worker Dockerfile (com Chrome)
# ===========================================================================

FROM python:3.11-slim

# Metadados
LABEL maintainer="Adriano"
LABEL version="3.0.0"
LABEL description="BRScraper Worker with Chrome"

# Vari√°veis de ambiente
ENV PYTHONDONTWRITEBYTECODE=1 \
    PYTHONUNBUFFERED=1 \
    PIP_NO_CACHE_DIR=1 \
    PIP_DISABLE_PIP_VERSION_CHECK=1 \
    DISPLAY=:99

# Diret√≥rio de trabalho
WORKDIR /app

# Instalar depend√™ncias do sistema e Chrome
RUN apt-get update && apt-get install -y --no-install-recommends \
    curl \
    gnupg \
    wget \
    xvfb \
    # Depend√™ncias do Chrome
    libnss3 \
    libatk1.0-0 \
    libatk-bridge2.0-0 \
    libcups2 \
    libdrm2 \
    libxkbcommon0 \
    libxcomposite1 \
    libxdamage1 \
    libxfixes3 \
    libxrandr2 \
    libgbm1 \
    libasound2 \
    libpango-1.0-0 \
    libcairo2 \
    fonts-liberation \
    && rm -rf /var/lib/apt/lists/*

# Instalar Chrome
RUN wget -q -O - https://dl-ssl.google.com/linux/linux_signing_key.pub | apt-key add - \
    && echo "deb [arch=amd64] http://dl.google.com/linux/chrome/deb/ stable main" >> /etc/apt/sources.list.d/google.list \
    && apt-get update \
    && apt-get install -y google-chrome-stable \
    && rm -rf /var/lib/apt/lists/*

# Copiar requirements e instalar depend√™ncias Python
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Instalar Playwright browsers (opcional)
RUN playwright install chromium

# Copiar c√≥digo fonte
COPY src/ ./src/
COPY config/ ./config/

# Criar diret√≥rios
RUN mkdir -p /app/data/raw /app/data/processed /app/data/consolidated \
    /app/data/analysis /app/data/reports /app/data/cookies \
    /app/data/cache /app/data/logs

# Script de inicializa√ß√£o
COPY scripts/start-worker.sh /start-worker.sh
RUN chmod +x /start-worker.sh

# Comando padr√£o
CMD ["/start-worker.sh"]
```

### scripts/start-worker.sh

```bash
#!/bin/bash

# Iniciar Xvfb para display virtual (se n√£o headless)
if [ "$BROWSER_HEADLESS" != "true" ]; then
    Xvfb :99 -screen 0 1920x1080x24 &
    export DISPLAY=:99
fi

# Iniciar worker
exec rq worker --url "$REDIS_URL" --with-scheduler
```

---

## ‚úÖ Checklist de Implementa√ß√£o Definitivo

### FASE 1: Core (Dia 1-2)
- [ ] 1.1 Criar estrutura de diret√≥rios completa
- [ ] 1.2 Implementar `config.py` com todas as settings
- [ ] 1.3 Criar `.env.example` e `.gitignore`
- [ ] 1.4 Implementar `requirements.txt`
- [ ] 1.5 Implementar logger estruturado

### FASE 2: Fetchers (Dia 3-5)
- [ ] 2.1 Implementar `BaseFetcher` e `FetchResult`
- [ ] 2.2 Implementar `APIFetcher` (BRAPI, BCB, IBGE, CoinGecko)
- [ ] 2.3 Implementar `SimpleFetcher` (requests)
- [ ] 2.4 Implementar `SessionManager` (cookies)
- [ ] 2.5 Implementar `BrowserFetcher` (nodriver base)
- [ ] 2.6 Implementar `GoogleAuthFetcher`
- [ ] 2.7 Implementar `CredentialsFetcher`
- [ ] 2.8 Implementar `LLMInterfaceFetcher`
- [ ] 2.9 Testar cada fetcher individualmente

### FASE 3: Parsers (Dia 6-8)
- [ ] 3.1 Implementar `HTMLToMarkdown` converter
- [ ] 3.2 Implementar parser `brapi`
- [ ] 3.3 Implementar parser `fundamentus`
- [ ] 3.4 Implementar parser `investidor10`
- [ ] 3.5 Implementar parsers restantes (30+)
- [ ] 3.6 Implementar `DataExtractor` gen√©rico

### FASE 4: Storage & Queue (Dia 9-10)
- [ ] 4.1 Implementar SQLite models
- [ ] 4.2 Implementar `Storage` layer
- [ ] 4.3 Implementar Redis `Cache`
- [ ] 4.4 Implementar `Queue` (RQ)
- [ ] 4.5 Implementar `Worker`

### FASE 5: An√°lise (Dia 11-13)
- [ ] 5.1 Implementar `DataConsolidator`
- [ ] 5.2 Implementar prompts (daytrade, swing, position)
- [ ] 5.3 Implementar `LLMAnalyzer`
- [ ] 5.4 Implementar `ReportGenerator`
- [ ] 5.5 Testar pipeline completo de an√°lise

### FASE 6: Orquestra√ß√£o (Dia 14-15)
- [ ] 6.1 Implementar `Scheduler` (APScheduler)
- [ ] 6.2 Implementar FastAPI `routes`
- [ ] 6.3 Implementar CLI (Typer)
- [ ] 6.4 Testar agendamento

### FASE 7: Docker & Deploy (Dia 16-17)
- [ ] 7.1 Criar `Dockerfile`
- [ ] 7.2 Criar `Dockerfile.worker`
- [ ] 7.3 Criar `docker-compose.yml`
- [ ] 7.4 Testar containers
- [ ] 7.5 Documentar processo de deploy

### FASE 8: Testes & Documenta√ß√£o (Dia 18-20)
- [ ] 8.1 Escrever testes unit√°rios
- [ ] 8.2 Escrever testes de integra√ß√£o
- [ ] 8.3 Escrever documenta√ß√£o (README, SETUP, USAGE)
- [ ] 8.4 Validar com dados reais
- [ ] 8.5 Ajustes finais

---

## üöÄ Prompt para Claude Code

```
Implemente o projeto BRScraper v3 seguindo a especifica√ß√£o em brscraper-spec-v3.md.

CONTEXTO CR√çTICO:
- O usu√°rio j√° possui sistema de coleta de cookies do Google
- Cookies s√£o fornecidos via arquivo JSON em ./data/cookies/google_session.json
- Objetivo final: an√°lise de trading (daytrade, swing, position)
- 36 sites mapeados com tipos de acesso diferentes
- 6 LLMs para automa√ß√£o de an√°lise (ChatGPT, Claude, etc.)
- Tudo 100% gratuito e self-hosted
- Credenciais sens√≠veis em .env (nunca hardcoded)

PRIORIDADE DE IMPLEMENTA√á√ÉO:

1. CORE (primeiro)
   - config.py com pydantic-settings
   - Estrutura de diret√≥rios
   - Logger estruturado

2. FETCHERS (em ordem)
   - APIFetcher (BRAPI, BCB - testar primeiro!)
   - SimpleFetcher (Fundamentus - segundo teste)
   - SessionManager (integrar com arquivo de cookies)
   - BrowserFetcher + GoogleAuthFetcher
   - LLMInterfaceFetcher (ChatGPT)

3. PARSERS
   - HTMLToMarkdown gen√©rico
   - Parsers espec√≠ficos por site

4. AN√ÅLISE
   - DataConsolidator
   - Prompts de trading
   - LLMAnalyzer

5. ORQUESTRA√á√ÉO
   - Scheduler
   - Worker
   - API

TESTES OBRIGAT√ìRIOS:
1. BRAPI API ‚Üí deve retornar cota√ß√£o de PETR4
2. Fundamentus ‚Üí deve extrair indicadores
3. Investidor10 com cookies ‚Üí deve acessar sem bloqueio
4. ChatGPT com cookies ‚Üí deve enviar prompt e receber resposta
```

---

**FIM DA ESPECIFICA√á√ÉO v3 - PRONTA PARA IMPLEMENTA√á√ÉO**
