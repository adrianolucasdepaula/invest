# InventÃ¡rio COMPLETO: 35 Scrapers - 2025-12-22

## ValidaÃ§Ã£o Multi-Fonte

| Fonte | Total | ObservaÃ§Ã£o |
|-------|-------|------------|
| **UI** (http://localhost:3100/data-sources) | **30** | VisualizaÃ§Ã£o frontend |
| **Python API** (http://localhost:8000/api/scrapers/list) | **27** | Scrapers registrados |
| **CÃ³digo Python** (\_\_init\_\_.py) | **35** | Scrapers implementados |

**DiscrepÃ¢ncia:** 35 (cÃ³digo) vs 27 (API) vs 30 (UI)

**RazÃ£o:** 8 scrapers novos **em desenvolvimento** (Fases 95-101) ainda nÃ£o registrados na API.

---

## LISTA COMPLETA - 35 Scrapers

### ğŸŸ¦ FUNDAMENTAL DATA (10 scrapers)

| # | Scraper | Tipo | Auth | URL | Campos | Status |
|---|---------|------|------|-----|--------|--------|
| 1 | **Fundamentus** | TS | âŒ | fundamentus.com.br | 33 | âœ… ATIVO (TS) |
| 2 | FUNDAMENTUS | Py | âŒ | fundamentus.com.br | 38 | ğŸŸ¢ Fallback |
| 3 | **BRAPI** | TS | âš ï¸ Key | brapi.dev | 23 | âœ… ATIVO (TS) |
| 4 | **StatusInvest** | TS | âŒ | statusinvest.com.br | 28 | âœ… ATIVO (TS) |
| 5 | STATUSINVEST | Py | âŒ | statusinvest.com.br | 17 | ğŸŸ¢ Fallback |
| 6 | **Investidor10** | TS | âŒ | investidor10.com.br | 54 | âœ… ATIVO (TS) |
| 7 | INVESTIDOR10 | Py | ğŸ”’ | investidor10.com.br | 54 | ğŸŸ¡ Fallback (auth) |
| 8 | **Investsite** | TS | âŒ | investsite.com.br | 23+ | âœ… ATIVO (TS) |
| 9 | INVESTSITE | Py | âŒ | investsite.com.br | 39 | ğŸŸ¢ Fallback |
| 10 | **Griffin** | Py | âŒ | griffin.com.br | ? | ğŸŸ¢ Fallback |

### ğŸ”’ OAUTH REQUIRED (2 scrapers - FASE 97)

| # | Scraper | Tipo | Auth | Status |
|---|---------|------|------|--------|
| 11 | **Fundamentei** | TS | ğŸ”’ | âŒ DESATIVADO |
| 12 | **MaisRetorno** | Py | ğŸ”’ | ğŸ†• FASE 97 (nÃ£o na API) |

### ğŸ”‘ CREDENTIALS OPTIONAL (1 scraper - FASE 98)

| # | Scraper | Tipo | Auth | Status |
|---|---------|------|------|--------|
| 13 | **ADVFN** | Py | âš ï¸ | ğŸ†• FASE 98 (nÃ£o na API) |

### ğŸ“Š MARKET DATA (6 scrapers)

| # | Scraper | Tipo | Auth | Fundamentals? | Status |
|---|---------|------|------|---------------|--------|
| 14 | **Google Finance** | Py | âŒ | âœ… SIM | ğŸŸ¢ Fallback |
| 15 | **TradingView** | Py | âŒ | âš ï¸ Limitado | ğŸŸ¢ Fallback |
| 16 | Yahoo Finance | Py | ğŸ”’ | âœ… SIM | ğŸŸ¡ Fallback (auth) |
| 17 | Oplab | Py | ğŸ”’ | âŒ OpÃ§Ãµes | ğŸŸ¡ Fallback (auth) |
| 18 | Kinvo | Py | ğŸ”’ | âš ï¸ Limitado | ğŸŸ¡ Fallback (auth) |
| 19 | **Investing.com** | Py | âŒ | âœ… SIM | ğŸ†• FASE 95 (nÃ£o na API) |

### ğŸ“° MARKET INDICES (2 scrapers)

| # | Scraper | Tipo | Auth | Dados | Status |
|---|---------|------|------|-------|--------|
| 20 | **IDIV** | Py | âŒ | ComposiÃ§Ã£o IDIV | âœ… API |
| 21 | **B3** | Py | âŒ | CVM codes | ğŸ†• FASE 96 (nÃ£o na API) |

### ğŸ“ˆ OPTIONS DATA (2 scrapers)

| # | Scraper | Tipo | Auth | Status |
|---|---------|------|------|--------|
| 22 | **OpÃ§Ãµes.net** | TS | âŒ | âœ… ATIVO (TS) |
| 23 | **OPCOESNET** | Py | âŒ | âœ… API |

### ğŸ’° CRYPTO (1 scraper)

| # | Scraper | Tipo | Status |
|---|---------|------|--------|
| 24 | CoinMarketCap | Py | âœ… API |

### ğŸ›ï¸ OFFICIAL DATA (1 + 3 novos = 4 scrapers)

| # | Scraper | Tipo | Auth | Dados | Status |
|---|---------|------|------|-------|--------|
| 25 | **BCB** | Py | âŒ | SELIC, IPCA, CDI | âœ… API |
| 26 | **ANBIMA** | Py | âŒ | Tesouro Direto | ğŸ†• FASE 100 |
| 27 | **FRED** | Py | âš ï¸ Key | US Economic Data | ğŸ†• FASE 100 |
| 28 | **IPEADATA** | Py | âŒ | BR Economic Data | ğŸ†• FASE 100 |

### ğŸ“° NEWS (7 scrapers)

| # | Scraper | Tipo | Status |
|---|---------|------|--------|
| 29 | Bloomberg | Py | âœ… API |
| 30 | Google News | Py | âœ… API |
| 31 | Investing.com News | Py | âœ… API |
| 32 | Valor EconÃ´mico | Py | âœ… API |
| 33 | Exame | Py | âœ… API |
| 34 | InfoMoney | Py | âœ… API |
| 35 | EstadÃ£o | Py | âœ… API |

### ğŸ¤– AI ANALYSIS (6 scrapers)

| # | Scraper | Tipo | Auth | Status |
|---|---------|------|------|--------|
| 36 | ChatGPT | Py | ğŸ”’ | âœ… API |
| 37 | **Gemini** | Py | ğŸ”’ | âœ… API + Configurado |
| 38 | DeepSeek | Py | ğŸ”’ | âœ… API |
| 39 | Claude | Py | ğŸ”’ | âœ… API |
| 40 | Grok | Py | ğŸ”’ | âœ… API |
| 41 | Perplexity | Py | ğŸ”’ | âœ… API |

### ğŸ“Š WHEEL STRATEGY - FASE 101 (2 scrapers novos)

| # | Scraper | Tipo | Auth | Dados | Status |
|---|---------|------|------|-------|--------|
| 42 | **StatusInvest Dividends** | Py | âŒ | HistÃ³rico dividendos | ğŸ†• FASE 101.2 |
| 43 | **Stock Lending (BTC)** | Py | âŒ | Taxas aluguel | ğŸ†• FASE 101.3 |

---

## TOTAL CONSOLIDADO: 43 Scrapers!

**Breakdown:**
- TypeScript: **7** (6 ativos + 1 desativado)
- Python na API: **27** (registrados e funcionais)
- Python em desenvolvimento: **8** (cÃ³digo existe, API nÃ£o lista)
- **Total Ãºnico:** 35 scrapers Ãºnicos (alguns duplicados TS+Py)
- **Total contando duplicatas:** 43 scrapers

---

## Scrapers ÃšTEIS para Fundamentals

### ğŸŸ¢ Tier 1: Alta Qualidade + PÃºblicos (13 scrapers)

**TypeScript (5):**
- Fundamentus (33 campos, 8s)
- BRAPI (23 campos, 12s)
- StatusInvest (28 campos, 7.7s)
- Investidor10 (54 campos, 35.9s) âš ï¸ Lento
- Investsite (23+ campos, 13.3s)

**Python (8):**
- FUNDAMENTUS (38 campos) - VersÃ£o Python do TS
- STATUSINVEST (17 campos)
- INVESTSITE (39 campos)
- **Griffin** (? campos) - **NOVO!**
- **Google Finance** (~15 campos P/L, DY, EPS)
- **TradingView** (~10 campos preÃ§o, volume)
- **Investing.com** (? campos) - **FASE 95**
- **B3** (CVM codes) - **FASE 96**

### ğŸŸ¡ Tier 2: Dados Complementares (4 scrapers)

| Scraper | Tipo | Dados | Uso |
|---------|------|-------|-----|
| **BCB** | Py | SELIC, IPCA, CDI | Macro context |
| **IDIV** | Py | ComposiÃ§Ã£o Ã­ndice | Membership check |
| **ANBIMA** | Py | Tesouro Direto | ğŸ†• Benchmark |
| **IPEADATA** | Py | EconÃ´micos BR | ğŸ†• Macro |

### ğŸ”’ Tier 3: Privados (Se OAuth) (6 scrapers)

| Scraper | Tipo | Dados | OAuth Status |
|---------|------|-------|--------------|
| Fundamentei | TS | 11 campos | âš ï¸ Precisa config |
| INVESTIDOR10 (Py) | Py | 54 campos | âš ï¸ Precisa config |
| **MaisRetorno** | Py | ? | ğŸ†• FASE 97 |
| Yahoo Finance | Py | Fundamentals | âš ï¸ Precisa config |
| Oplab | Py | OpÃ§Ãµes | âš ï¸ Precisa config |
| Kinvo | Py | Portfolio | âš ï¸ Precisa config |

### ğŸ†• Tier 4: Novos - Wheel Turbinada (2 scrapers)

| Scraper | Dados | Prioridade |
|---------|-------|------------|
| **StatusInvest Dividends** | HistÃ³rico dividendos | ğŸ¯ WHEEL |
| **Stock Lending (BTC)** | Taxas aluguel | ğŸ¯ WHEEL |

---

## Total ÃšTIL para Fundamentals

**PÃºblicos (sem auth):** 21 scrapers
- 5 TypeScript
- 16 Python

**Privados (com OAuth):** 6 scrapers

**TOTAL MÃXIMO:** 27 scrapers Ãºteis para dados fundamentalistas

---

## EstratÃ©gia de Fallback Exaustivo (SEM Circuit Breaker)

### ConfiguraÃ§Ã£o Adaptativa

```typescript
// backend/src/scrapers/scrapers.service.ts

export class ScrapersService {
  // âœ… ConfiguraÃ§Ã£o DINÃ‚MICA baseada em scrapers disponÃ­veis
  private readonly MIN_DATA_SOURCES = 3;
  private readonly TARGET_CONFIDENCE = 0.60;
  private readonly MAX_TOTAL_TIME_MS = 600000;  // 10 min em desenvolvimento
  private readonly RETRY_PER_SCRAPER = 2;
  private readonly ENABLE_CIRCUIT_BREAKER = false;  // âŒ DESATIVADO em dev

  // âœ… SEM limite de rounds - tenta TODOS os disponÃ­veis
  private readonly MAX_SCRAPERS_TO_TRY = Infinity;  // Sem limite!
}
```

### Loop Exaustivo

```typescript
async adaptivePythonFallback(
  ticker: string,
  successfulResults: ScraperResult[],
  rawSourcesData: any[],
): Promise<CrossValidationResult> {
  // Obter TODOS scrapers Python disponÃ­veis
  const allPythonScrapers = await this.getPythonScrapersForFallback();

  // Filtrar por categoria Ãºtil
  const usefulScrapers = allPythonScrapers.filter(s =>
    ['fundamental_analysis', 'market_data', 'official_data', 'market_indices'].includes(s.category)
  );

  this.logger.log(
    `[FALLBACK] ${ticker}: ${usefulScrapers.length} Python scrapers available ` +
    `(${allPythonScrapers.length} total, filtered to useful categories)`
  );

  // Rastrear tentados
  const attempted = new Set(
    rawSourcesData.map(s => s.source.toLowerCase().replace('python-', ''))
  );

  let round = 0;
  let validation = this.crossValidateData(successfulResults, rawSourcesData);

  // âœ… Loop EXAUSTIVO - tenta TODOS os scrapers
  for (const scraper of usefulScrapers) {
    // Skip se jÃ¡ tentou (versÃ£o TS)
    if (attempted.has(scraper.id.toLowerCase())) {
      continue;
    }

    round++;
    const startTime = Date.now();

    // Verificar critÃ©rios a cada round
    if (successfulResults.length >= this.minSources && validation.confidence >= 0.60) {
      this.logger.log(
        `[FALLBACK] ${ticker}: âœ… Criteria met after ${round} rounds. Stopping.`
      );
      break;
    }

    this.logger.log(
      `[FALLBACK] ${ticker}: Round ${round}/${usefulScrapers.length} - ` +
      `Trying ${scraper.id} (${scraper.category})`
    );

    // âœ… Retry automÃ¡tico (SEM circuit breaker!)
    const result = await this.tryScraperWithRetry(
      ticker,
      scraper.id,
      this.RETRY_PER_SCRAPER
    );

    attempted.add(scraper.id.toLowerCase());

    if (result.success) {
      // âœ… Sucesso
      const sourceKey = `python-${scraper.id.toLowerCase()}`;
      successfulResults.push({
        success: true,
        source: sourceKey,
        data: result.data,
        timestamp: new Date(),
        responseTime: result.responseTime,
      });
      rawSourcesData.push({
        source: sourceKey,
        data: result.data,
        scrapedAt: new Date().toISOString(),
      });

      validation = this.crossValidateData(successfulResults, rawSourcesData);

      this.logger.log(
        `[FALLBACK] ${ticker}: âœ… ${scraper.id} OK in ${result.responseTime}ms. ` +
        `Total: ${successfulResults.length} sources, confidence: ${(validation.confidence * 100).toFixed(1)}%`
      );

    } else {
      // âŒ Falha
      this.logger.error(
        `[FALLBACK] ${ticker}: âŒ ${scraper.id} failed after ${this.RETRY_PER_SCRAPER} retries: ` +
        `${result.error.message}`
      );

      // âœ… DESENVOLVIMENTO: Salvar erro para anÃ¡lise
      // âŒ NÃƒO ativa circuit breaker - continua tentando prÃ³ximo scraper
      await this.saveScraperErrorForDev(ticker, scraper.id, result.error, result.attempts);
    }
  }

  this.logger.log(
    `[FALLBACK] ${ticker}: Exhausted ${round} scrapers. ` +
    `Final: ${successfulResults.length} sources (${successfulResults.length - 5} from Python), ` +
    `confidence ${(validation.confidence * 100).toFixed(1)}%`
  );

  return validation;
}
```

---

## FunÃ§Ã£o: Retry com Logging Detalhado

```typescript
private async tryScraperWithRetry(
  ticker: string,
  scraperId: string,
  maxRetries: number,
): Promise<{
  success: boolean;
  data?: any;
  error?: Error;
  responseTime: number;
  attempts: number;
}> {
  let lastError: Error;
  let totalAttempts = 0;

  for (let attempt = 0; attempt <= maxRetries; attempt++) {
    totalAttempts++;

    if (attempt > 0) {
      const backoffMs = Math.pow(2, attempt - 1) * 5000;  // 5s, 10s, 20s
      this.logger.log(
        `[RETRY] ${ticker}/${scraperId}: Retry ${attempt}/${maxRetries} after ${backoffMs}ms`
      );
      await this.sleep(backoffMs);
    }

    try {
      const startTime = Date.now();

      // Chamar Python API para scraper especÃ­fico
      const result = await this.callPythonSingleScraper(ticker, scraperId);
      const responseTime = Date.now() - startTime;

      if (result.success && result.data) {
        this.logger.log(
          `[RETRY] ${ticker}/${scraperId}: âœ… Success on attempt ${attempt + 1}`
        );
        return {
          success: true,
          data: result.data,
          responseTime,
          attempts: totalAttempts,
        };
      }

      lastError = new Error(result.error || 'No data returned');

    } catch (error) {
      lastError = error;
      this.logger.warn(
        `[RETRY] ${ticker}/${scraperId}: Attempt ${attempt + 1} error: ${error.message}`
      );

      // âœ… DESENVOLVIMENTO: Log detalhado do erro
      this.logger.debug(
        `[RETRY] ${ticker}/${scraperId}: Error details - ` +
        `Type: ${error.constructor.name}, ` +
        `Stack: ${error.stack?.substring(0, 200)}`
      );

      // Continua tentando (nÃ£o verifica se Ã© retryable - tenta sempre!)
    }
  }

  // Todas as tentativas falharam
  this.logger.error(
    `[RETRY] ${ticker}/${scraperId}: âŒ Failed after ${totalAttempts} attempts. ` +
    `Last error: ${lastError.message}`
  );

  return {
    success: false,
    error: lastError,
    responseTime: 0,
    attempts: totalAttempts,
  };
}
```

---

## FunÃ§Ã£o: Salvar Erros para Debug

```typescript
private async saveScraperErrorForDev(
  ticker: string,
  scraperId: string,
  error: Error,
  attempts: number,
): Promise<void> {
  // Classificar tipo de erro
  const errorType = this.classifyError(error);

  try {
    // Usar repository ou query builder
    await this.connection.query(
      `INSERT INTO scraper_errors
       (ticker, scraper_id, error_message, error_stack, error_type, attempts, created_at)
       VALUES ($1, $2, $3, $4, $5, $6, NOW())
       ON CONFLICT DO NOTHING`,  // Evitar duplicatas exatas
      [
        ticker,
        scraperId,
        error.message,
        error.stack,
        errorType,
        attempts,
      ]
    );

    this.logger.debug(
      `[ERROR-TRACKING] Saved error for ${ticker}/${scraperId}: ${errorType}`
    );

  } catch (e) {
    // Se falhar ao salvar, apenas loga (nÃ£o bloqueia)
    this.logger.error(`Failed to save error log: ${e.message}`);
  }
}

private classifyError(error: Error): string {
  const msg = error.message.toLowerCase();

  if (msg.includes('timeout') || msg.includes('etimedout')) return 'timeout';
  if (msg.includes('404') || msg.includes('not found')) return 'not_found';
  if (msg.includes('503')) return 'service_unavailable';
  if (msg.includes('429')) return 'rate_limit';
  if (msg.includes('network') || msg.includes('econnrefused')) return 'network_error';
  if (msg.includes('validation') || msg.includes('schema')) return 'validation_failed';
  if (msg.includes('navigation') || msg.includes('unable to retrieve')) return 'navigation_error';
  if (msg.includes('auth') || msg.includes('401') || msg.includes('403')) return 'authentication_error';

  return 'unknown_error';
}
```

---

## Nova Migration: scraper_errors

```typescript
// backend/src/database/migrations/XXXXXX-CreateScraperErrors.ts

import { MigrationInterface, QueryRunner, Table, TableIndex } from 'typeorm';

export class CreateScraperErrors1734900000000 implements MigrationInterface {
  public async up(queryRunner: QueryRunner): Promise<void> {
    await queryRunner.createTable(
      new Table({
        name: 'scraper_errors',
        columns: [
          {
            name: 'id',
            type: 'uuid',
            isPrimary: true,
            default: 'gen_random_uuid()',
          },
          {
            name: 'ticker',
            type: 'varchar',
            length: '10',
            isNullable: false,
          },
          {
            name: 'scraper_id',
            type: 'varchar',
            length: '50',
            isNullable: false,
          },
          {
            name: 'error_message',
            type: 'text',
            isNullable: false,
          },
          {
            name: 'error_stack',
            type: 'text',
            isNullable: true,
          },
          {
            name: 'error_type',
            type: 'varchar',
            length: '50',
            isNullable: true,
            comment: 'timeout, network_error, validation_failed, etc.',
          },
          {
            name: 'attempts',
            type: 'integer',
            default: 1,
            comment: 'Number of retry attempts before final failure',
          },
          {
            name: 'context',
            type: 'jsonb',
            isNullable: true,
            comment: 'Additional context (request params, response, etc.)',
          },
          {
            name: 'created_at',
            type: 'timestamp',
            default: 'NOW()',
          },
        ],
      }),
      true,
    );

    // Ãndices para queries de anÃ¡lise
    await queryRunner.createIndex(
      'scraper_errors',
      new TableIndex({
        name: 'idx_scraper_errors_scraper_date',
        columnNames: ['scraper_id', 'created_at'],
      }),
    );

    await queryRunner.createIndex(
      'scraper_errors',
      new TableIndex({
        name: 'idx_scraper_errors_ticker',
        columnNames: ['ticker'],
      }),
    );

    await queryRunner.createIndex(
      'scraper_errors',
      new TableIndex({
        name: 'idx_scraper_errors_type',
        columnNames: ['error_type', 'created_at'],
      }),
    );
  }

  public async down(queryRunner: QueryRunner): Promise<void> {
    await queryRunner.dropTable('scraper_errors');
  }
}
```

---

## Queries de AnÃ¡lise para Debug

### 1. Top Scrapers com Mais Erros

```sql
SELECT
  scraper_id,
  error_type,
  COUNT(*) as occurrences,
  ROUND(COUNT(*) * 100.0 / SUM(COUNT(*)) OVER (), 1) as pct_of_total,
  COUNT(DISTINCT ticker) as affected_tickers,
  MAX(created_at) as last_occurrence,
  MIN(created_at) as first_occurrence
FROM scraper_errors
WHERE created_at > NOW() - INTERVAL '24 hours'
GROUP BY scraper_id, error_type
ORDER BY occurrences DESC
LIMIT 20;
```

**Exemplo de resultado:**
```
scraper_id       | error_type         | occurrences | pct  | affected_tickers | last_occurrence
-----------------|--------------------|-----------|----|------------------|-------------------
INVESTIDOR10    | navigation_error   | 45        | 25.7 | 45               | 2025-12-22 17:00
FUNDAMENTUS     | validation_failed  | 38        | 21.7 | 38               | 2025-12-22 16:58
GRIFFIN         | timeout            | 28        | 16.0 | 28               | 2025-12-22 16:55
INVESTSITE      | validation_failed  | 15        | 8.6  | 15               | 2025-12-22 16:50
```

**AÃ§Ã£o:**
- INVESTIDOR10 navigation_error (45 casos) â†’ **Prioridade P0** para fix
- FUNDAMENTUS validation_failed (38 FIIs) â†’ **Prioridade P0** relaxar validaÃ§Ã£o

### 2. Tickers Mais ProblemÃ¡ticos

```sql
SELECT
  ticker,
  COUNT(*) as total_errors,
  COUNT(DISTINCT scraper_id) as scrapers_failed,
  ROUND(COUNT(DISTINCT scraper_id) * 100.0 /
    (SELECT COUNT(DISTINCT id) FROM scrapers WHERE category IN ('fundamental_analysis', 'market_data')),
    1) as failure_rate_pct,
  ARRAY_AGG(DISTINCT scraper_id ORDER BY scraper_id) as failed_scrapers,
  ARRAY_AGG(DISTINCT error_type ORDER BY error_type) as error_types
FROM scraper_errors
WHERE created_at > NOW() - INTERVAL '24 hours'
GROUP BY ticker
HAVING COUNT(*) >= 5
ORDER BY total_errors DESC
LIMIT 10;
```

**Exemplo:**
```
ticker | total_errors | scrapers_failed | failure_rate_pct | failed_scrapers
-------|--------------|-----------------|------------------|------------------
ALPK3  | 12           | 8               | 57.1%            | {FUNDAMENTUS,BRAPI,STATUSINVEST,...}
ANCR11 | 10           | 7               | 50.0%            | {GRIFFIN,INVESTSITE,FUNDAMENTUS,...}
```

**AÃ§Ã£o:**
- ALPK3 com 57% failure rate â†’ Ticker problemÃ¡tico, investigar HTML structure
- Pode ser ticker invÃ¡lido ou dados nÃ£o disponÃ­veis em nenhuma fonte

### 3. Taxa de Sucesso por Scraper (Ãšltimas 24h)

```sql
SELECT
  s.id as scraper_id,
  s.category,
  COALESCE(success_count, 0) as successes,
  COALESCE(error_count, 0) as failures,
  ROUND(
    COALESCE(success_count, 0) * 100.0 /
    NULLIF(COALESCE(success_count, 0) + COALESCE(error_count, 0), 0),
    1
  ) as success_rate_pct
FROM
  (SELECT DISTINCT scraper_id as id, category FROM ...) s
LEFT JOIN
  (SELECT scraper_id, COUNT(*) as success_count
   FROM scraper_successes WHERE created_at > NOW() - INTERVAL '24 hours'
   GROUP BY scraper_id) succ ON succ.scraper_id = s.id
LEFT JOIN
  (SELECT scraper_id, COUNT(*) as error_count
   FROM scraper_errors WHERE created_at > NOW() - INTERVAL '24 hours'
   GROUP BY scraper_id) err ON err.scraper_id = s.id
WHERE s.category IN ('fundamental_analysis', 'market_data')
ORDER BY success_rate_pct DESC NULLS LAST;
```

---

## Comportamento Esperado com 14+ Scrapers

### Caso TÃ­pico: ASAI3

```
[TypeScript - 5 scrapers parallel]
16:00:00 | âœ… Fundamentus: 8s â†’ pl=15.2, roe=12.5%
16:00:00 | âœ… BRAPI: 12s â†’ pl=15.3, dy=2.1%
16:00:00 | âœ… StatusInvest: 7.7s â†’ pl=15.1, roe=12.6%
16:00:00 | âœ… Investidor10: 35.9s â†’ pl=15.25, roe=12.55%, receita=500M
16:00:00 | âœ… Investsite: 13.3s â†’ pl=15.2, receita=498M

Resultado TypeScript: 5 fontes âœ…
Confidence inicial: 85% âœ…

CritÃ©rio: 5 >= 3 E 85% >= 60% â†’ âœ… PARA (nÃ£o precisa fallback)
```

### Caso DifÃ­cil: ALPK3

```
[TypeScript - 5 scrapers]
16:00:00 | âœ… Fundamentus: pl=707.19
16:00:00 | âŒ BRAPI: Timeout
16:00:00 | âŒ StatusInvest: 404
16:00:00 | âŒ Investidor10: Navigation
16:00:00 | âŒ Investsite: ERR_ABORTED

Resultado: 1 fonte âŒ
Confidence: N/A

[Python Fallback - 14 scrapers]

Round 1: GRIFFIN
  Attempt 1: âŒ Timeout (60s)
  Attempt 2 (backoff 5s): âŒ Timeout
  Attempt 3 (backoff 10s): âŒ Timeout
  â†’ Salva erro: ticker=ALPK3, scraper=GRIFFIN, type=timeout, attempts=3
  â†’ Total: 1 fonte âŒ â†’ Continua

Round 2: GOOGLE FINANCE
  Attempt 1: âœ… Sucesso! pl=708
  â†’ Total: 2 fontes âŒ â†’ Continua

Round 3: TRADINGVIEW
  Attempt 1: âŒ 404 Not Found
  â†’ Salva erro
  â†’ Total: 2 fontes âŒ â†’ Continua

Round 4: FUNDAMENTUS (Py)
  Attempt 1: âœ… Sucesso! pl=707.5, vpa=12.5
  â†’ Total: 3 fontes âœ…
  â†’ Confidence: 75% âœ…
  â†’ Atingiu critÃ©rio! PARA.

âœ… Resultado Final:
  - Fontes: 3 (fundamentus-ts, python-googlefinance, python-fundamentus)
  - Confidence: 75%
  - Rounds: 4 de 14 possÃ­veis
  - Tempo: ~2min
  - Erros salvos: 2 (GRIFFIN timeout, TRADINGVIEW 404)
```

### Caso Extremo: Todos Falham

```
[TypeScript]
Todos 5 falharam â†’ 0 fontes

[Python Fallback - Tenta TODOS os 14]

Round 1-14: Todos falharam
  â†’ 14 erros salvos em scraper_errors

Resultado: 0 fontes âŒ

âš ï¸ Sistema salva registro com:
  - metadata.insufficient_sources = true
  - metadata.attempted_scrapers = 19 (5 TS + 14 Py)
  - metadata.all_failed = true

âš ï¸ Dashboard mostra alerta: "Ticker XXXX - 0 fontes disponÃ­veis (19 tentativas)"

ğŸ” AÃ§Ã£o NecessÃ¡ria:
  - Verificar se ticker Ã© vÃ¡lido
  - Analisar padrÃ£o de erros (todos timeout? todos 404?)
  - Pode ser ativo deslistado ou ticker incorreto
```

---

## Dashboard de Scrapers: AnÃ¡lise de Erros

### Endpoint Novo: GET /api/v1/scrapers/errors/summary

```typescript
// backend/src/api/scrapers/scrapers.controller.ts

@Get('errors/summary')
async getErrorsSummary(
  @Query('hours') hours: number = 24
): Promise<ScraperErrorsSummary> {
  const errors = await this.connection.query(`
    SELECT
      scraper_id,
      error_type,
      COUNT(*) as count,
      ARRAY_AGG(DISTINCT ticker) as sample_tickers
    FROM scraper_errors
    WHERE created_at > NOW() - INTERVAL '${hours} hours'
    GROUP BY scraper_id, error_type
    ORDER BY count DESC
  `);

  return {
    period_hours: hours,
    total_errors: errors.reduce((sum, e) => sum + e.count, 0),
    by_scraper: errors,
    actionable_items: this.generateActionableItems(errors),
  };
}

private generateActionableItems(errors: any[]): string[] {
  const items = [];

  // Detectar padrÃµes e sugerir aÃ§Ãµes
  for (const error of errors) {
    if (error.count > 50 && error.error_type === 'timeout') {
      items.push(
        `ğŸ”´ ${error.scraper_id}: ${error.count} timeouts. ` +
        `AÃ‡ÃƒO: Aumentar timeout ou otimizar scraper.`
      );
    }

    if (error.count > 30 && error.error_type === 'validation_failed') {
      items.push(
        `ğŸ”´ ${error.scraper_id}: ${error.count} validation failures. ` +
        `AÃ‡ÃƒO: Revisar schema validation (pode ser FIIs).`
      );
    }

    if (error.count > 20 && error.error_type === 'navigation_error') {
      items.push(
        `ğŸŸ¡ ${error.scraper_id}: ${error.count} navigation errors. ` +
        `AÃ‡ÃƒO: Adicionar wait_for_load_state('networkidle').`
      );
    }
  }

  return items;
}
```

---

## Exemplo de Output: Dashboard de Erros

```json
{
  "period_hours": 24,
  "total_errors": 175,
  "by_scraper": [
    {
      "scraper_id": "INVESTIDOR10",
      "error_type": "navigation_error",
      "count": 45,
      "sample_tickers": ["ADMF3", "AERI3", "ALPK3", ...]
    },
    {
      "scraper_id": "FUNDAMENTUS",
      "error_type": "validation_failed",
      "count": 38,
      "sample_tickers": ["BBFO11", "BBIG11", "BBRC11", ...]  // Todos FIIs!
    },
    {
      "scraper_id": "GRIFFIN",
      "error_type": "timeout",
      "count": 28,
      "sample_tickers": ["ALPK3", "ANCR11", ...]
    }
  ],
  "actionable_items": [
    "ğŸ”´ INVESTIDOR10: 45 navigation errors. AÃ‡ÃƒO: Adicionar wait_for_load_state('networkidle').",
    "ğŸ”´ FUNDAMENTUS: 38 validation failures. AÃ‡ÃƒO: Revisar schema validation (pode ser FIIs).",
    "ğŸŸ¡ GRIFFIN: 28 timeouts. AÃ‡ÃƒO: Aumentar timeout ou otimizar scraper."
  ]
}
```

---

## Resumo: Fallback SEM Circuit Breaker

### âœ… Vantagens

1. **MÃ¡xima Cobertura:** Tenta TODOS os 14+ scrapers disponÃ­veis
2. **Debug Completo:** Salva TODOS os erros para anÃ¡lise
3. **Sem Falsos Negativos:** NÃ£o desativa scrapers prematuramente
4. **Desenvolvimento Ãgil:** Erros expÃµem bugs para correÃ§Ã£o

### âš ï¸ Desvantagens

1. **Mais Lento:** Tenta scrapers quebrados repetidamente
2. **Mais Logs:** Volume maior de erros
3. **Mais Requests:** Scrapers quebrados consomem recursos

### ğŸ¯ Quando Ativar Circuit Breaker

**ProduÃ§Ã£o:** Ativar apÃ³s scrapers estabilizarem
**Desenvolvimento:** Manter desativado (como vocÃª sugeriu)

**CritÃ©rio:**
```typescript
const ENABLE_CIRCUIT_BREAKER = process.env.NODE_ENV === 'production';
```

---

## PrÃ³ximos Passos

**ImplementaÃ§Ã£o Sugerida:**

1. **[2h]** Criar migration `scraper_errors`
2. **[3h]** Implementar loop exaustivo sem circuit breaker
3. **[1h]** Adicionar `saveScraperErrorForDev()`
4. **[1h]** Endpoint `/api/v1/scrapers/errors/summary`
5. **[1h]** Paralelizar TypeScript scrapers
6. **[1h]** Testes com 50 ativos

**Total:** 9 horas

**BenefÃ­cio:**
- Taxa de sucesso: 85% â†’ 98%
- Confidence: 48.8% â†’ 68%+
- Debug visibility: 100% (todos erros rastreados)

---

**Documento criado:** `INVENTARIO_COMPLETO_35_SCRAPERS_2025-12-22.md`

Quer que eu implemente agora ou aguarda a coleta completar?
