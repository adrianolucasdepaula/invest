# üöÄ Guia de Configura√ß√£o - B3 AI Analysis Platform

**√öltima Atualiza√ß√£o:** 2025-01-08
**Vers√£o:** 2.0
**Fase:** 1 - Prepara√ß√£o e Configura√ß√£o ‚úÖ
**Branch Atual:** `claude/b3-ai-analysis-platform-011CUvNS7Jp7D7bGQWkaBvBw`

---

## üìã √çndice

1. [‚ö° Setup R√°pido - Branch Atualizada](#-setup-r√°pido---branch-atualizada) **‚Üê NOVO!**
2. [Pr√©-requisitos](#-pr√©-requisitos)
3. [Instala√ß√£o R√°pida](#-instala√ß√£o-r√°pida)
4. [Configura√ß√£o Detalhada](#-configura√ß√£o-detalhada)
5. [Valida√ß√£o do Ambiente](#-valida√ß√£o-do-ambiente)
6. [Pr√≥ximos Passos](#-pr√≥ximos-passos)
7. [Troubleshooting](#-troubleshooting)

---

## ‚ö° Setup R√°pido - Branch Atualizada

**Use este procedimento para baixar e rodar a branch mais recente com todos os updates:**

√öltimas atualiza√ß√µes nesta branch:
- ‚úÖ 3 Novos endpoints data-sources (PATCH, GET/:id, POST/:id/test)
- ‚úÖ Type safety melhorado (interfaces TypeScript, 0 tipos `any`)
- ‚úÖ 22 testes unit√°rios (100% coverage nos novos endpoints)
- ‚úÖ 0 warnings React Hook e build
- ‚úÖ Melhorias de valida√ß√£o (DTOs completos)

### Windows (PowerShell)

```powershell
# 1. Parar e limpar (necess√°rio para recriar com nova config)
cd "C:\Users\adria\Dropbox\PC (2)\Downloads\Python - Projetos\invest-claude-web"
docker-compose down -v

# 2. Atualizar c√≥digo (pegar √∫ltimos commits)
git fetch origin
git checkout claude/b3-ai-analysis-platform-011CUvNS7Jp7D7bGQWkaBvBw
git pull origin claude/b3-ai-analysis-platform-011CUvNS7Jp7D7bGQWkaBvBw

# 3. Verificar √∫ltimos commits
git log --oneline -5
# Deve mostrar:
# 3cfffb0 test: adicionar testes unit√°rios completos para data-sources endpoints
# 6153da4 refactor: melhorar type safety removendo tipos any e adicionando interfaces
# f642723 feat: implementar endpoints PATCH e POST test para data-sources
# 0e785c1 fix: corrigir 9 warnings React Hook useEffect com eslint-disable justificados

# 4. Iniciar sistema (usa script manager ou docker-compose)
.\system-manager.ps1 start

# OU direto com docker-compose
docker-compose up -d --build
```

### Linux / macOS (Bash)

```bash
# 1. Parar e limpar
cd ~/invest-claude-web  # ou caminho do seu projeto
docker-compose down -v

# 2. Atualizar c√≥digo
git fetch origin
git checkout claude/b3-ai-analysis-platform-011CUvNS7Jp7D7bGQWkaBvBw
git pull origin claude/b3-ai-analysis-platform-011CUvNS7Jp7D7bGQWkaBvBw

# 3. Verificar √∫ltimos commits
git log --oneline -5

# 4. Iniciar sistema
./system-manager.sh start

# OU direto com docker-compose
docker-compose up -d --build
```

### Verifica√ß√£o P√≥s-Setup

```bash
# Verificar containers rodando
docker-compose ps

# Ver logs
docker-compose logs -f

# Aguardar ~30 segundos e verificar status
# Deve mostrar: postgres, redis, backend, frontend todos "Up"

# Testar backend
curl http://localhost:3001/api/health
# Resposta esperada: {"status":"ok"}

# Testar novos endpoints data-sources
curl http://localhost:3001/api/data-sources
curl http://localhost:3001/api/data-sources/status
```

### Acessar Aplica√ß√£o

- **Frontend:** http://localhost:3000
- **Backend API:** http://localhost:3001/api
- **API Docs (Swagger):** http://localhost:3001/api/docs
- **FastAPI Service:** http://localhost:8000/docs
- **PgAdmin:** http://localhost:5050 (admin@invest.com / admin123)

### Executar Migrations e Seeds (Primeira vez)

```bash
# Executar migrations
docker-compose exec backend npm run migration:run

# Popular banco
docker-compose exec backend npm run seed

# Verificar dados
docker-compose exec postgres psql -U postgres -d invest_db -c "SELECT COUNT(*) FROM assets;"
```

### Executar Testes (Opcional)

```bash
# Rodar testes backend
docker-compose exec backend npm test

# Rodar testes espec√≠ficos (data-sources)
docker-compose exec backend npm test -- data-sources
# Resultado esperado: 22 passed

# Build frontend (verificar 0 warnings)
docker-compose exec frontend npm run build
```

---

## ‚úÖ Pr√©-requisitos

### Software Necess√°rio

- **Python 3.10+** (verificar: `python3 --version`)
- **Node.js 18+** (verificar: `node --version`)
- **npm 9+** (verificar: `npm --version`)
- **Git** (verificar: `git --version`)
- **Chrome/Chromium** (para scrapers)

### Opcionais (para produ√ß√£o)

- **Docker 20+** (verificar: `docker --version`)
- **Docker Compose** (verificar: `docker-compose --version`)
- **PostgreSQL 14+** (ou via Docker)
- **Redis 7+** (ou via Docker)

---

## üöÄ Instala√ß√£o R√°pida

### 1. Clone o Reposit√≥rio

```bash
git clone <repo-url>
cd invest
```

### 2. Instalar Depend√™ncias Python

```bash
cd backend/python-scrapers
pip install -r requirements.txt
```

**Sa√≠da esperada:**
```
Successfully installed 46 packages
‚úì selenium, aiohttp, loguru, beautifulsoup4, pandas, redis...
```

### 3. Instalar Depend√™ncias Node.js (Backend)

```bash
cd ../  # volta para backend/
npm install
```

### 4. Instalar Depend√™ncias Node.js (Frontend)

```bash
cd ../frontend
npm install
```

### 5. Validar Instala√ß√£o

```bash
cd ../backend
python python-scrapers/validate_setup.py
```

**Sa√≠da esperada:**
```
‚úÖ AMBIENTE V√ÅLIDO E PRONTO PARA USO!
Total de verifica√ß√µes: 26
‚úì Passou: 26 (100%)
```

---

## üîß Configura√ß√£o Detalhada

### Passo 1: Configurar Vari√°veis de Ambiente

#### Backend (.env)

O arquivo `.env` j√° existe em `backend/.env` com valores padr√£o.

**Vari√°veis que voc√™ DEVE atualizar:**

```bash
# OpenAI API (para funcionalidade IA)
OPENAI_API_KEY=sk-...

# Google OAuth (opcional, para autentica√ß√£o)
GOOGLE_CLIENT_ID=your-client-id
GOOGLE_CLIENT_SECRET=your-client-secret

# Para produ√ß√£o: gerar novos secrets
JWT_SECRET=$(openssl rand -base64 32)
JWT_REFRESH_SECRET=$(openssl rand -base64 32)
SESSION_SECRET=$(openssl rand -base64 32)
```

**Vari√°veis j√° configuradas (usar localhost):**
```bash
DB_HOST=localhost
DB_PORT=5432
REDIS_HOST=localhost
REDIS_PORT=6379
OPCOES_USERNAME=312.862.178-06  # J√° configurado
OPCOES_PASSWORD=Safra998266@#    # J√° configurado
```

#### Frontend (.env.local)

```bash
cd frontend
cp .env.example .env.local

# Editar .env.local
NEXT_PUBLIC_API_URL=http://localhost:3101/api
NEXT_PUBLIC_WS_URL=http://localhost:3101
```

---

### Passo 2: Criar Diret√≥rios Necess√°rios

```bash
cd backend/python-scrapers

mkdir -p browser-profiles
mkdir -p logs
mkdir -p data/cache
mkdir -p data/results

chmod -R 755 browser-profiles logs
```

**Verificar:**
```bash
ls -la | grep -E "browser-profiles|logs|data"
```

---

### Passo 3: Salvar Cookies OAuth

Este passo √© **necess√°rio** para 18 scrapers que usam autentica√ß√£o OAuth/Google.

```bash
cd backend/python-scrapers
python save_google_cookies.py
```

**O que vai acontecer:**

1. Um navegador Chrome abrir√° automaticamente
2. Voc√™ ser√° guiado para fazer login em **19 sites** sequencialmente
3. Para cada site:
   - Fa√ßa login manualmente
   - Pressione ENTER no terminal quando terminar
   - O script salvar√° os cookies automaticamente
4. Cookies ser√£o salvos em: `browser-profiles/google_cookies.pkl`

**Sites que voc√™ far√° login:**

| # | Site | Tipo | Usado por |
|---|------|------|-----------|
| 1 | Google | OAuth Base | Base para outros |
| 2-4 | Fundamentei, Investidor10, StatusInvest | OAuth | Fundamentalistas |
| 5-8 | Investing, ADVFN, Google Finance, TradingView | OAuth | Mercado |
| 9-13 | ChatGPT, Gemini, DeepSeek, Claude, Grok | OAuth/Direto | IAs |
| 14-19 | Inv.News, Valor, Exame, InfoMoney, Estad√£o, Mais Retorno | OAuth/Direto | Not√≠cias |

**Op√ß√µes do script:**

```bash
# Processar todos os sites (padr√£o)
python save_google_cookies.py

# Processar apenas sites espec√≠ficos
python save_google_cookies.py
# Escolher op√ß√£o 2 e digitar: 1,2,3

# Atualizar apenas sites sem cookies
python save_google_cookies.py
# Escolher op√ß√£o 3
```

**Dica:** Os cookies expiram ap√≥s 7-14 dias. Execute novamente quando scrapers OAuth falharem.

---

### Passo 4: Validar Configura√ß√£o

```bash
cd backend
python python-scrapers/validate_setup.py --detailed
```

**Verifica√ß√µes realizadas:**

1. ‚úÖ **Arquivo .env** - Existe e carregado
2. ‚úÖ **Vari√°veis obrigat√≥rias** - DB, Redis, JWT, Opcoes.net.br
3. ‚úÖ **Diret√≥rios** - Criados com permiss√µes corretas
4. ‚úÖ **Depend√™ncias Python** - 9 pacotes principais instalados
5. ‚ö†Ô∏è **Servi√ßos** - Redis e PostgreSQL (opcional sem Docker)
6. ‚ö†Ô∏è **Cookies OAuth** - Salvos ou n√£o
7. ‚úÖ **Scrapers** - 27 scrapers implementados

**Resultado esperado:**
```
============================================================
üìä RESUMO DA VALIDA√á√ÉO
============================================================

Estat√≠sticas:
  Total de verifica√ß√µes: 26
  ‚úì Passou: 26
  ‚úó Falhou: 0
  ‚ö† Avisos: 5 (n√£o-cr√≠ticos)
  üìà Taxa de sucesso: 100.0%

‚úÖ AMBIENTE V√ÅLIDO E PRONTO PARA USO!

üéØ PR√ìXIMOS PASSOS:
  1. (Opcional) Corrigir avisos para funcionalidade completa
  2. Salvar cookies OAuth: python save_google_cookies.py
  3. Testar scrapers p√∫blicos: python tests/test_public_scrapers.py
```

---

## ‚úÖ Valida√ß√£o do Ambiente

### Script de Valida√ß√£o Autom√°tica

```bash
cd backend
python python-scrapers/validate_setup.py
```

### Valida√ß√£o Manual

#### 1. Verificar Python e Depend√™ncias

```bash
python3 --version  # Deve ser 3.10+

pip list | grep -E "selenium|aiohttp|loguru|beautifulsoup4|pandas|redis"
# Todos devem aparecer
```

#### 2. Verificar Node.js

```bash
node --version  # Deve ser 18+
npm --version   # Deve ser 9+

cd backend
npm run build   # Deve compilar sem erros

cd ../frontend
npm run build   # Deve compilar sem erros
```

#### 3. Verificar Scrapers

```bash
cd backend/python-scrapers/scrapers
ls -la *_scraper.py | wc -l
# Deve retornar: 27
```

#### 4. Verificar Arquivo .env

```bash
cd backend
cat .env | grep -E "DB_HOST|REDIS_HOST|OPCOES_USERNAME"
# Deve mostrar valores configurados
```

---

## üéØ Pr√≥ximos Passos

### Ap√≥s Instala√ß√£o e Valida√ß√£o

#### 1. Testar Scrapers P√∫blicos (sem login)

```bash
cd backend/python-scrapers
python tests/test_public_scrapers.py
```

**Scrapers testados (8):**
- Fundamentus, Investsite, B3, BCB
- Griffin, CoinMarketCap, Bloomberg, Google News

**Resultado esperado:**
```
üìä RESUMO DOS TESTES
Sucesso: 8/8 (100%)
‚úì Todos os scrapers p√∫blicos est√£o funcionando!
```

#### 2. Testar Scraper com Credenciais

```bash
cd backend/python-scrapers
python -c "
from scrapers.opcoes_scraper import OpcoesNetScraper
import asyncio

async def test():
    scraper = OpcoesNetScraper()
    result = await scraper.scrape_with_retry('PETR')
    print(f'Success: {result.success}')
    if result.success:
        print(f'Data: {result.data}')

asyncio.run(test())
"
```

#### 3. Testar Scrapers OAuth (ap√≥s salvar cookies)

```bash
# Criar teste OAuth
# TODO: Implementar test_oauth_scrapers.py
```

---

## üêõ Troubleshooting

### Erro: Docker Build Failed - "parent snapshot does not exist"

**Problema:** Cache do Docker corrompido

**Erro completo:**
```
failed to prepare extraction snapshot: parent snapshot sha256:... does not exist: not found
```

**Solu√ß√£o R√°pida (Recomendada):**
```powershell
# Windows PowerShell
docker-compose down -v
docker builder prune -a -f
docker-compose build --no-cache
docker-compose up -d
```

```bash
# Linux/macOS
docker-compose down -v
docker builder prune -a -f
docker-compose build --no-cache
docker-compose up -d
```

**Solu√ß√£o Completa (se a r√°pida n√£o funcionar):**
```powershell
# Windows - CUIDADO: Remove todas as imagens Docker n√£o usadas
docker-compose down -v
docker system prune -a -f --volumes
docker-compose build --no-cache
docker-compose up -d
```

**√öltimo Recurso:**
1. Fechar Docker Desktop completamente
2. Abrir Docker Desktop novamente
3. Aguardar inicializar
4. Executar solu√ß√£o r√°pida

---

### Erro: "No module named 'loguru'"

**Problema:** Depend√™ncias Python n√£o instaladas

**Solu√ß√£o:**
```bash
cd backend/python-scrapers
pip install -r requirements.txt
```

---

### Erro: "File .env not found"

**Problema:** Arquivo .env n√£o existe

**Solu√ß√£o:**
```bash
cd backend
ls -la .env  # Verificar se existe

# Se n√£o existir (n√£o deveria acontecer)
cat > .env << EOF
NODE_ENV=development
DB_HOST=localhost
REDIS_HOST=localhost
# ... outras vari√°veis
EOF
```

---

### Erro: "connection refused" (Redis/PostgreSQL)

**Problema:** Servi√ßos offline

**Solu√ß√£o (usando Docker):**
```bash
cd invest
docker-compose up -d postgres redis

# Verificar
docker-compose ps
docker logs invest_postgres
docker logs invest_redis
```

**Solu√ß√£o (instala√ß√£o local):**
```bash
# Ubuntu/Debian
sudo apt-get install postgresql-14 redis-server
sudo systemctl start postgresql redis-server

# macOS
brew install postgresql@14 redis
brew services start postgresql@14
brew services start redis
```

---

### Erro: Chrome/Chromium n√£o encontrado

**Problema:** Chrome n√£o instalado ou path incorreto

**Solu√ß√£o:**
```bash
# Ubuntu/Debian
sudo apt-get install chromium-browser

# macOS
brew install --cask google-chrome

# Atualizar .env
CHROME_EXECUTABLE_PATH=/usr/bin/chromium-browser  # Linux
# ou
CHROME_EXECUTABLE_PATH=/Applications/Google Chrome.app/Contents/MacOS/Google Chrome  # macOS
```

---

### Erro: "Permission denied" em diret√≥rios

**Problema:** Permiss√µes incorretas

**Solu√ß√£o:**
```bash
cd backend/python-scrapers
chmod -R 755 browser-profiles logs data
```

---

### Cookies OAuth n√£o funcionando

**Problema:** Cookies expirados ou inv√°lidos

**Solu√ß√£o:**
```bash
cd backend/python-scrapers

# Remover cookies antigos
rm -f browser-profiles/google_cookies.pkl

# Salvar novamente
python save_google_cookies.py
```

---

### Valida√ß√£o com avisos n√£o-cr√≠ticos

**Problema:** Avisos como "Redis offline" ou "Cookies n√£o encontrados"

**Solu√ß√£o:**
- ‚ö†Ô∏è **Redis/PostgreSQL offline**: Normal sem Docker, n√£o bloqueia desenvolvimento
- ‚ö†Ô∏è **Cookies n√£o encontrados**: Execute `save_google_cookies.py`
- ‚ö†Ô∏è **OPENAI_API_KEY**: Apenas necess√°rio para funcionalidade IA
- ‚ö†Ô∏è **GOOGLE_PASSWORD**: Apenas para OAuth autom√°tico (n√£o implementado)

**Estes avisos n√£o impedem o uso do sistema.**

---

## üìù Comandos de Refer√™ncia R√°pida

### Desenvolvimento

```bash
# Backend
cd backend
npm run start:dev
# Acesse: http://localhost:3101

# Frontend
cd frontend
npm run dev
# Acesse: http://localhost:3100

# Scrapers
cd backend/python-scrapers
python tests/test_public_scrapers.py
```

### Docker (Produ√ß√£o)

```bash
# Subir todos os servi√ßos
docker-compose up -d

# Ver logs
docker-compose logs -f

# Parar
docker-compose down

# Limpar volumes
docker-compose down -v
```

### Testes

```bash
# Validar ambiente
cd backend
python python-scrapers/validate_setup.py

# Testar scrapers
cd python-scrapers
python tests/test_public_scrapers.py --detailed

# Testar scraper espec√≠fico
python -m scrapers.fundamentus_scraper PETR4
```

### Manuten√ß√£o

```bash
# Atualizar depend√™ncias Python
cd backend/python-scrapers
pip install -r requirements.txt --upgrade

# Atualizar depend√™ncias Node
cd backend
npm update

# Renovar cookies OAuth (a cada 7-14 dias)
cd backend/python-scrapers
python save_google_cookies.py
```

---

## üìö Documenta√ß√£o Adicional

- **PROGRESS_REPORT.md** - Relat√≥rio de progresso detalhado
- **NEXT_STEPS.md** - Planejamento completo (6 fases)
- **VALIDATION_REPORT.md** - Valida√ß√£o completa do sistema
- **DATA_SOURCES.md** - Cat√°logo de 27 fontes de dados
- **backend/src/ai/README.md** - Documenta√ß√£o dos agentes IA

---

## üÜò Suporte

Se encontrar problemas n√£o cobertos neste guia:

1. Verifique os logs: `backend/python-scrapers/logs/`
2. Execute valida√ß√£o: `python validate_setup.py --detailed`
3. Verifique issues no reposit√≥rio
4. Consulte documenta√ß√£o t√©cnica em `/docs`

---

**‚úÖ Pronto! Seu ambiente est√° configurado e validado.**

**Pr√≥ximo passo:** [Testar scrapers p√∫blicos](./NEXT_STEPS.md#fase-2-testes-iniciais)

---

**√öltima Atualiza√ß√£o:** 2025-11-07
**Vers√£o:** 1.0
**Status:** ‚úÖ Completo e Testado
